# ç´¢å…‹ç”Ÿæ´»å¹³å°ç”Ÿäº§éƒ¨ç½²æŒ‡å—

## æ¦‚è¿°

æœ¬æŒ‡å—è¯¦ç»†è¯´æ˜äº†å¦‚ä½•åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²ç´¢å…‹ç”Ÿæ´»å¹³å°çš„ç°ä»£åŒ–å¾®æœåŠ¡æ¶æ„ï¼ŒåŒ…æ‹¬Kubernetesé›†ç¾¤ã€æœåŠ¡ç½‘æ ¼ã€æ¶ˆæ¯é˜Ÿåˆ—ã€æœç´¢å¼•æ“ã€æµå¤„ç†å¼•æ“å’ŒAI/MLå¹³å°çš„å®Œæ•´éƒ¨ç½²æµç¨‹ã€‚

## å‰ç½®æ¡ä»¶

### ç¡¬ä»¶è¦æ±‚
- **æœ€å°é…ç½®**: 3ä¸ªèŠ‚ç‚¹ï¼Œæ¯ä¸ªèŠ‚ç‚¹8æ ¸CPUï¼Œ32GBå†…å­˜ï¼Œ500GB SSDå­˜å‚¨
- **æ¨èé…ç½®**: 5ä¸ªèŠ‚ç‚¹ï¼Œæ¯ä¸ªèŠ‚ç‚¹16æ ¸CPUï¼Œ64GBå†…å­˜ï¼Œ1TB NVMe SSDå­˜å‚¨
- **ç½‘ç»œ**: 10Gbpså†…ç½‘å¸¦å®½ï¼Œ1Gbpså¤–ç½‘å¸¦å®½

### è½¯ä»¶è¦æ±‚
- **æ“ä½œç³»ç»Ÿ**: Ubuntu 22.04 LTS æˆ– CentOS 8+
- **Kubernetes**: v1.28+
- **Docker**: v24.0+
- **Helm**: v3.12+
- **Istio**: v1.19+

### åŸŸåå’Œè¯ä¹¦
- ä¸»åŸŸå: `suoke.life`
- APIåŸŸå: `api.suoke.life`
- ç›‘æ§åŸŸå: `monitoring.suoke.life`
- SSLè¯ä¹¦: Let's Encryptæˆ–å•†ä¸šè¯ä¹¦

## éƒ¨ç½²æ¶æ„

### é›†ç¾¤æ‹“æ‰‘
```
ç”Ÿäº§ç¯å¢ƒé›†ç¾¤æ¶æ„:
â”œâ”€â”€ MasterèŠ‚ç‚¹ (3ä¸ª)
â”‚   â”œâ”€â”€ kube-apiserver
â”‚   â”œâ”€â”€ etcd
â”‚   â””â”€â”€ kube-controller-manager
â”œâ”€â”€ WorkerèŠ‚ç‚¹ (2-5ä¸ª)
â”‚   â”œâ”€â”€ åº”ç”¨æœåŠ¡
â”‚   â”œâ”€â”€ æ•°æ®å¤„ç†æœåŠ¡
â”‚   â””â”€â”€ ç›‘æ§æœåŠ¡
â””â”€â”€ å­˜å‚¨èŠ‚ç‚¹ (3ä¸ª)
    â”œâ”€â”€ æ•°æ®åº“å­˜å‚¨
    â”œâ”€â”€ æ¶ˆæ¯é˜Ÿåˆ—å­˜å‚¨
    â””â”€â”€ æœç´¢å¼•æ“å­˜å‚¨
```

### ç½‘ç»œæ¶æ„
```
ç½‘ç»œå±‚æ¬¡:
â”œâ”€â”€ å¤–éƒ¨è´Ÿè½½å‡è¡¡å™¨ (äº‘å‚å•†LB)
â”œâ”€â”€ Istio Gateway (å…¥å£ç½‘å…³)
â”œâ”€â”€ Service Mesh (æœåŠ¡é—´é€šä¿¡)
â”œâ”€â”€ Pod Network (å®¹å™¨ç½‘ç»œ)
â””â”€â”€ å­˜å‚¨ç½‘ç»œ (æŒä¹…åŒ–å­˜å‚¨)
```

## éƒ¨ç½²æ­¥éª¤

### ç¬¬ä¸€é˜¶æ®µ: åŸºç¡€è®¾æ–½éƒ¨ç½²

#### 1.1 Kubernetesé›†ç¾¤éƒ¨ç½²
```bash
# ä½¿ç”¨kubeadméƒ¨ç½²é›†ç¾¤
sudo kubeadm init --pod-network-cidr=10.244.0.0/16 \
  --service-cidr=10.96.0.0/12 \
  --kubernetes-version=v1.28.0

# é…ç½®kubectl
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# å®‰è£…ç½‘ç»œæ’ä»¶ (Flannel)
kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml

# åŠ å…¥WorkerèŠ‚ç‚¹
kubeadm join <master-ip>:6443 --token <token> --discovery-token-ca-cert-hash <hash>
```

#### 1.2 å­˜å‚¨ç±»é…ç½®
```yaml
# k8s/storage/storage-class.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-ssd
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp3
  iops: "3000"
  throughput: "125"
  encrypted: "true"
allowVolumeExpansion: true
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer
```

#### 1.3 å‘½åç©ºé—´åˆ›å»º
```bash
# åˆ›å»ºå‘½åç©ºé—´
kubectl create namespace suoke-life
kubectl create namespace istio-system
kubectl create namespace monitoring
kubectl create namespace data-processing
kubectl create namespace elasticsearch

# è®¾ç½®é»˜è®¤å‘½åç©ºé—´
kubectl config set-context --current --namespace=suoke-life
```

### ç¬¬äºŒé˜¶æ®µ: æœåŠ¡ç½‘æ ¼éƒ¨ç½²

#### 2.1 Istioå®‰è£…
```bash
# ä¸‹è½½å¹¶å®‰è£…Istio
curl -L https://istio.io/downloadIstio | sh -
cd istio-*
export PATH=$PWD/bin:$PATH

# å®‰è£…Istioæ§åˆ¶å¹³é¢
istioctl install --set values.defaultRevision=default -y

# å¯ç”¨è‡ªåŠ¨æ³¨å…¥
kubectl label namespace suoke-life istio-injection=enabled
```

#### 2.2 Istioé…ç½®éƒ¨ç½²
```bash
# éƒ¨ç½²Gatewayå’ŒVirtualService
kubectl apply -f k8s/istio/istio-installation.yaml
kubectl apply -f k8s/istio/traffic-management.yaml

# éªŒè¯Istioå®‰è£…
istioctl verify-install
kubectl get pods -n istio-system
```

### ç¬¬ä¸‰é˜¶æ®µ: æ•°æ®å±‚éƒ¨ç½²

#### 3.1 PostgreSQLæ•°æ®åº“
```bash
# ä½¿ç”¨Helmå®‰è£…PostgreSQL
helm repo add bitnami https://charts.bitnami.com/bitnami
helm install postgresql bitnami/postgresql \
  --namespace suoke-life \
  --set auth.postgresPassword=<strong-password> \
  --set primary.persistence.size=100Gi \
  --set primary.persistence.storageClass=fast-ssd \
  --set metrics.enabled=true
```

#### 3.2 Redisç¼“å­˜
```bash
# å®‰è£…Redisé›†ç¾¤
helm install redis bitnami/redis \
  --namespace suoke-life \
  --set auth.password=<redis-password> \
  --set cluster.enabled=true \
  --set cluster.slaveCount=2 \
  --set persistence.size=20Gi \
  --set persistence.storageClass=fast-ssd
```

#### 3.3 MongoDBæ–‡æ¡£æ•°æ®åº“
```bash
# å®‰è£…MongoDB
helm install mongodb bitnami/mongodb \
  --namespace suoke-life \
  --set auth.rootPassword=<mongo-password> \
  --set replicaSet.enabled=true \
  --set replicaSet.replicas.secondary=2 \
  --set persistence.size=50Gi \
  --set persistence.storageClass=fast-ssd
```

### ç¬¬å››é˜¶æ®µ: æ•°æ®å¤„ç†æœåŠ¡éƒ¨ç½²

#### 4.1 äº‹ä»¶æ€»çº¿æœåŠ¡
```bash
# éƒ¨ç½²äº‹ä»¶æ€»çº¿æœåŠ¡
kubectl apply -f k8s/services/event-bus-deployment.yaml

# éªŒè¯äº‹ä»¶æ€»çº¿éƒ¨ç½²
kubectl get pods -l app=event-bus
kubectl get svc event-bus-service
```

#### 4.2 æ‰¹å¤„ç†æœåŠ¡éƒ¨ç½²
```bash
# éƒ¨ç½²æ‰¹å¤„ç†æœåŠ¡
kubectl apply -f k8s/services/batch-processor-deployment.yaml

# åˆ›å»ºå®šæ—¶ä»»åŠ¡
kubectl apply -f - <<EOF
apiVersion: batch/v1
kind: CronJob
metadata:
  name: health-data-aggregation
spec:
  schedule: "*/5 * * * *"  # æ¯5åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: batch-processor
            image: suoke/batch-processor:latest
            command:
            - python
            - -m
            - services.data_processing.health_aggregation
            env:
            - name: DATABASE_URL
              valueFrom:
                secretKeyRef:
                  name: suoke-secrets
                  key: database-url
            - name: ELASTICSEARCH_URL
              value: "http://elasticsearch:9200"
          restartPolicy: OnFailure
EOF
```

### ç¬¬äº”é˜¶æ®µ: æœç´¢å¼•æ“éƒ¨ç½²

#### 5.1 Elasticsearché›†ç¾¤
```bash
# éƒ¨ç½²Elasticsearch
kubectl apply -f k8s/elasticsearch/elasticsearch-cluster.yaml

# ç­‰å¾…é›†ç¾¤å°±ç»ª
kubectl wait --for=condition=ready pod -l app=elasticsearch --timeout=600s

# éªŒè¯é›†ç¾¤çŠ¶æ€
kubectl port-forward svc/elasticsearch-service 9200:9200 &
curl -X GET "localhost:9200/_cluster/health?pretty"
```

#### 5.2 Kibanaéƒ¨ç½²
```bash
# éƒ¨ç½²Kibana
kubectl apply -f k8s/elasticsearch/kibana-deployment.yaml

# é…ç½®Kibanaè®¿é—®
kubectl port-forward svc/kibana-service 5601:5601 &
```

### ç¬¬å…­é˜¶æ®µ: AI/MLå¹³å°éƒ¨ç½²

#### 6.1 MLflowéƒ¨ç½²
```bash
# éƒ¨ç½²MLflow
kubectl apply -f k8s/mlflow/mlflow-deployment.yaml

# éªŒè¯MLflowæœåŠ¡
kubectl get pods -l app=mlflow
kubectl get svc mlflow-service
```

#### 6.2 æ¨¡å‹å­˜å‚¨é…ç½®
```yaml
# k8s/mlflow/minio-storage.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: minio
spec:
  replicas: 1
  selector:
    matchLabels:
      app: minio
  template:
    metadata:
      labels:
        app: minio
    spec:
      containers:
      - name: minio
        image: minio/minio:latest
        args:
        - server
        - /data
        - --console-address
        - ":9001"
        env:
        - name: MINIO_ROOT_USER
          value: "minioadmin"
        - name: MINIO_ROOT_PASSWORD
          value: "minioadmin123"
        ports:
        - containerPort: 9000
        - containerPort: 9001
        volumeMounts:
        - name: data
          mountPath: /data
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: minio-pvc
```

### ç¬¬ä¸ƒé˜¶æ®µ: åº”ç”¨æœåŠ¡éƒ¨ç½²

#### 7.1 é…ç½®ç®¡ç†
```bash
# åˆ›å»ºConfigMap
kubectl create configmap suoke-config \
  --from-file=config/production.env \
  --namespace=suoke-life

# åˆ›å»ºSecret
kubectl create secret generic suoke-secrets \
  --from-literal=database-password=<db-password> \
  --from-literal=redis-password=<redis-password> \
  --from-literal=jwt-secret=<jwt-secret> \
  --namespace=suoke-life
```

#### 7.2 æ™ºèƒ½ä½“æœåŠ¡éƒ¨ç½²
```bash
# éƒ¨ç½²æ™ºèƒ½ä½“æœåŠ¡
kubectl apply -f k8s/services/agents/
kubectl apply -f k8s/services/diagnosis/
kubectl apply -f k8s/services/foundation/

# éªŒè¯æœåŠ¡éƒ¨ç½²
kubectl get pods -l tier=agents
kubectl get svc -l tier=agents
```

#### 7.3 æ•°æ®å¤„ç†æœåŠ¡éƒ¨ç½²
```bash
# éƒ¨ç½²æµå¤„ç†æœåŠ¡
kubectl apply -f k8s/services/stream-processing/

# éƒ¨ç½²äº‹ä»¶å¤„ç†æœåŠ¡
kubectl apply -f k8s/services/event-processing/

# éªŒè¯æ•°æ®å¤„ç†æœåŠ¡
kubectl get pods -l tier=data-processing
```

### ç¬¬å…«é˜¶æ®µ: ç›‘æ§ç³»ç»Ÿéƒ¨ç½²

#### 8.1 Prometheusç›‘æ§
```bash
# ä½¿ç”¨Helmå®‰è£…Prometheus
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --set prometheus.prometheusSpec.retention=30d \
  --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=100Gi \
  --set grafana.adminPassword=<grafana-password>
```

#### 8.2 æ—¥å¿—èšåˆ
```bash
# éƒ¨ç½²ELK Stack
kubectl apply -f k8s/logging/elasticsearch-logging.yaml
kubectl apply -f k8s/logging/logstash-deployment.yaml
kubectl apply -f k8s/logging/filebeat-daemonset.yaml
```

#### 8.3 é“¾è·¯è¿½è¸ª
```bash
# éƒ¨ç½²Jaeger
kubectl apply -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/main/deploy/crds/jaegertracing.io_jaegers_crd.yaml
kubectl apply -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/main/deploy/service_account.yaml
kubectl apply -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/main/deploy/role.yaml
kubectl apply -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/main/deploy/role_binding.yaml
kubectl apply -f https://raw.githubusercontent.com/jaegertracing/jaeger-operator/main/deploy/operator.yaml

# åˆ›å»ºJaegerå®ä¾‹
kubectl apply -f k8s/monitoring/jaeger-instance.yaml
```

## è´¨é‡ä¿è¯éƒ¨ç½²

### SonarQubeä»£ç è´¨é‡
```bash
# éƒ¨ç½²SonarQube
kubectl apply -f k8s/sonarqube/sonarqube-deployment.yaml

# é…ç½®è´¨é‡é—¨ç¦
kubectl apply -f k8s/sonarqube/quality-gate-config.yaml
```

### å®‰å…¨æ‰«æé…ç½®
```bash
# é…ç½®Snykæ‰«æ
kubectl create secret generic snyk-token \
  --from-literal=token=<snyk-token> \
  --namespace=suoke-life

# éƒ¨ç½²å®‰å…¨æ‰«æä½œä¸š
kubectl apply -f k8s/security/snyk-scan-cronjob.yaml
```

## ç½‘ç»œå’Œå®‰å…¨é…ç½®

### 1. ç½‘ç»œç­–ç•¥
```yaml
# k8s/network/network-policy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: suoke-network-policy
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: suoke-life
    - namespaceSelector:
        matchLabels:
          name: istio-system
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: suoke-life
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: TCP
      port: 53
    - protocol: UDP
      port: 53
```

### 2. TLSè¯ä¹¦é…ç½®
```bash
# å®‰è£…cert-manager
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.0/cert-manager.yaml

# é…ç½®Let's Encryptè¯ä¹¦
kubectl apply -f - <<EOF
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: admin@suoke.life
    privateKeySecretRef:
      name: letsencrypt-prod
    solvers:
    - http01:
        ingress:
          class: istio
EOF
```

### 3. RBACæƒé™é…ç½®
```yaml
# k8s/rbac/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: suoke-service-account
  namespace: suoke-life
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: suoke-role
  namespace: suoke-life
rules:
- apiGroups: [""]
  resources: ["pods", "services", "configmaps", "secrets"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: suoke-role-binding
  namespace: suoke-life
subjects:
- kind: ServiceAccount
  name: suoke-service-account
  namespace: suoke-life
roleRef:
  kind: Role
  name: suoke-role
  apiGroup: rbac.authorization.k8s.io
```

## å¤‡ä»½å’Œæ¢å¤

### 1. æ•°æ®åº“å¤‡ä»½
```bash
# PostgreSQLå¤‡ä»½è„šæœ¬
kubectl apply -f - <<EOF
apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgres-backup
spec:
  schedule: "0 2 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: postgres-backup
            image: postgres:15
            command:
            - /bin/bash
            - -c
            - |
              pg_dump -h postgresql -U postgres -d suoke_life > /backup/suoke_life_\$(date +%Y%m%d_%H%M%S).sql
              aws s3 cp /backup/ s3://suoke-backups/postgres/ --recursive
            env:
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: suoke-secrets
                  key: database-password
            volumeMounts:
            - name: backup-storage
              mountPath: /backup
          volumes:
          - name: backup-storage
            emptyDir: {}
          restartPolicy: OnFailure
EOF
```

### 2. é…ç½®å¤‡ä»½
```bash
# å¤‡ä»½Kubernetesé…ç½®
kubectl get all,configmap,secret -o yaml > backup/k8s-config-$(date +%Y%m%d).yaml

# å¤‡ä»½åˆ°äº‘å­˜å‚¨
aws s3 cp backup/ s3://suoke-backups/k8s-config/ --recursive
```

## æ€§èƒ½è°ƒä¼˜

### 1. èµ„æºé™åˆ¶é…ç½®
```yaml
# ç¤ºä¾‹ï¼šæ™ºèƒ½ä½“æœåŠ¡èµ„æºé…ç½®
resources:
  requests:
    memory: "512Mi"
    cpu: "250m"
  limits:
    memory: "1Gi"
    cpu: "500m"
```

### 2. HPAè‡ªåŠ¨æ‰©ç¼©å®¹
```yaml
# k8s/hpa/agent-hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: xiaoai-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: xiaoai-service
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

### 3. æ•°æ®åº“è¿æ¥æ± ä¼˜åŒ–
```python
# æ•°æ®åº“è¿æ¥æ± é…ç½®
DATABASE_CONFIG = {
    "pool_size": 20,
    "max_overflow": 30,
    "pool_timeout": 30,
    "pool_recycle": 3600,
    "pool_pre_ping": True
}
```

## ç›‘æ§å’Œå‘Šè­¦

### 1. å…³é”®æŒ‡æ ‡ç›‘æ§
```yaml
# monitoring/alerts/suoke-alerts.yaml
groups:
- name: suoke-alerts
  rules:
  - alert: HighCPUUsage
    expr: cpu_usage_percent > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High CPU usage detected"
      description: "CPU usage is above 80% for more than 5 minutes"
  
  - alert: HighMemoryUsage
    expr: memory_usage_percent > 85
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High memory usage detected"
      description: "Memory usage is above 85% for more than 5 minutes"
  
  - alert: ServiceDown
    expr: up == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "Service is down"
      description: "Service {{ $labels.instance }} is down"
```

### 2. æ—¥å¿—ç›‘æ§
```yaml
# æ—¥å¿—å‘Šè­¦é…ç½®
- alert: ErrorRateHigh
  expr: rate(log_entries{level="error"}[5m]) > 0.1
  for: 2m
  labels:
    severity: warning
  annotations:
    summary: "High error rate in logs"
    description: "Error rate is above 0.1 errors per second"
```

## éƒ¨ç½²éªŒè¯

### 1. å¥åº·æ£€æŸ¥
```bash
# æ£€æŸ¥æ‰€æœ‰PodçŠ¶æ€
kubectl get pods --all-namespaces

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
kubectl get svc --all-namespaces

# æ£€æŸ¥IstioçŠ¶æ€
istioctl proxy-status

# æ£€æŸ¥åº”ç”¨å¥åº·
curl -f http://api.suoke.life/health
```

### 2. æ€§èƒ½æµ‹è¯•
```bash
# è¿è¡ŒK6æ€§èƒ½æµ‹è¯•
k6 run k6/performance-tests/scenarios/production-load-test.js

# æ£€æŸ¥ç›‘æ§æŒ‡æ ‡
kubectl port-forward -n monitoring svc/prometheus-server 9090:80 &
open http://localhost:9090
```

### 3. åŠŸèƒ½æµ‹è¯•
```bash
# è¿è¡Œç«¯åˆ°ç«¯æµ‹è¯•
cd testing/e2e
python run_production_tests.py

# æ£€æŸ¥æµ‹è¯•ç»“æœ
cat test_results/production_test_report.html
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

#### 1. Podå¯åŠ¨å¤±è´¥
```bash
# æŸ¥çœ‹Podè¯¦æƒ…
kubectl describe pod <pod-name>

# æŸ¥çœ‹Podæ—¥å¿—
kubectl logs <pod-name> -c <container-name>

# æ£€æŸ¥èµ„æºé™åˆ¶
kubectl top pods
kubectl top nodes
```

#### 2. æœåŠ¡è¿æ¥é—®é¢˜
```bash
# æ£€æŸ¥æœåŠ¡ç«¯ç‚¹
kubectl get endpoints <service-name>

# æµ‹è¯•æœåŠ¡è¿é€šæ€§
kubectl run test-pod --image=busybox --rm -it -- sh
nslookup <service-name>
wget -qO- http://<service-name>:<port>/health
```

#### 3. å­˜å‚¨é—®é¢˜
```bash
# æ£€æŸ¥PVå’ŒPVCçŠ¶æ€
kubectl get pv,pvc

# æ£€æŸ¥å­˜å‚¨ç±»
kubectl get storageclass

# æŸ¥çœ‹å­˜å‚¨äº‹ä»¶
kubectl get events --field-selector involvedObject.kind=PersistentVolumeClaim
```

## ç»´æŠ¤å’Œæ›´æ–°

### 1. æ»šåŠ¨æ›´æ–°
```bash
# æ›´æ–°åº”ç”¨é•œåƒ
kubectl set image deployment/xiaoai-service xiaoai=suoke/xiaoai:v2.1.0

# æ£€æŸ¥æ›´æ–°çŠ¶æ€
kubectl rollout status deployment/xiaoai-service

# å›æ»šæ›´æ–°
kubectl rollout undo deployment/xiaoai-service
```

### 2. é›†ç¾¤ç»´æŠ¤
```bash
# èŠ‚ç‚¹ç»´æŠ¤æ¨¡å¼
kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data

# èŠ‚ç‚¹æ¢å¤
kubectl uncordon <node-name>

# é›†ç¾¤å‡çº§
kubeadm upgrade plan
kubeadm upgrade apply v1.28.1
```

### 3. æ•°æ®åº“ç»´æŠ¤
```bash
# æ•°æ®åº“å¤‡ä»½
kubectl exec -it postgresql-0 -- pg_dump -U postgres suoke_life > backup.sql

# æ•°æ®åº“ä¼˜åŒ–
kubectl exec -it postgresql-0 -- psql -U postgres -c "VACUUM ANALYZE;"

# ç´¢å¼•é‡å»º
kubectl exec -it postgresql-0 -- psql -U postgres -c "REINDEX DATABASE suoke_life;"
```

## å®‰å…¨æœ€ä½³å®è·µ

### 1. é•œåƒå®‰å…¨
- ä½¿ç”¨å®˜æ–¹åŸºç¡€é•œåƒ
- å®šæœŸæ›´æ–°é•œåƒ
- æ‰«æé•œåƒæ¼æ´
- ä½¿ç”¨érootç”¨æˆ·è¿è¡Œ

### 2. ç½‘ç»œå®‰å…¨
- å®æ–½ç½‘ç»œç­–ç•¥
- ä½¿ç”¨æœåŠ¡ç½‘æ ¼mTLS
- é™åˆ¶å‡ºå…¥æµé‡
- å®šæœŸå®¡è®¡ç½‘ç»œé…ç½®

### 3. è®¿é—®æ§åˆ¶
- å®æ–½æœ€å°æƒé™åŸåˆ™
- ä½¿ç”¨æœåŠ¡è´¦æˆ·
- å®šæœŸè½®æ¢å¯†é’¥
- å¯ç”¨å®¡è®¡æ—¥å¿—

## æ€»ç»“

é€šè¿‡æœ¬éƒ¨ç½²æŒ‡å—ï¼Œæ‚¨å¯ä»¥åœ¨ç”Ÿäº§ç¯å¢ƒä¸­æˆåŠŸéƒ¨ç½²ç´¢å…‹ç”Ÿæ´»å¹³å°çš„ç°ä»£åŒ–å¾®æœåŠ¡æ¶æ„ã€‚å…³é”®è¦ç‚¹åŒ…æ‹¬ï¼š

1. **åˆ†é˜¶æ®µéƒ¨ç½²**: æŒ‰ç…§åŸºç¡€è®¾æ–½â†’æ•°æ®å±‚â†’åº”ç”¨å±‚çš„é¡ºåºéƒ¨ç½²
2. **ç›‘æ§å…ˆè¡Œ**: åœ¨éƒ¨ç½²åº”ç”¨å‰å…ˆå»ºç«‹ç›‘æ§ä½“ç³»
3. **å®‰å…¨ç¬¬ä¸€**: å®æ–½å…¨é¢çš„å®‰å…¨ç­–ç•¥å’Œè®¿é—®æ§åˆ¶
4. **å¤‡ä»½ç­–ç•¥**: å»ºç«‹å®Œå–„çš„å¤‡ä»½å’Œæ¢å¤æœºåˆ¶
5. **æ€§èƒ½ä¼˜åŒ–**: åˆç†é…ç½®èµ„æºé™åˆ¶å’Œè‡ªåŠ¨æ‰©ç¼©å®¹
6. **æ•…éšœé¢„æ¡ˆ**: å‡†å¤‡å®Œæ•´çš„æ•…éšœæ’é™¤å’Œæ¢å¤æµç¨‹

è¿™ä¸ªç°ä»£åŒ–çš„éƒ¨ç½²æ¶æ„ä¸ºç´¢å…‹ç”Ÿæ´»å¹³å°æä¾›äº†é«˜å¯ç”¨ã€é«˜æ€§èƒ½ã€é«˜å®‰å…¨çš„ç”Ÿäº§ç¯å¢ƒåŸºç¡€ã€‚

---

**ç´¢å…‹ç”Ÿæ´»å¹³å° - ç”Ÿäº§éƒ¨ç½²å®Œæˆ** ğŸš€ 