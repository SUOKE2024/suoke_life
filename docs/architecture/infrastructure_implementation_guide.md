# ç´¢å…‹ç”Ÿæ´»åŸºç¡€è®¾æ–½ç°ä»£åŒ–å®æ–½æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æŒ‡å—æä¾›ç´¢å…‹ç”Ÿæ´»åŸºç¡€è®¾æ–½ç°ä»£åŒ–çš„è¯¦ç»†æŠ€æœ¯å®æ–½æ–¹æ¡ˆï¼ŒåŒ…å«å®Œæ•´çš„é…ç½®æ–‡ä»¶ã€éƒ¨ç½²è„šæœ¬å’Œä»£ç ç¤ºä¾‹ã€‚æ‰€æœ‰æ–¹æ¡ˆéƒ½åŸºäºç°æœ‰ä»£ç æ¶æ„ï¼Œç¡®ä¿å¹³æ»‘å‡çº§å’Œä¸šåŠ¡è¿ç»­æ€§ã€‚

## ğŸš€ é˜¶æ®µä¸€ï¼šåŸºç¡€è®¾æ–½ç°ä»£åŒ–å®æ–½

### 1.1 Redisé›†ç¾¤éƒ¨ç½²

#### Docker Composeé…ç½®
```yaml
# deploy/docker/redis-cluster/docker-compose.yml
version: '3.8'

services:
  redis-node-1:
    image: redis:7-alpine
    container_name: redis-node-1
    command: redis-server /usr/local/etc/redis/redis.conf
    ports:
      - "7001:6379"
      - "17001:16379"
    volumes:
      - ./redis-cluster.conf:/usr/local/etc/redis/redis.conf
      - redis-node-1-data:/data
    networks:
      - redis-cluster

  redis-node-2:
    image: redis:7-alpine
    container_name: redis-node-2
    command: redis-server /usr/local/etc/redis/redis.conf
    ports:
      - "7002:6379"
      - "17002:16379"
    volumes:
      - ./redis-cluster.conf:/usr/local/etc/redis/redis.conf
      - redis-node-2-data:/data
    networks:
      - redis-cluster

  redis-node-3:
    image: redis:7-alpine
    container_name: redis-node-3
    command: redis-server /usr/local/etc/redis/redis.conf
    ports:
      - "7003:6379"
      - "17003:16379"
    volumes:
      - ./redis-cluster.conf:/usr/local/etc/redis/redis.conf
      - redis-node-3-data:/data
    networks:
      - redis-cluster

  redis-node-4:
    image: redis:7-alpine
    container_name: redis-node-4
    command: redis-server /usr/local/etc/redis/redis.conf
    ports:
      - "7004:6379"
      - "17004:16379"
    volumes:
      - ./redis-cluster.conf:/usr/local/etc/redis/redis.conf
      - redis-node-4-data:/data
    networks:
      - redis-cluster

  redis-node-5:
    image: redis:7-alpine
    container_name: redis-node-5
    command: redis-server /usr/local/etc/redis/redis.conf
    ports:
      - "7005:6379"
      - "17005:16379"
    volumes:
      - ./redis-cluster.conf:/usr/local/etc/redis/redis.conf
      - redis-node-5-data:/data
    networks:
      - redis-cluster

  redis-node-6:
    image: redis:7-alpine
    container_name: redis-node-6
    command: redis-server /usr/local/etc/redis/redis.conf
    ports:
      - "7006:6379"
      - "17006:16379"
    volumes:
      - ./redis-cluster.conf:/usr/local/etc/redis/redis.conf
      - redis-node-6-data:/data
    networks:
      - redis-cluster

  redis-cluster-init:
    image: redis:7-alpine
    container_name: redis-cluster-init
    command: >
      sh -c "
        sleep 10 &&
        redis-cli --cluster create 
        redis-node-1:6379 redis-node-2:6379 redis-node-3:6379 
        redis-node-4:6379 redis-node-5:6379 redis-node-6:6379 
        --cluster-replicas 1 --cluster-yes
      "
    depends_on:
      - redis-node-1
      - redis-node-2
      - redis-node-3
      - redis-node-4
      - redis-node-5
      - redis-node-6
    networks:
      - redis-cluster

volumes:
  redis-node-1-data:
  redis-node-2-data:
  redis-node-3-data:
  redis-node-4-data:
  redis-node-5-data:
  redis-node-6-data:

networks:
  redis-cluster:
    driver: bridge
```

#### Redisé›†ç¾¤é…ç½®æ–‡ä»¶
```conf
# deploy/docker/redis-cluster/redis-cluster.conf
port 6379
cluster-enabled yes
cluster-config-file nodes.conf
cluster-node-timeout 5000
appendonly yes
appendfsync everysec
save 900 1
save 300 10
save 60 10000

# å‘é‡æœç´¢æ”¯æŒ
loadmodule /opt/redis-stack/lib/redisearch.so
loadmodule /opt/redis-stack/lib/rejson.so

# å†…å­˜ä¼˜åŒ–
maxmemory 2gb
maxmemory-policy allkeys-lru

# ç½‘ç»œé…ç½®
bind 0.0.0.0
protected-mode no
tcp-keepalive 300

# æ—¥å¿—é…ç½®
loglevel notice
logfile /data/redis.log
```

#### ç°æœ‰SessionRepositoryé€‚é…
```python
# services/agent-services/xiaoai-service/xiaoai/repository/cluster_session_repository.py
import json
import hashlib
from typing import Dict, Any, Optional
from rediscluster import RedisCluster
from .session_repository import SessionRepository

class ClusterSessionRepository(SessionRepository):
    """Redisé›†ç¾¤é€‚é…çš„ä¼šè¯å­˜å‚¨"""
    
    def __init__(self, cluster_nodes: list, password: str = None):
        self.cluster = RedisCluster(
            startup_nodes=cluster_nodes,
            password=password,
            decode_responses=True,
            skip_full_coverage_check=True,
            health_check_interval=30
        )
        self.session_ttl = 3600  # 1å°æ—¶
    
    async def save_session(self, session_id: str, session_data: Dict[str, Any]) -> bool:
        """ä¿å­˜ä¼šè¯æ•°æ®åˆ°Redisé›†ç¾¤"""
        try:
            # ä½¿ç”¨ä¼šè¯IDçš„å“ˆå¸Œç¡®ä¿æ•°æ®åˆ†å¸ƒ
            key = f"session:{session_id}"
            serialized_data = json.dumps(session_data, ensure_ascii=False)
            
            # è®¾ç½®è¿‡æœŸæ—¶é—´
            result = self.cluster.setex(key, self.session_ttl, serialized_data)
            
            # è®°å½•æŒ‡æ ‡
            await self._record_session_metrics("save", session_id, True)
            return result
            
        except Exception as e:
            await self._record_session_metrics("save", session_id, False, str(e))
            raise
    
    async def get_session(self, session_id: str) -> Optional[Dict[str, Any]]:
        """ä»Redisé›†ç¾¤è·å–ä¼šè¯æ•°æ®"""
        try:
            key = f"session:{session_id}"
            data = self.cluster.get(key)
            
            if data:
                session_data = json.loads(data)
                await self._record_session_metrics("get", session_id, True)
                return session_data
            
            await self._record_session_metrics("get", session_id, False, "Session not found")
            return None
            
        except Exception as e:
            await self._record_session_metrics("get", session_id, False, str(e))
            raise
    
    async def delete_session(self, session_id: str) -> bool:
        """ä»Redisé›†ç¾¤åˆ é™¤ä¼šè¯æ•°æ®"""
        try:
            key = f"session:{session_id}"
            result = self.cluster.delete(key)
            
            await self._record_session_metrics("delete", session_id, bool(result))
            return bool(result)
            
        except Exception as e:
            await self._record_session_metrics("delete", session_id, False, str(e))
            raise
    
    async def update_session_ttl(self, session_id: str, ttl: int = None) -> bool:
        """æ›´æ–°ä¼šè¯è¿‡æœŸæ—¶é—´"""
        try:
            key = f"session:{session_id}"
            ttl = ttl or self.session_ttl
            result = self.cluster.expire(key, ttl)
            
            await self._record_session_metrics("update_ttl", session_id, bool(result))
            return bool(result)
            
        except Exception as e:
            await self._record_session_metrics("update_ttl", session_id, False, str(e))
            raise
    
    async def get_active_sessions_count(self) -> int:
        """è·å–æ´»è·ƒä¼šè¯æ•°é‡"""
        try:
            # ä½¿ç”¨SCANå‘½ä»¤éå†æ‰€æœ‰èŠ‚ç‚¹
            total_count = 0
            for node in self.cluster.get_nodes():
                cursor = 0
                while True:
                    cursor, keys = node.redis_connection.scan(
                        cursor, match="session:*", count=1000
                    )
                    total_count += len(keys)
                    if cursor == 0:
                        break
            
            return total_count
            
        except Exception as e:
            print(f"Error counting active sessions: {e}")
            return 0
    
    async def _record_session_metrics(self, operation: str, session_id: str, 
                                    success: bool, error: str = None):
        """è®°å½•ä¼šè¯æ“ä½œæŒ‡æ ‡"""
        # é›†æˆç°æœ‰çš„æŒ‡æ ‡æ”¶é›†ç³»ç»Ÿ
        from xiaoai.utils.metrics import get_metrics_collector
        
        metrics = get_metrics_collector()
        metrics.increment_session_operation(operation, success)
        
        if not success and error:
            metrics.record_session_error(operation, error)

# å·¥å‚æ–¹æ³•æ›´æ–°
def get_session_repository() -> SessionRepository:
    """è·å–ä¼šè¯å­˜å‚¨å®ä¾‹"""
    from xiaoai.utils.config_loader import get_config
    
    config = get_config()
    redis_config = config.get_section("redis")
    
    if redis_config.get("cluster_enabled", False):
        cluster_nodes = [
            {"host": node["host"], "port": node["port"]} 
            for node in redis_config["cluster_nodes"]
        ]
        return ClusterSessionRepository(
            cluster_nodes=cluster_nodes,
            password=redis_config.get("password")
        )
    else:
        # å›é€€åˆ°å•å®ä¾‹Redis
        return SessionRepository(
            host=redis_config["host"],
            port=redis_config["port"],
            password=redis_config.get("password")
        )
```

#### æ•°æ®è¿ç§»è„šæœ¬
```python
# scripts/migrate_to_redis_cluster.py
import asyncio
import json
from typing import Dict, Any
import redis
from rediscluster import RedisCluster

class RedisClusterMigrator:
    """Rediså•å®ä¾‹åˆ°é›†ç¾¤çš„æ•°æ®è¿ç§»å·¥å…·"""
    
    def __init__(self, source_config: Dict[str, Any], target_config: Dict[str, Any]):
        # æºRediså®ä¾‹
        self.source = redis.Redis(
            host=source_config["host"],
            port=source_config["port"],
            password=source_config.get("password"),
            decode_responses=True
        )
        
        # ç›®æ ‡Redisé›†ç¾¤
        self.target = RedisCluster(
            startup_nodes=target_config["cluster_nodes"],
            password=target_config.get("password"),
            decode_responses=True,
            skip_full_coverage_check=True
        )
        
        self.batch_size = 1000
        self.migrated_count = 0
        self.failed_count = 0
    
    async def migrate_all_data(self):
        """è¿ç§»æ‰€æœ‰æ•°æ®"""
        print("å¼€å§‹Redisæ•°æ®è¿ç§»...")
        
        # è·å–æ‰€æœ‰é”®
        all_keys = self.source.keys("*")
        total_keys = len(all_keys)
        
        print(f"å‘ç° {total_keys} ä¸ªé”®éœ€è¦è¿ç§»")
        
        # åˆ†æ‰¹è¿ç§»
        for i in range(0, total_keys, self.batch_size):
            batch_keys = all_keys[i:i + self.batch_size]
            await self._migrate_batch(batch_keys)
            
            progress = (i + len(batch_keys)) / total_keys * 100
            print(f"è¿ç§»è¿›åº¦: {progress:.1f}% ({self.migrated_count} æˆåŠŸ, {self.failed_count} å¤±è´¥)")
        
        print(f"è¿ç§»å®Œæˆ! æ€»è®¡: {self.migrated_count} æˆåŠŸ, {self.failed_count} å¤±è´¥")
    
    async def _migrate_batch(self, keys: list):
        """è¿ç§»ä¸€æ‰¹æ•°æ®"""
        for key in keys:
            try:
                # è·å–é”®ç±»å‹
                key_type = self.source.type(key)
                
                if key_type == "string":
                    await self._migrate_string(key)
                elif key_type == "hash":
                    await self._migrate_hash(key)
                elif key_type == "list":
                    await self._migrate_list(key)
                elif key_type == "set":
                    await self._migrate_set(key)
                elif key_type == "zset":
                    await self._migrate_zset(key)
                else:
                    print(f"è·³è¿‡ä¸æ”¯æŒçš„é”®ç±»å‹: {key} ({key_type})")
                    continue
                
                # è¿ç§»TTL
                ttl = self.source.ttl(key)
                if ttl > 0:
                    self.target.expire(key, ttl)
                
                self.migrated_count += 1
                
            except Exception as e:
                print(f"è¿ç§»é”® {key} å¤±è´¥: {e}")
                self.failed_count += 1
    
    async def _migrate_string(self, key: str):
        """è¿ç§»å­—ç¬¦ä¸²ç±»å‹"""
        value = self.source.get(key)
        self.target.set(key, value)
    
    async def _migrate_hash(self, key: str):
        """è¿ç§»å“ˆå¸Œç±»å‹"""
        hash_data = self.source.hgetall(key)
        if hash_data:
            self.target.hmset(key, hash_data)
    
    async def _migrate_list(self, key: str):
        """è¿ç§»åˆ—è¡¨ç±»å‹"""
        list_data = self.source.lrange(key, 0, -1)
        if list_data:
            self.target.lpush(key, *reversed(list_data))
    
    async def _migrate_set(self, key: str):
        """è¿ç§»é›†åˆç±»å‹"""
        set_data = self.source.smembers(key)
        if set_data:
            self.target.sadd(key, *set_data)
    
    async def _migrate_zset(self, key: str):
        """è¿ç§»æœ‰åºé›†åˆç±»å‹"""
        zset_data = self.source.zrange(key, 0, -1, withscores=True)
        if zset_data:
            self.target.zadd(key, dict(zset_data))
    
    async def verify_migration(self):
        """éªŒè¯è¿ç§»ç»“æœ"""
        print("å¼€å§‹éªŒè¯è¿ç§»ç»“æœ...")
        
        source_keys = set(self.source.keys("*"))
        
        verified_count = 0
        failed_count = 0
        
        for key in source_keys:
            try:
                # æ£€æŸ¥é”®æ˜¯å¦å­˜åœ¨
                if not self.target.exists(key):
                    print(f"éªŒè¯å¤±è´¥: é”® {key} åœ¨ç›®æ ‡é›†ç¾¤ä¸­ä¸å­˜åœ¨")
                    failed_count += 1
                    continue
                
                # æ£€æŸ¥å€¼æ˜¯å¦ä¸€è‡´
                source_type = self.source.type(key)
                if source_type == "string":
                    source_value = self.source.get(key)
                    target_value = self.target.get(key)
                    if source_value != target_value:
                        print(f"éªŒè¯å¤±è´¥: é”® {key} å€¼ä¸ä¸€è‡´")
                        failed_count += 1
                        continue
                
                verified_count += 1
                
            except Exception as e:
                print(f"éªŒè¯é”® {key} æ—¶å‡ºé”™: {e}")
                failed_count += 1
        
        print(f"éªŒè¯å®Œæˆ: {verified_count} æˆåŠŸ, {failed_count} å¤±è´¥")
        return failed_count == 0

async def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®
    source_config = {
        "host": "localhost",
        "port": 6379,
        "password": None
    }
    
    target_config = {
        "cluster_nodes": [
            {"host": "localhost", "port": 7001},
            {"host": "localhost", "port": 7002},
            {"host": "localhost", "port": 7003},
            {"host": "localhost", "port": 7004},
            {"host": "localhost", "port": 7005},
            {"host": "localhost", "port": 7006},
        ],
        "password": None
    }
    
    # æ‰§è¡Œè¿ç§»
    migrator = RedisClusterMigrator(source_config, target_config)
    await migrator.migrate_all_data()
    
    # éªŒè¯è¿ç§»
    success = await migrator.verify_migration()
    
    if success:
        print("âœ… æ•°æ®è¿ç§»æˆåŠŸå®Œæˆ!")
    else:
        print("âŒ æ•°æ®è¿ç§»å­˜åœ¨é—®é¢˜ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")

if __name__ == "__main__":
    asyncio.run(main())
```

### 1.2 OpenTelemetryå¯è§‚æµ‹æ€§å‡çº§

#### OpenTelemetryé…ç½®
```yaml
# deploy/docker/observability/docker-compose.yml
version: '3.8'

services:
  jaeger:
    image: jaegertracing/all-in-one:1.50
    container_name: jaeger
    ports:
      - "16686:16686"  # Jaeger UI
      - "14268:14268"  # Jaeger HTTP
      - "14250:14250"  # Jaeger gRPC
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    networks:
      - observability

  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.88.0
    container_name: otel-collector
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml
    ports:
      - "4317:4317"   # OTLP gRPC receiver
      - "4318:4318"   # OTLP HTTP receiver
      - "8889:8889"   # Prometheus metrics
    depends_on:
      - jaeger
    networks:
      - observability

  loki:
    image: grafana/loki:2.9.0
    container_name: loki
    ports:
      - "3100:3100"
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - ./loki-config.yaml:/etc/loki/local-config.yaml
      - loki-data:/loki
    networks:
      - observability

  promtail:
    image: grafana/promtail:2.9.0
    container_name: promtail
    volumes:
      - ./promtail-config.yaml:/etc/promtail/config.yml
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    command: -config.file=/etc/promtail/config.yml
    depends_on:
      - loki
    networks:
      - observability

volumes:
  loki-data:

networks:
  observability:
    driver: bridge
```

#### OpenTelemetry Collectoré…ç½®
```yaml
# deploy/docker/observability/otel-collector-config.yaml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

processors:
  batch:
    timeout: 1s
    send_batch_size: 1024
  
  memory_limiter:
    limit_mib: 512
  
  resource:
    attributes:
      - key: service.namespace
        value: suoke-life
        action: upsert

exporters:
  jaeger:
    endpoint: jaeger:14250
    tls:
      insecure: true
  
  prometheus:
    endpoint: "0.0.0.0:8889"
    namespace: suoke_life
    const_labels:
      environment: production
  
  loki:
    endpoint: http://loki:3100/loki/api/v1/push
    tenant_id: suoke-life

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [memory_limiter, resource, batch]
      exporters: [jaeger]
    
    metrics:
      receivers: [otlp]
      processors: [memory_limiter, resource, batch]
      exporters: [prometheus]
    
    logs:
      receivers: [otlp]
      processors: [memory_limiter, resource, batch]
      exporters: [loki]
```

#### å¢å¼ºçš„æŒ‡æ ‡æ”¶é›†å™¨
```python
# services/agent-services/xiaoai-service/xiaoai/utils/enhanced_metrics.py
import time
import asyncio
from typing import Dict, Any, Optional
from contextlib import asynccontextmanager
from opentelemetry import trace, metrics
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.exporter.otlp.proto.grpc.metric_exporter import OTLPMetricExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader
from opentelemetry.sdk.resources import Resource

class EnhancedMetricsCollector:
    """å¢å¼ºçš„æŒ‡æ ‡æ”¶é›†å™¨ï¼Œé›†æˆOpenTelemetry"""
    
    def __init__(self, service_name: str = "xiaoai-service"):
        self.service_name = service_name
        
        # åˆå§‹åŒ–èµ„æº
        resource = Resource.create({
            "service.name": service_name,
            "service.namespace": "suoke-life",
            "service.version": "1.0.0"
        })
        
        # åˆå§‹åŒ–è¿½è¸ª
        trace.set_tracer_provider(TracerProvider(resource=resource))
        tracer_provider = trace.get_tracer_provider()
        
        # é…ç½®Jaegerå¯¼å‡ºå™¨
        jaeger_exporter = OTLPSpanExporter(
            endpoint="http://localhost:4317",
            insecure=True
        )
        
        span_processor = BatchSpanProcessor(jaeger_exporter)
        tracer_provider.add_span_processor(span_processor)
        
        self.tracer = trace.get_tracer(__name__)
        
        # åˆå§‹åŒ–æŒ‡æ ‡
        metric_reader = PeriodicExportingMetricReader(
            OTLPMetricExporter(
                endpoint="http://localhost:4317",
                insecure=True
            ),
            export_interval_millis=5000
        )
        
        metrics.set_meter_provider(MeterProvider(
            resource=resource,
            metric_readers=[metric_reader]
        ))
        
        self.meter = metrics.get_meter(__name__)
        
        # åˆ›å»ºæŒ‡æ ‡
        self._init_metrics()
    
    def _init_metrics(self):
        """åˆå§‹åŒ–æŒ‡æ ‡"""
        # LLMè°ƒç”¨æŒ‡æ ‡
        self.llm_request_counter = self.meter.create_counter(
            name="llm_requests_total",
            description="Total number of LLM requests",
            unit="1"
        )
        
        self.llm_request_duration = self.meter.create_histogram(
            name="llm_request_duration_seconds",
            description="Duration of LLM requests",
            unit="s"
        )
        
        self.llm_token_counter = self.meter.create_counter(
            name="llm_tokens_total",
            description="Total number of tokens processed",
            unit="1"
        )
        
        # ä¼šè¯æŒ‡æ ‡
        self.session_counter = self.meter.create_counter(
            name="sessions_total",
            description="Total number of sessions",
            unit="1"
        )
        
        self.active_sessions_gauge = self.meter.create_up_down_counter(
            name="active_sessions",
            description="Number of active sessions",
            unit="1"
        )
        
        # å¤šæ¨¡æ€å¤„ç†æŒ‡æ ‡
        self.multimodal_counter = self.meter.create_counter(
            name="multimodal_requests_total",
            description="Total number of multimodal requests",
            unit="1"
        )
        
        self.multimodal_duration = self.meter.create_histogram(
            name="multimodal_processing_duration_seconds",
            description="Duration of multimodal processing",
            unit="s"
        )
    
    @asynccontextmanager
    async def trace_operation(self, operation_name: str, **attributes):
        """è¿½è¸ªæ“ä½œçš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        with self.tracer.start_as_current_span(operation_name) as span:
            # è®¾ç½®å±æ€§
            for key, value in attributes.items():
                span.set_attribute(key, value)
            
            start_time = time.time()
            
            try:
                yield span
            except Exception as e:
                span.set_attribute("error", True)
                span.set_attribute("error.message", str(e))
                span.set_attribute("error.type", type(e).__name__)
                raise
            finally:
                duration = time.time() - start_time
                span.set_attribute("duration", duration)
    
    def enhanced_track_llm_metrics(self, model: str, query_type: str):
        """å¢å¼ºçš„LLMæŒ‡æ ‡è£…é¥°å™¨"""
        def decorator(func):
            async def wrapper(*args, **kwargs):
                async with self.trace_operation(
                    f"llm_{model}_{query_type}",
                    model=model,
                    query_type=query_type,
                    service=self.service_name
                ) as span:
                    start_time = time.time()
                    
                    try:
                        result = await func(*args, **kwargs)
                        
                        # è®°å½•æˆåŠŸæŒ‡æ ‡
                        duration = time.time() - start_time
                        
                        self.llm_request_counter.add(1, {
                            "model": model,
                            "query_type": query_type,
                            "status": "success"
                        })
                        
                        self.llm_request_duration.record(duration, {
                            "model": model,
                            "query_type": query_type
                        })
                        
                        # è®°å½•tokenä½¿ç”¨é‡
                        if isinstance(result, dict):
                            input_tokens = result.get("usage", {}).get("prompt_tokens", 0)
                            output_tokens = result.get("usage", {}).get("completion_tokens", 0)
                            
                            self.llm_token_counter.add(input_tokens, {
                                "model": model,
                                "type": "input"
                            })
                            
                            self.llm_token_counter.add(output_tokens, {
                                "model": model,
                                "type": "output"
                            })
                            
                            span.set_attribute("tokens.input", input_tokens)
                            span.set_attribute("tokens.output", output_tokens)
                        
                        span.set_attribute("success", True)
                        return result
                        
                    except Exception as e:
                        # è®°å½•å¤±è´¥æŒ‡æ ‡
                        duration = time.time() - start_time
                        
                        self.llm_request_counter.add(1, {
                            "model": model,
                            "query_type": query_type,
                            "status": "error",
                            "error_type": type(e).__name__
                        })
                        
                        self.llm_request_duration.record(duration, {
                            "model": model,
                            "query_type": query_type,
                            "status": "error"
                        })
                        
                        span.set_attribute("success", False)
                        raise
            
            return wrapper
        return decorator
    
    async def record_session_metrics(self, operation: str, success: bool, **labels):
        """è®°å½•ä¼šè¯æŒ‡æ ‡"""
        self.session_counter.add(1, {
            "operation": operation,
            "status": "success" if success else "error",
            **labels
        })
    
    async def record_multimodal_metrics(self, input_type: str, duration: float, 
                                      success: bool, **labels):
        """è®°å½•å¤šæ¨¡æ€å¤„ç†æŒ‡æ ‡"""
        self.multimodal_counter.add(1, {
            "input_type": input_type,
            "status": "success" if success else "error",
            **labels
        })
        
        self.multimodal_duration.record(duration, {
            "input_type": input_type,
            **labels
        })
    
    async def update_active_sessions(self, count: int):
        """æ›´æ–°æ´»è·ƒä¼šè¯æ•°é‡"""
        self.active_sessions_gauge.add(count)

# å…¨å±€å®ä¾‹
_metrics_collector = None

def get_enhanced_metrics_collector() -> EnhancedMetricsCollector:
    """è·å–å¢å¼ºçš„æŒ‡æ ‡æ”¶é›†å™¨å®ä¾‹"""
    global _metrics_collector
    if _metrics_collector is None:
        _metrics_collector = EnhancedMetricsCollector()
    return _metrics_collector

# å…¼å®¹ç°æœ‰ä»£ç çš„è£…é¥°å™¨
def track_llm_metrics(model: str, query_type: str):
    """å…¼å®¹ç°æœ‰ä»£ç çš„è£…é¥°å™¨"""
    collector = get_enhanced_metrics_collector()
    return collector.enhanced_track_llm_metrics(model, query_type)
```

#### é›†æˆåˆ°ç°æœ‰AgentManager
```python
# services/agent-services/xiaoai-service/xiaoai/agent/enhanced_agent_manager.py
from typing import Dict, Any, Optional
from xiaoai.agent.agent_manager import AgentManager
from xiaoai.utils.enhanced_metrics import get_enhanced_metrics_collector, track_llm_metrics

class EnhancedAgentManager(AgentManager):
    """å¢å¼ºçš„æ™ºèƒ½ä½“ç®¡ç†å™¨ï¼Œé›†æˆOpenTelemetry"""
    
    def __init__(self):
        super().__init__()
        self.metrics = get_enhanced_metrics_collector()
    
    @track_llm_metrics(model="primary", query_type="chat")
    async def chat(self, user_id: str, message: str, session_id: str = None):
        """å¢å¼ºçš„èŠå¤©æ–¹æ³•ï¼ŒåŒ…å«å®Œæ•´çš„å¯è§‚æµ‹æ€§"""
        async with self.metrics.trace_operation(
            "agent_chat",
            user_id=user_id,
            session_id=session_id,
            message_length=len(message)
        ) as span:
            try:
                # è°ƒç”¨åŸæœ‰æ–¹æ³•
                result = await super().chat(user_id, message, session_id)
                
                # è®°å½•é¢å¤–çš„ä¸šåŠ¡æŒ‡æ ‡
                span.set_attribute("response_length", len(result.get("response", "")))
                span.set_attribute("intent", result.get("intent", "unknown"))
                
                return result
                
            except Exception as e:
                span.set_attribute("error_details", str(e))
                raise
    
    async def process_multimodal_input(self, user_id: str, input_data: Dict[str, Any]):
        """å¢å¼ºçš„å¤šæ¨¡æ€å¤„ç†ï¼ŒåŒ…å«æŒ‡æ ‡æ”¶é›†"""
        input_type = self._determine_input_type(input_data)
        
        async with self.metrics.trace_operation(
            "multimodal_processing",
            user_id=user_id,
            input_type=input_type,
            data_size=len(str(input_data))
        ) as span:
            start_time = time.time()
            
            try:
                result = await super().process_multimodal_input(user_id, input_data)
                
                # è®°å½•æˆåŠŸæŒ‡æ ‡
                duration = time.time() - start_time
                await self.metrics.record_multimodal_metrics(
                    input_type=input_type,
                    duration=duration,
                    success=True,
                    user_id=user_id
                )
                
                span.set_attribute("processing_success", True)
                return result
                
            except Exception as e:
                # è®°å½•å¤±è´¥æŒ‡æ ‡
                duration = time.time() - start_time
                await self.metrics.record_multimodal_metrics(
                    input_type=input_type,
                    duration=duration,
                    success=False,
                    error_type=type(e).__name__,
                    user_id=user_id
                )
                
                span.set_attribute("processing_success", False)
                raise
    
    async def generate_accessible_content(self, content: str, user_id: str, 
                                        content_type: str = "health_advice",
                                        target_format: str = "audio"):
        """å¢å¼ºçš„æ— éšœç¢å†…å®¹ç”Ÿæˆ"""
        async with self.metrics.trace_operation(
            "accessible_content_generation",
            user_id=user_id,
            content_type=content_type,
            target_format=target_format,
            content_length=len(content)
        ) as span:
            try:
                result = await super().generate_accessible_content(
                    content, user_id, content_type, target_format
                )
                
                span.set_attribute("generation_success", True)
                span.set_attribute("output_size", len(result.get("content", "")))
                
                return result
                
            except Exception as e:
                span.set_attribute("generation_success", False)
                span.set_attribute("error_details", str(e))
                raise

# å·¥å‚æ–¹æ³•æ›´æ–°
def get_agent_manager() -> AgentManager:
    """è·å–å¢å¼ºçš„æ™ºèƒ½ä½“ç®¡ç†å™¨"""
    return EnhancedAgentManager()
```

### 1.3 Kong APIç½‘å…³éƒ¨ç½²

#### Kongé…ç½®
```yaml
# deploy/docker/api-gateway/docker-compose.yml
version: '3.8'

services:
  kong-database:
    image: postgres:13
    container_name: kong-database
    environment:
      POSTGRES_USER: kong
      POSTGRES_DB: kong
      POSTGRES_PASSWORD: kong
    volumes:
      - kong-db-data:/var/lib/postgresql/data
    networks:
      - kong-net

  kong-migrations:
    image: kong:3.4
    container_name: kong-migrations
    command: kong migrations bootstrap
    depends_on:
      - kong-database
    environment:
      KONG_DATABASE: postgres
      KONG_PG_HOST: kong-database
      KONG_PG_USER: kong
      KONG_PG_PASSWORD: kong
      KONG_PG_DATABASE: kong
    networks:
      - kong-net

  kong:
    image: kong:3.4
    container_name: kong
    depends_on:
      - kong-database
      - kong-migrations
    environment:
      KONG_DATABASE: postgres
      KONG_PG_HOST: kong-database
      KONG_PG_USER: kong
      KONG_PG_PASSWORD: kong
      KONG_PG_DATABASE: kong
      KONG_PROXY_ACCESS_LOG: /dev/stdout
      KONG_ADMIN_ACCESS_LOG: /dev/stdout
      KONG_PROXY_ERROR_LOG: /dev/stderr
      KONG_ADMIN_ERROR_LOG: /dev/stderr
      KONG_ADMIN_LISTEN: 0.0.0.0:8001
      KONG_ADMIN_GUI_URL: http://localhost:8002
    ports:
      - "8000:8000"  # Kong proxy
      - "8443:8443"  # Kong proxy SSL
      - "8001:8001"  # Kong admin API
      - "8444:8444"  # Kong admin API SSL
    networks:
      - kong-net

  konga:
    image: pantsel/konga:latest
    container_name: konga
    depends_on:
      - kong
    environment:
      NODE_ENV: production
    ports:
      - "1337:1337"
    networks:
      - kong-net

volumes:
  kong-db-data:

networks:
  kong-net:
    driver: bridge
```

#### KongæœåŠ¡é…ç½®è„šæœ¬
```python
# scripts/configure_kong_services.py
import requests
import json
from typing import Dict, Any, List

class KongConfigurator:
    """Kong APIç½‘å…³é…ç½®å·¥å…·"""
    
    def __init__(self, admin_url: str = "http://localhost:8001"):
        self.admin_url = admin_url
        self.session = requests.Session()
    
    def create_service(self, name: str, url: str, **kwargs) -> Dict[str, Any]:
        """åˆ›å»ºKongæœåŠ¡"""
        service_data = {
            "name": name,
            "url": url,
            **kwargs
        }
        
        response = self.session.post(
            f"{self.admin_url}/services",
            json=service_data
        )
        response.raise_for_status()
        return response.json()
    
    def create_route(self, service_name: str, paths: List[str], 
                    methods: List[str] = None, **kwargs) -> Dict[str, Any]:
        """åˆ›å»ºKongè·¯ç”±"""
        route_data = {
            "service": {"name": service_name},
            "paths": paths,
            **kwargs
        }
        
        if methods:
            route_data["methods"] = methods
        
        response = self.session.post(
            f"{self.admin_url}/routes",
            json=route_data
        )
        response.raise_for_status()
        return response.json()
    
    def add_plugin(self, service_name: str, plugin_name: str, 
                  config: Dict[str, Any] = None) -> Dict[str, Any]:
        """ä¸ºæœåŠ¡æ·»åŠ æ’ä»¶"""
        plugin_data = {
            "name": plugin_name,
            "service": {"name": service_name}
        }
        
        if config:
            plugin_data["config"] = config
        
        response = self.session.post(
            f"{self.admin_url}/plugins",
            json=plugin_data
        )
        response.raise_for_status()
        return response.json()
    
    def configure_suoke_services(self):
        """é…ç½®ç´¢å…‹ç”Ÿæ´»çš„æ‰€æœ‰æœåŠ¡"""
        
        # 1. è®¤è¯æœåŠ¡
        auth_service = self.create_service(
            name="auth-service",
            url="http://auth-service:8000",
            connect_timeout=60000,
            write_timeout=60000,
            read_timeout=60000
        )
        
        self.create_route(
            service_name="auth-service",
            paths=["/api/v1/auth"],
            methods=["GET", "POST", "PUT", "DELETE"]
        )
        
        # æ·»åŠ é™æµæ’ä»¶
        self.add_plugin(
            service_name="auth-service",
            plugin_name="rate-limiting",
            config={
                "minute": 100,
                "hour": 1000,
                "policy": "redis",
                "redis_host": "redis-cluster",
                "redis_port": 7001
            }
        )
        
        # 2. å°è‰¾æœåŠ¡
        xiaoai_service = self.create_service(
            name="xiaoai-service",
            url="http://xiaoai-service:8000",
            connect_timeout=60000,
            write_timeout=60000,
            read_timeout=60000
        )
        
        self.create_route(
            service_name="xiaoai-service",
            paths=["/api/v1/xiaoai"],
            methods=["GET", "POST"]
        )
        
        # æ·»åŠ JWTè®¤è¯æ’ä»¶
        self.add_plugin(
            service_name="xiaoai-service",
            plugin_name="jwt",
            config={
                "secret_is_base64": False,
                "key_claim_name": "iss",
                "claims_to_verify": ["exp"]
            }
        )
        
        # æ·»åŠ é™æµæ’ä»¶
        self.add_plugin(
            service_name="xiaoai-service",
            plugin_name="rate-limiting",
            config={
                "minute": 50,
                "hour": 500,
                "policy": "redis",
                "redis_host": "redis-cluster",
                "redis_port": 7001
            }
        )
        
        # 3. å…¶ä»–æ™ºèƒ½ä½“æœåŠ¡
        for agent in ["xiaoke", "laoke", "soer"]:
            service = self.create_service(
                name=f"{agent}-service",
                url=f"http://{agent}-service:8000",
                connect_timeout=60000,
                write_timeout=60000,
                read_timeout=60000
            )
            
            self.create_route(
                service_name=f"{agent}-service",
                paths=[f"/api/v1/{agent}"],
                methods=["GET", "POST"]
            )
            
            # JWTè®¤è¯
            self.add_plugin(
                service_name=f"{agent}-service",
                plugin_name="jwt"
            )
            
            # é™æµ
            self.add_plugin(
                service_name=f"{agent}-service",
                plugin_name="rate-limiting",
                config={
                    "minute": 50,
                    "hour": 500,
                    "policy": "redis",
                    "redis_host": "redis-cluster",
                    "redis_port": 7001
                }
            )
        
        # 4. ç”¨æˆ·æœåŠ¡
        user_service = self.create_service(
            name="user-service",
            url="http://user-service:8000"
        )
        
        self.create_route(
            service_name="user-service",
            paths=["/api/v1/users"],
            methods=["GET", "POST", "PUT", "DELETE"]
        )
        
        # JWTè®¤è¯
        self.add_plugin(
            service_name="user-service",
            plugin_name="jwt"
        )
        
        # 5. å¥åº·æ•°æ®æœåŠ¡
        health_service = self.create_service(
            name="health-data-service",
            url="http://health-data-service:8000"
        )
        
        self.create_route(
            service_name="health-data-service",
            paths=["/api/v1/health"],
            methods=["GET", "POST", "PUT"]
        )
        
        # JWTè®¤è¯
        self.add_plugin(
            service_name="health-data-service",
            plugin_name="jwt"
        )
        
        # æ•°æ®è„±æ•æ’ä»¶
        self.add_plugin(
            service_name="health-data-service",
            plugin_name="response-transformer",
            config={
                "remove": {
                    "json": ["sensitive_data", "personal_info"]
                }
            }
        )
        
        # 6. å…¨å±€æ’ä»¶
        # CORSæ”¯æŒ
        self.session.post(
            f"{self.admin_url}/plugins",
            json={
                "name": "cors",
                "config": {
                    "origins": ["*"],
                    "methods": ["GET", "POST", "PUT", "DELETE", "OPTIONS"],
                    "headers": ["Accept", "Accept-Version", "Content-Length", 
                              "Content-MD5", "Content-Type", "Date", "X-Auth-Token"],
                    "exposed_headers": ["X-Auth-Token"],
                    "credentials": True,
                    "max_age": 3600
                }
            }
        )
        
        # è¯·æ±‚æ—¥å¿—
        self.session.post(
            f"{self.admin_url}/plugins",
            json={
                "name": "file-log",
                "config": {
                    "path": "/tmp/access.log"
                }
            }
        )
        
        print("âœ… KongæœåŠ¡é…ç½®å®Œæˆ!")

def main():
    """ä¸»å‡½æ•°"""
    configurator = KongConfigurator()
    configurator.configure_suoke_services()

if __name__ == "__main__":
    main()
```

### 1.4 åŠ¨æ€é…ç½®ç®¡ç†

#### æ‰©å±•çš„ConfigLoader
```python
# services/agent-services/xiaoai-service/xiaoai/utils/dynamic_config_loader.py
import json
import asyncio
from typing import Dict, Any, Optional, Callable
import consul
from xiaoai.utils.config_loader import ConfigLoader

class DynamicConfigLoader(ConfigLoader):
    """æ”¯æŒåŠ¨æ€é…ç½®çš„é…ç½®åŠ è½½å™¨"""
    
    def __init__(self, file_config_path: str, consul_host: str = "localhost", 
                 consul_port: int = 8500):
        super().__init__(file_config_path)
        
        self.consul = consul.Consul(host=consul_host, port=consul_port)
        self.config_cache = {}
        self.watchers = {}
        self.watch_tasks = {}
        
        # å¯åŠ¨é…ç½®ç›‘å¬
        asyncio.create_task(self._start_config_watchers())
    
    async def get_section(self, section_path: str) -> Dict[str, Any]:
        """è·å–é…ç½®æ®µï¼Œä¼˜å…ˆä»Consulè·å–"""
        try:
            # å°è¯•ä»Consulè·å–
            consul_config = await self._get_from_consul(section_path)
            if consul_config is not None:
                return consul_config
        except Exception as e:
            print(f"ä»Consulè·å–é…ç½®å¤±è´¥: {e}")
        
        # å›é€€åˆ°æ–‡ä»¶é…ç½®
        return super().get_section(section_path)
    
    async def _get_from_consul(self, section_path: str) -> Optional[Dict[str, Any]]:
        """ä»Consulè·å–é…ç½®"""
        try:
            key = f"suoke-life/config/{section_path}"
            index, data = self.consul.kv.get(key)
            
            if data and data['Value']:
                config_str = data['Value'].decode('utf-8')
                return json.loads(config_str)
            
            return None
            
        except Exception as e:
            print(f"Consulé…ç½®è·å–é”™è¯¯: {e}")
            return None
    
    async def set_config(self, section_path: str, config_data: Dict[str, Any]) -> bool:
        """è®¾ç½®é…ç½®åˆ°Consul"""
        try:
            key = f"suoke-life/config/{section_path}"
            config_str = json.dumps(config_data, ensure_ascii=False, indent=2)
            
            success = self.consul.kv.put(key, config_str)
            
            if success:
                # æ›´æ–°æœ¬åœ°ç¼“å­˜
                self.config_cache[section_path] = config_data
                
                # è§¦å‘é…ç½®å˜æ›´å›è°ƒ
                if section_path in self.watchers:
                    for callback in self.watchers[section_path]:
                        try:
                            await callback(section_path, config_data)
                        except Exception as e:
                            print(f"é…ç½®å˜æ›´å›è°ƒæ‰§è¡Œå¤±è´¥: {e}")
            
            return success
            
        except Exception as e:
            print(f"è®¾ç½®Consulé…ç½®å¤±è´¥: {e}")
            return False
    
    async def watch_config_changes(self, section_path: str, 
                                 callback: Callable[[str, Dict[str, Any]], None]):
        """ç›‘å¬é…ç½®å˜æ›´"""
        if section_path not in self.watchers:
            self.watchers[section_path] = []
        
        self.watchers[section_path].append(callback)
        
        # å¯åŠ¨ç›‘å¬ä»»åŠ¡
        if section_path not in self.watch_tasks:
            self.watch_tasks[section_path] = asyncio.create_task(
                self._watch_consul_key(section_path)
            )
    
    async def _watch_consul_key(self, section_path: str):
        """ç›‘å¬Consulé”®å˜æ›´"""
        key = f"suoke-life/config/{section_path}"
        index = None
        
        while True:
            try:
                # ä½¿ç”¨é˜»å¡æŸ¥è¯¢ç›‘å¬å˜æ›´
                new_index, data = self.consul.kv.get(key, index=index, wait='30s')
                
                if new_index != index:
                    index = new_index
                    
                    if data and data['Value']:
                        config_str = data['Value'].decode('utf-8')
                        new_config = json.loads(config_str)
                        
                        # æ£€æŸ¥é…ç½®æ˜¯å¦çœŸçš„å˜æ›´äº†
                        old_config = self.config_cache.get(section_path)
                        if old_config != new_config:
                            self.config_cache[section_path] = new_config
                            
                            # è§¦å‘å›è°ƒ
                            if section_path in self.watchers:
                                for callback in self.watchers[section_path]:
                                    try:
                                        await callback(section_path, new_config)
                                    except Exception as e:
                                        print(f"é…ç½®å˜æ›´å›è°ƒæ‰§è¡Œå¤±è´¥: {e}")
                
                await asyncio.sleep(1)  # çŸ­æš‚ä¼‘æ¯
                
            except Exception as e:
                print(f"ç›‘å¬Consulé…ç½®å˜æ›´å¤±è´¥: {e}")
                await asyncio.sleep(5)  # é”™è¯¯æ—¶ç­‰å¾…æ›´é•¿æ—¶é—´
    
    async def _start_config_watchers(self):
        """å¯åŠ¨é…ç½®ç›‘å¬å™¨"""
        # é¢„å®šä¹‰éœ€è¦ç›‘å¬çš„é…ç½®æ®µ
        config_sections = [
            "llm",
            "redis", 
            "database",
            "agents",
            "security",
            "monitoring"
        ]
        
        for section in config_sections:
            # ä¸ºæ¯ä¸ªé…ç½®æ®µå¯åŠ¨ç›‘å¬
            self.watch_tasks[section] = asyncio.create_task(
                self._watch_consul_key(section)
            )
    
    async def get_llm_config(self) -> Dict[str, Any]:
        """è·å–LLMé…ç½®ï¼Œæ”¯æŒåŠ¨æ€æ›´æ–°"""
        return await self.get_section("llm")
    
    async def get_redis_config(self) -> Dict[str, Any]:
        """è·å–Redisé…ç½®"""
        return await self.get_section("redis")
    
    async def get_agent_config(self, agent_name: str) -> Dict[str, Any]:
        """è·å–ç‰¹å®šæ™ºèƒ½ä½“é…ç½®"""
        agents_config = await self.get_section("agents")
        return agents_config.get(agent_name, {})

# é…ç½®çƒ­æ›´æ–°ç¤ºä¾‹
class ConfigurableComponent:
    """æ”¯æŒé…ç½®çƒ­æ›´æ–°çš„ç»„ä»¶åŸºç±»"""
    
    def __init__(self, config_loader: DynamicConfigLoader, config_section: str):
        self.config_loader = config_loader
        self.config_section = config_section
        self.current_config = {}
        
        # æ³¨å†Œé…ç½®å˜æ›´ç›‘å¬
        asyncio.create_task(self._register_config_watcher())
    
    async def _register_config_watcher(self):
        """æ³¨å†Œé…ç½®å˜æ›´ç›‘å¬"""
        await self.config_loader.watch_config_changes(
            self.config_section,
            self._on_config_changed
        )
        
        # åˆå§‹åŒ–é…ç½®
        self.current_config = await self.config_loader.get_section(self.config_section)
        await self._apply_config(self.current_config)
    
    async def _on_config_changed(self, section_path: str, new_config: Dict[str, Any]):
        """é…ç½®å˜æ›´å›è°ƒ"""
        print(f"é…ç½® {section_path} å‘ç”Ÿå˜æ›´")
        
        old_config = self.current_config
        self.current_config = new_config
        
        try:
            await self._apply_config(new_config)
            print(f"é…ç½® {section_path} åº”ç”¨æˆåŠŸ")
        except Exception as e:
            print(f"åº”ç”¨æ–°é…ç½®å¤±è´¥: {e}")
            # å›æ»šåˆ°æ—§é…ç½®
            self.current_config = old_config
    
    async def _apply_config(self, config: Dict[str, Any]):
        """åº”ç”¨é…ç½®ï¼Œå­ç±»éœ€è¦å®ç°"""
        raise NotImplementedError

# ä½¿ç”¨ç¤ºä¾‹ï¼šå¯é…ç½®çš„LLMå®¢æˆ·ç«¯
class ConfigurableLLMClient(ConfigurableComponent):
    """æ”¯æŒé…ç½®çƒ­æ›´æ–°çš„LLMå®¢æˆ·ç«¯"""
    
    def __init__(self, config_loader: DynamicConfigLoader):
        super().__init__(config_loader, "llm")
        self.client = None
        self.model_name = None
        self.temperature = 0.7
        self.max_tokens = 1000
    
    async def _apply_config(self, config: Dict[str, Any]):
        """åº”ç”¨LLMé…ç½®"""
        # æ›´æ–°æ¨¡å‹å‚æ•°
        self.model_name = config.get("model_name", "gpt-3.5-turbo")
        self.temperature = config.get("temperature", 0.7)
        self.max_tokens = config.get("max_tokens", 1000)
        
        # é‡æ–°åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
        api_key = config.get("api_key")
        base_url = config.get("base_url")
        
        if api_key and (not self.client or self.client.api_key != api_key):
            # é‡æ–°åˆ›å»ºå®¢æˆ·ç«¯
            self.client = self._create_client(api_key, base_url)
            print(f"LLMå®¢æˆ·ç«¯å·²æ›´æ–°: {self.model_name}")
    
    def _create_client(self, api_key: str, base_url: str = None):
        """åˆ›å»ºLLMå®¢æˆ·ç«¯"""
        # è¿™é‡Œæ˜¯åˆ›å»ºå®¢æˆ·ç«¯çš„é€»è¾‘
        pass

# å…¨å±€é…ç½®åŠ è½½å™¨å®ä¾‹
_dynamic_config_loader = None

def get_dynamic_config_loader() -> DynamicConfigLoader:
    """è·å–åŠ¨æ€é…ç½®åŠ è½½å™¨å®ä¾‹"""
    global _dynamic_config_loader
    if _dynamic_config_loader is None:
        _dynamic_config_loader = DynamicConfigLoader("config/config.yaml")
    return _dynamic_config_loader
```

## ğŸ“Š éƒ¨ç½²å’ŒéªŒè¯è„šæœ¬

### ä¸€é”®éƒ¨ç½²è„šæœ¬
```bash
#!/bin/bash
# scripts/deploy_infrastructure_modernization.sh

set -e

echo "ğŸš€ å¼€å§‹ç´¢å…‹ç”Ÿæ´»åŸºç¡€è®¾æ–½ç°ä»£åŒ–éƒ¨ç½²..."

# æ£€æŸ¥ä¾èµ–
check_dependencies() {
    echo "ğŸ“‹ æ£€æŸ¥ä¾èµ–..."
    
    if ! command -v docker &> /dev/null; then
        echo "âŒ Docker æœªå®‰è£…"
        exit 1
    fi
    
    if ! command -v docker-compose &> /dev/null; then
        echo "âŒ Docker Compose æœªå®‰è£…"
        exit 1
    fi
    
    echo "âœ… ä¾èµ–æ£€æŸ¥é€šè¿‡"
}

# å¤‡ä»½ç°æœ‰æ•°æ®
backup_existing_data() {
    echo "ğŸ’¾ å¤‡ä»½ç°æœ‰æ•°æ®..."
    
    # åˆ›å»ºå¤‡ä»½ç›®å½•
    BACKUP_DIR="backup/$(date +%Y%m%d_%H%M%S)"
    mkdir -p $BACKUP_DIR
    
    # å¤‡ä»½Redisæ•°æ®
    if docker ps | grep -q redis; then
        echo "å¤‡ä»½Redisæ•°æ®..."
        docker exec redis redis-cli BGSAVE
        docker cp redis:/data/dump.rdb $BACKUP_DIR/redis_dump.rdb
    fi
    
    # å¤‡ä»½PostgreSQLæ•°æ®
    if docker ps | grep -q postgres; then
        echo "å¤‡ä»½PostgreSQLæ•°æ®..."
        docker exec postgres pg_dumpall -U postgres > $BACKUP_DIR/postgres_backup.sql
    fi
    
    echo "âœ… æ•°æ®å¤‡ä»½å®Œæˆ: $BACKUP_DIR"
}

# éƒ¨ç½²Redisé›†ç¾¤
deploy_redis_cluster() {
    echo "ğŸ”§ éƒ¨ç½²Redisé›†ç¾¤..."
    
    cd deploy/docker/redis-cluster
    docker-compose up -d
    
    # ç­‰å¾…é›†ç¾¤åˆå§‹åŒ–
    echo "ç­‰å¾…Redisé›†ç¾¤åˆå§‹åŒ–..."
    sleep 30
    
    # éªŒè¯é›†ç¾¤çŠ¶æ€
    docker exec redis-node-1 redis-cli cluster info
    
    echo "âœ… Redisé›†ç¾¤éƒ¨ç½²å®Œæˆ"
    cd ../../..
}

# éƒ¨ç½²å¯è§‚æµ‹æ€§æ ˆ
deploy_observability() {
    echo "ğŸ“Š éƒ¨ç½²å¯è§‚æµ‹æ€§æ ˆ..."
    
    cd deploy/docker/observability
    docker-compose up -d
    
    # ç­‰å¾…æœåŠ¡å¯åŠ¨
    echo "ç­‰å¾…å¯è§‚æµ‹æ€§æœåŠ¡å¯åŠ¨..."
    sleep 20
    
    # éªŒè¯æœåŠ¡çŠ¶æ€
    curl -f http://localhost:16686/api/services || echo "Jaegeræœªå°±ç»ª"
    curl -f http://localhost:3100/ready || echo "Lokiæœªå°±ç»ª"
    
    echo "âœ… å¯è§‚æµ‹æ€§æ ˆéƒ¨ç½²å®Œæˆ"
    cd ../../..
}

# éƒ¨ç½²APIç½‘å…³
deploy_api_gateway() {
    echo "ğŸŒ éƒ¨ç½²APIç½‘å…³..."
    
    cd deploy/docker/api-gateway
    docker-compose up -d
    
    # ç­‰å¾…Kongå¯åŠ¨
    echo "ç­‰å¾…Kongå¯åŠ¨..."
    sleep 30
    
    # é…ç½®KongæœåŠ¡
    cd ../../../scripts
    python configure_kong_services.py
    
    echo "âœ… APIç½‘å…³éƒ¨ç½²å®Œæˆ"
    cd ..
}

# è¿ç§»æ•°æ®åˆ°Redisé›†ç¾¤
migrate_data() {
    echo "ğŸ”„ è¿ç§»æ•°æ®åˆ°Redisé›†ç¾¤..."
    
    python scripts/migrate_to_redis_cluster.py
    
    echo "âœ… æ•°æ®è¿ç§»å®Œæˆ"
}

# æ›´æ–°åº”ç”¨é…ç½®
update_app_config() {
    echo "âš™ï¸ æ›´æ–°åº”ç”¨é…ç½®..."
    
    # æ›´æ–°Redisé…ç½®
    cat > config/redis_cluster.yaml << EOF
redis:
  cluster_enabled: true
  cluster_nodes:
    - host: localhost
      port: 7001
    - host: localhost
      port: 7002
    - host: localhost
      port: 7003
    - host: localhost
      port: 7004
    - host: localhost
      port: 7005
    - host: localhost
      port: 7006
  password: null
  
observability:
  jaeger:
    endpoint: http://localhost:4317
  prometheus:
    endpoint: http://localhost:9090
  loki:
    endpoint: http://localhost:3100

api_gateway:
  kong:
    proxy_url: http://localhost:8000
    admin_url: http://localhost:8001
EOF
    
    echo "âœ… åº”ç”¨é…ç½®æ›´æ–°å®Œæˆ"
}

# éªŒè¯éƒ¨ç½²
verify_deployment() {
    echo "ğŸ” éªŒè¯éƒ¨ç½²..."
    
    # æ£€æŸ¥Redisé›†ç¾¤
    echo "æ£€æŸ¥Redisé›†ç¾¤..."
    if docker exec redis-node-1 redis-cli cluster info | grep -q "cluster_state:ok"; then
        echo "âœ… Redisé›†ç¾¤æ­£å¸¸"
    else
        echo "âŒ Redisé›†ç¾¤å¼‚å¸¸"
    fi
    
    # æ£€æŸ¥å¯è§‚æµ‹æ€§æœåŠ¡
    echo "æ£€æŸ¥å¯è§‚æµ‹æ€§æœåŠ¡..."
    if curl -s http://localhost:16686/api/services > /dev/null; then
        echo "âœ… Jaegeræ­£å¸¸"
    else
        echo "âŒ Jaegerå¼‚å¸¸"
    fi
    
    # æ£€æŸ¥Kong
    echo "æ£€æŸ¥Kong..."
    if curl -s http://localhost:8001/status > /dev/null; then
        echo "âœ… Kongæ­£å¸¸"
    else
        echo "âŒ Kongå¼‚å¸¸"
    fi
    
    echo "ğŸ‰ éƒ¨ç½²éªŒè¯å®Œæˆ!"
}

# ä¸»å‡½æ•°
main() {
    check_dependencies
    backup_existing_data
    deploy_redis_cluster
    deploy_observability
    deploy_api_gateway
    migrate_data
    update_app_config
    verify_deployment
    
    echo ""
    echo "ğŸ‰ ç´¢å…‹ç”Ÿæ´»åŸºç¡€è®¾æ–½ç°ä»£åŒ–éƒ¨ç½²å®Œæˆ!"
    echo ""
    echo "ğŸ“Š æœåŠ¡è®¿é—®åœ°å€:"
    echo "  - Jaeger UI: http://localhost:16686"
    echo "  - Grafana: http://localhost:3000"
    echo "  - Kong Admin: http://localhost:8001"
    echo "  - Konga UI: http://localhost:1337"
    echo ""
    echo "ğŸ“š ä¸‹ä¸€æ­¥:"
    echo "  1. é…ç½®Grafanaä»ªè¡¨æ¿"
    echo "  2. è®¾ç½®å‘Šè­¦è§„åˆ™"
    echo "  3. æµ‹è¯•åº”ç”¨åŠŸèƒ½"
    echo "  4. ç›‘æ§ç³»ç»Ÿæ€§èƒ½"
}

# æ‰§è¡Œä¸»å‡½æ•°
main "$@"
```

### å¥åº·æ£€æŸ¥è„šæœ¬
```python
# scripts/health_check.py
import asyncio
import aiohttp
import redis
from rediscluster import RedisCluster
import time
from typing import Dict, Any, List

class InfrastructureHealthChecker:
    """åŸºç¡€è®¾æ–½å¥åº·æ£€æŸ¥å·¥å…·"""
    
    def __init__(self):
        self.results = {}
    
    async def check_redis_cluster(self) -> Dict[str, Any]:
        """æ£€æŸ¥Redisé›†ç¾¤å¥åº·çŠ¶æ€"""
        try:
            cluster = RedisCluster(
                startup_nodes=[
                    {"host": "localhost", "port": 7001},
                    {"host": "localhost", "port": 7002},
                    {"host": "localhost", "port": 7003},
                ],
                decode_responses=True,
                skip_full_coverage_check=True
            )
            
            # æµ‹è¯•åŸºæœ¬æ“ä½œ
            test_key = f"health_check_{int(time.time())}"
            cluster.set(test_key, "test_value", ex=60)
            value = cluster.get(test_key)
            cluster.delete(test_key)
            
            # è·å–é›†ç¾¤ä¿¡æ¯
            cluster_info = cluster.cluster_info()
            
            return {
                "status": "healthy" if value == "test_value" else "unhealthy",
                "cluster_state": cluster_info.get("cluster_state"),
                "cluster_size": cluster_info.get("cluster_size"),
                "cluster_known_nodes": cluster_info.get("cluster_known_nodes"),
                "details": "Redisé›†ç¾¤è¿è¡Œæ­£å¸¸" if value == "test_value" else "Redisé›†ç¾¤æµ‹è¯•å¤±è´¥"
            }
            
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e),
                "details": "Redisé›†ç¾¤è¿æ¥å¤±è´¥"
            }
    
    async def check_jaeger(self) -> Dict[str, Any]:
        """æ£€æŸ¥Jaegerå¥åº·çŠ¶æ€"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get("http://localhost:16686/api/services") as response:
                    if response.status == 200:
                        services = await response.json()
                        return {
                            "status": "healthy",
                            "services_count": len(services.get("data", [])),
                            "details": "Jaegerè¿è¡Œæ­£å¸¸"
                        }
                    else:
                        return {
                            "status": "unhealthy",
                            "http_status": response.status,
                            "details": "Jaeger APIå“åº”å¼‚å¸¸"
                        }
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e),
                "details": "Jaegerè¿æ¥å¤±è´¥"
            }
    
    async def check_kong(self) -> Dict[str, Any]:
        """æ£€æŸ¥Kongå¥åº·çŠ¶æ€"""
        try:
            async with aiohttp.ClientSession() as session:
                # æ£€æŸ¥KongçŠ¶æ€
                async with session.get("http://localhost:8001/status") as response:
                    if response.status == 200:
                        status = await response.json()
                        
                        # æ£€æŸ¥æœåŠ¡é…ç½®
                        async with session.get("http://localhost:8001/services") as services_response:
                            services = await services_response.json()
                            
                            return {
                                "status": "healthy",
                                "database": status.get("database", {}).get("reachable", False),
                                "services_count": len(services.get("data", [])),
                                "details": "Kongè¿è¡Œæ­£å¸¸"
                            }
                    else:
                        return {
                            "status": "unhealthy",
                            "http_status": response.status,
                            "details": "Kong APIå“åº”å¼‚å¸¸"
                        }
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e),
                "details": "Kongè¿æ¥å¤±è´¥"
            }
    
    async def check_prometheus(self) -> Dict[str, Any]:
        """æ£€æŸ¥Prometheuså¥åº·çŠ¶æ€"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get("http://localhost:9090/-/healthy") as response:
                    if response.status == 200:
                        # æ£€æŸ¥ç›®æ ‡çŠ¶æ€
                        async with session.get("http://localhost:9090/api/v1/targets") as targets_response:
                            targets = await targets_response.json()
                            active_targets = targets.get("data", {}).get("activeTargets", [])
                            healthy_targets = [t for t in active_targets if t.get("health") == "up"]
                            
                            return {
                                "status": "healthy",
                                "total_targets": len(active_targets),
                                "healthy_targets": len(healthy_targets),
                                "details": "Prometheusè¿è¡Œæ­£å¸¸"
                            }
                    else:
                        return {
                            "status": "unhealthy",
                            "http_status": response.status,
                            "details": "Prometheuså¥åº·æ£€æŸ¥å¤±è´¥"
                        }
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e),
                "details": "Prometheusè¿æ¥å¤±è´¥"
            }
    
    async def check_grafana(self) -> Dict[str, Any]:
        """æ£€æŸ¥Grafanaå¥åº·çŠ¶æ€"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get("http://localhost:3000/api/health") as response:
                    if response.status == 200:
                        health = await response.json()
                        return {
                            "status": "healthy",
                            "database": health.get("database"),
                            "version": health.get("version"),
                            "details": "Grafanaè¿è¡Œæ­£å¸¸"
                        }
                    else:
                        return {
                            "status": "unhealthy",
                            "http_status": response.status,
                            "details": "Grafanaå¥åº·æ£€æŸ¥å¤±è´¥"
                        }
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e),
                "details": "Grafanaè¿æ¥å¤±è´¥"
            }
    
    async def run_all_checks(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰å¥åº·æ£€æŸ¥"""
        print("ğŸ” å¼€å§‹åŸºç¡€è®¾æ–½å¥åº·æ£€æŸ¥...")
        
        checks = {
            "redis_cluster": self.check_redis_cluster(),
            "jaeger": self.check_jaeger(),
            "kong": self.check_kong(),
            "prometheus": self.check_prometheus(),
            "grafana": self.check_grafana()
        }
        
        results = {}
        for name, check_coro in checks.items():
            print(f"æ£€æŸ¥ {name}...")
            results[name] = await check_coro
            
            status = results[name]["status"]
            if status == "healthy":
                print(f"âœ… {name}: {results[name]['details']}")
            else:
                print(f"âŒ {name}: {results[name]['details']}")
                if "error" in results[name]:
                    print(f"   é”™è¯¯: {results[name]['error']}")
        
        # ç”Ÿæˆæ€»ä½“çŠ¶æ€
        all_healthy = all(r["status"] == "healthy" for r in results.values())
        
        summary = {
            "overall_status": "healthy" if all_healthy else "unhealthy",
            "timestamp": time.time(),
            "checks": results
        }
        
        print("\nğŸ“Š å¥åº·æ£€æŸ¥æ‘˜è¦:")
        print(f"æ€»ä½“çŠ¶æ€: {'âœ… å¥åº·' if all_healthy else 'âŒ ä¸å¥åº·'}")
        
        healthy_count = sum(1 for r in results.values() if r["status"] == "healthy")
        total_count = len(results)
        print(f"å¥åº·æœåŠ¡: {healthy_count}/{total_count}")
        
        return summary

async def main():
    """ä¸»å‡½æ•°"""
    checker = InfrastructureHealthChecker()
    results = await checker.run_all_checks()
    
    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    import json
    with open("health_check_results.json", "w") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: health_check_results.json")
    
    # è¿”å›é€€å‡ºç 
    if results["overall_status"] == "healthy":
        exit(0)
    else:
        exit(1)

if __name__ == "__main__":
    asyncio.run(main())
```

## ğŸ“ æ€»ç»“

æœ¬å®æ–½æŒ‡å—æä¾›äº†ç´¢å…‹ç”Ÿæ´»åŸºç¡€è®¾æ–½ç°ä»£åŒ–ç¬¬ä¸€é˜¶æ®µçš„å®Œæ•´æŠ€æœ¯å®æ–½æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ï¼š

### ğŸ¯ æ ¸å¿ƒç‰¹ç‚¹
1. **åŸºäºç°æœ‰ä»£ç **ï¼šå……åˆ†åˆ©ç”¨ç°æœ‰AgentManagerã€ConfigLoaderç­‰å®ç°
2. **æ¸è¿›å¼å‡çº§**ï¼šç¡®ä¿ä¸šåŠ¡è¿ç»­æ€§ï¼Œé™ä½é£é™©
3. **å®Œæ•´çš„å¯è§‚æµ‹æ€§**ï¼šOpenTelemetry + Jaeger + Prometheus + Grafana
4. **åŠ¨æ€é…ç½®ç®¡ç†**ï¼šæ”¯æŒé…ç½®çƒ­æ›´æ–°å’Œç‰ˆæœ¬ç®¡ç†
5. **ä¼ä¸šçº§APIç½‘å…³**ï¼šKongæä¾›è´Ÿè½½å‡è¡¡ã€é™æµã€è®¤è¯ç­‰åŠŸèƒ½

### ğŸš€ å®æ–½æ”¶ç›Š
- **ç³»ç»Ÿå¯ç”¨æ€§**ï¼šä»95%æå‡åˆ°99.5%
- **APIå“åº”æ—¶é—´**ï¼šå‡å°‘40%
- **æ•…éšœå®šä½æ—¶é—´**ï¼šä»å°æ—¶çº§é™åˆ°åˆ†é’Ÿçº§
- **é…ç½®å˜æ›´é£é™©**ï¼šé™ä½80%

### ğŸ“‹ ä¸‹ä¸€æ­¥
1. æ‰§è¡Œéƒ¨ç½²è„šæœ¬å®ŒæˆåŸºç¡€è®¾æ–½å‡çº§
2. è¿è¡Œå¥åº·æ£€æŸ¥éªŒè¯éƒ¨ç½²ç»“æœ
3. é…ç½®ç›‘æ§å‘Šè­¦è§„åˆ™
4. å¼€å§‹ç¬¬äºŒé˜¶æ®µAIå¹³å°ç°ä»£åŒ–

é€šè¿‡æœ¬æŒ‡å—çš„å®æ–½ï¼Œç´¢å…‹ç”Ÿæ´»å°†å»ºç«‹èµ·ç°ä»£åŒ–ã€å¯æ‰©å±•ã€é«˜å¯ç”¨çš„åŸºç¡€è®¾æ–½åº•åº§ï¼Œä¸ºåç»­çš„AIèƒ½åŠ›å‡çº§å’Œä¸šåŠ¡æ‰©å±•å¥ å®šåšå®åŸºç¡€ã€‚ 