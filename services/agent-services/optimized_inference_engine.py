#!/usr/bin/env python3
"""
ç´¢å…‹ç”Ÿæ´» - ä¼˜åŒ–åçš„AIæ¨ç†å¼•æ“
å®ç°è·¨è¿›ç¨‹å†…å­˜éš”ç¦»ã€å¼‚æ­¥I/Oå’ŒJITç¼–è¯‘ä¼˜åŒ–
"""

import asyncio
import time
import multiprocessing
import numpy as np
import psutil
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
from multiprocessing import shared_memory, Queue, Manager
from typing import List, Dict, Any, Optional, Union, Callable
import json
import logging
from dataclasses import dataclass, asdict
from numba import jit, cuda
import pickle
import threading
from contextlib import asynccontextmanager
from abc import ABC, abstractmethod
import uuid
from datetime import datetime
import aiohttp
import asyncpg
from functools import lru_cache, wraps


# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class InferenceRequest:
    """æ¨ç†è¯·æ±‚æ•°æ®ç»“æ„"""
    request_id: str
    agent_type: str  # xiaoai, xiaoke, laoke, soer
    input_data: Dict[str, Any]
    priority: int = 1  # 1-10, 10ä¸ºæœ€é«˜ä¼˜å…ˆçº§
    timeout: float = 30.0
    created_at: datetime = None
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.now()


@dataclass
class InferenceResult:
    """æ¨ç†ç»“æœæ•°æ®ç»“æ„"""
    request_id: str
    agent_type: str
    result: Dict[str, Any]
    execution_time: float
    memory_usage: float
    success: bool
    error_message: Optional[str] = None
    completed_at: datetime = None
    
    def __post_init__(self):
        if self.completed_at is None:
            self.completed_at = datetime.now()


class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'average_response_time': 0.0,
            'peak_memory_usage': 0.0,
            'cpu_utilization': 0.0
        }
        self.request_times = []
        self.lock = threading.Lock()
    
    def record_request(self, execution_time: float, memory_usage: float, success: bool):
        """è®°å½•è¯·æ±‚æ€§èƒ½æŒ‡æ ‡"""
        with self.lock:
            self.metrics['total_requests'] += 1
            if success:
                self.metrics['successful_requests'] += 1
            else:
                self.metrics['failed_requests'] += 1
            
            self.request_times.append(execution_time)
            if len(self.request_times) > 1000:  # ä¿æŒæœ€è¿‘1000ä¸ªè¯·æ±‚çš„è®°å½•
                self.request_times.pop(0)
            
            self.metrics['average_response_time'] = sum(self.request_times) / len(self.request_times)
            self.metrics['peak_memory_usage'] = max(self.metrics['peak_memory_usage'], memory_usage)
            self.metrics['cpu_utilization'] = psutil.cpu_percent()
    
    def get_metrics(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŒ‡æ ‡"""
        with self.lock:
            return self.metrics.copy()


class JITOptimizedAlgorithms:
    """JITä¼˜åŒ–çš„æ ¸å¿ƒç®—æ³•"""
    
    @staticmethod
    @jit(nopython=True, cache=True)
    def tcm_syndrome_analysis(symptoms: np.ndarray, weights: np.ndarray) -> np.ndarray:
        """ä¸­åŒ»è¾¨è¯åˆ†æç®—æ³• - JITä¼˜åŒ–ç‰ˆæœ¬"""
        result = np.zeros(symptoms.shape[0])
        for i in range(symptoms.shape[0]):
            score = 0.0
            for j in range(symptoms.shape[1]):
                score += symptoms[i, j] * weights[j]
            result[i] = score
        return result
    
    @staticmethod
    @jit(nopython=True, cache=True)
    def health_risk_calculation(biomarkers: np.ndarray, risk_factors: np.ndarray) -> float:
        """å¥åº·é£é™©è®¡ç®—ç®—æ³• - JITä¼˜åŒ–ç‰ˆæœ¬"""
        risk_score = 0.0
        for i in range(len(biomarkers)):
            risk_score += biomarkers[i] * risk_factors[i]
        return min(max(risk_score, 0.0), 1.0)  # é™åˆ¶åœ¨0-1èŒƒå›´å†…
    
    @staticmethod
    @jit(nopython=True, cache=True)
    def personalized_recommendation_scoring(user_profile: np.ndarray, 
                                          recommendations: np.ndarray) -> np.ndarray:
        """ä¸ªæ€§åŒ–æ¨èè¯„åˆ†ç®—æ³• - JITä¼˜åŒ–ç‰ˆæœ¬"""
        scores = np.zeros(recommendations.shape[0])
        for i in range(recommendations.shape[0]):
            similarity = 0.0
            for j in range(len(user_profile)):
                similarity += user_profile[j] * recommendations[i, j]
            scores[i] = similarity
        return scores
    
    @staticmethod
    @jit(nopython=True, cache=True)
    def multi_agent_consensus(agent_outputs: np.ndarray, confidence_weights: np.ndarray) -> np.ndarray:
        """å¤šæ™ºèƒ½ä½“å…±è¯†ç®—æ³• - JITä¼˜åŒ–ç‰ˆæœ¬"""
        weighted_sum = np.zeros(agent_outputs.shape[1])
        total_weight = 0.0
        
        for i in range(agent_outputs.shape[0]):
            weight = confidence_weights[i]
            total_weight += weight
            for j in range(agent_outputs.shape[1]):
                weighted_sum[j] += agent_outputs[i, j] * weight
        
        if total_weight > 0:
            for j in range(len(weighted_sum)):
                weighted_sum[j] /= total_weight
        
        return weighted_sum


class SharedMemoryManager:
    """å…±äº«å†…å­˜ç®¡ç†å™¨"""
    
    def __init__(self):
        self.shared_arrays = {}
        self.lock = threading.Lock()
    
    def create_shared_array(self, name: str, shape: tuple, dtype=np.float32) -> np.ndarray:
        """åˆ›å»ºå…±äº«å†…å­˜æ•°ç»„"""
        with self.lock:
            if name in self.shared_arrays:
                return self.shared_arrays[name]['array']
            
            size = np.prod(shape) * np.dtype(dtype).itemsize
            shm = shared_memory.SharedMemory(create=True, size=size, name=name)
            array = np.ndarray(shape, dtype=dtype, buffer=shm.buf)
            
            self.shared_arrays[name] = {
                'shm': shm,
                'array': array,
                'shape': shape,
                'dtype': dtype
            }
            
            return array
    
    def get_shared_array(self, name: str) -> Optional[np.ndarray]:
        """è·å–å…±äº«å†…å­˜æ•°ç»„"""
        with self.lock:
            if name in self.shared_arrays:
                return self.shared_arrays[name]['array']
            return None
    
    def cleanup(self):
        """æ¸…ç†å…±äº«å†…å­˜"""
        with self.lock:
            for name, info in self.shared_arrays.items():
                try:
                    info['shm'].close()
                    info['shm'].unlink()
                except:
                    pass
            self.shared_arrays.clear()


class AsyncDatabasePool:
    """å¼‚æ­¥æ•°æ®åº“è¿æ¥æ± """
    
    def __init__(self, database_url: str, min_size: int = 5, max_size: int = 20):
        self.database_url = database_url
        self.min_size = min_size
        self.max_size = max_size
        self.pool = None
    
    async def initialize(self):
        """åˆå§‹åŒ–è¿æ¥æ± """
        self.pool = await asyncpg.create_pool(
            self.database_url,
            min_size=self.min_size,
            max_size=self.max_size
        )
    
    @asynccontextmanager
    async def acquire(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        async with self.pool.acquire() as connection:
            yield connection
    
    async def close(self):
        """å…³é—­è¿æ¥æ± """
        if self.pool:
            await self.pool.close()


class OptimizedInferenceEngine:
    """ä¼˜åŒ–åçš„AIæ¨ç†å¼•æ“"""
    
    def __init__(self, 
                 max_workers: Optional[int] = None,
                 enable_gpu: bool = False,
                 database_url: Optional[str] = None):
        self.max_workers = max_workers or multiprocessing.cpu_count()
        self.enable_gpu = enable_gpu and cuda.is_available()
        
        # è¿›ç¨‹æ± å’Œçº¿ç¨‹æ± 
        self.cpu_pool = ProcessPoolExecutor(max_workers=self.max_workers)
        self.io_pool = ThreadPoolExecutor(max_workers=self.max_workers * 2)
        
        # æ€§èƒ½ç›‘æ§
        self.monitor = PerformanceMonitor()
        
        # å…±äº«å†…å­˜ç®¡ç†
        self.shared_memory = SharedMemoryManager()
        
        # å¼‚æ­¥æ•°æ®åº“è¿æ¥æ± 
        self.db_pool = AsyncDatabasePool(database_url) if database_url else None
        
        # è¯·æ±‚é˜Ÿåˆ—å’Œç»“æœç¼“å­˜
        self.request_queue = asyncio.Queue(maxsize=1000)
        self.result_cache = {}
        self.cache_lock = asyncio.Lock()
        
        # JITä¼˜åŒ–ç®—æ³•å®ä¾‹
        self.algorithms = JITOptimizedAlgorithms()
        
        logger.info(f"ä¼˜åŒ–æ¨ç†å¼•æ“åˆå§‹åŒ–å®Œæˆ - CPUæ ¸å¿ƒ: {self.max_workers}, GPU: {self.enable_gpu}")
    
    async def initialize(self):
        """å¼‚æ­¥åˆå§‹åŒ–"""
        if self.db_pool:
            await self.db_pool.initialize()
        
        # é¢„çƒ­JITç¼–è¯‘
        await self._warmup_jit_algorithms()
        
        logger.info("æ¨ç†å¼•æ“å¼‚æ­¥åˆå§‹åŒ–å®Œæˆ")
    
    async def _warmup_jit_algorithms(self):
        """é¢„çƒ­JITç¼–è¯‘ç®—æ³•"""
        logger.info("å¼€å§‹é¢„çƒ­JITç¼–è¯‘ç®—æ³•...")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_symptoms = np.random.rand(100, 50).astype(np.float32)
        test_weights = np.random.rand(50).astype(np.float32)
        test_biomarkers = np.random.rand(20).astype(np.float32)
        test_risk_factors = np.random.rand(20).astype(np.float32)
        
        # é¢„çƒ­ç®—æ³•
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(
            self.cpu_pool,
            self.algorithms.tcm_syndrome_analysis,
            test_symptoms,
            test_weights
        )
        
        await loop.run_in_executor(
            self.cpu_pool,
            self.algorithms.health_risk_calculation,
            test_biomarkers,
            test_risk_factors
        )
        
        logger.info("JITç¼–è¯‘ç®—æ³•é¢„çƒ­å®Œæˆ")
    
    async def submit_inference_request(self, request: InferenceRequest) -> str:
        """æäº¤æ¨ç†è¯·æ±‚"""
        await self.request_queue.put(request)
        logger.info(f"æ¨ç†è¯·æ±‚å·²æäº¤: {request.request_id}")
        return request.request_id
    
    async def get_inference_result(self, request_id: str, timeout: float = 30.0) -> Optional[InferenceResult]:
        """è·å–æ¨ç†ç»“æœ"""
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            async with self.cache_lock:
                if request_id in self.result_cache:
                    result = self.result_cache.pop(request_id)
                    return result
            
            await asyncio.sleep(0.1)  # é¿å…å¿™ç­‰å¾…
        
        return None  # è¶…æ—¶
    
    async def process_inference_batch(self, requests: List[InferenceRequest]) -> List[InferenceResult]:
        """æ‰¹é‡å¤„ç†æ¨ç†è¯·æ±‚"""
        tasks = []
        for request in requests:
            task = asyncio.create_task(self._process_single_inference(request))
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†å¼‚å¸¸
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                error_result = InferenceResult(
                    request_id=requests[i].request_id,
                    agent_type=requests[i].agent_type,
                    result={},
                    execution_time=0.0,
                    memory_usage=0.0,
                    success=False,
                    error_message=str(result)
                )
                processed_results.append(error_result)
            else:
                processed_results.append(result)
        
        return processed_results
    
    async def _process_single_inference(self, request: InferenceRequest) -> InferenceResult:
        """å¤„ç†å•ä¸ªæ¨ç†è¯·æ±‚"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        try:
            # æ ¹æ®æ™ºèƒ½ä½“ç±»å‹é€‰æ‹©å¤„ç†ç­–ç•¥
            if request.agent_type == "xiaoai":
                result = await self._process_xiaoai_inference(request)
            elif request.agent_type == "xiaoke":
                result = await self._process_xiaoke_inference(request)
            elif request.agent_type == "laoke":
                result = await self._process_laoke_inference(request)
            elif request.agent_type == "soer":
                result = await self._process_soer_inference(request)
            else:
                raise ValueError(f"æœªçŸ¥çš„æ™ºèƒ½ä½“ç±»å‹: {request.agent_type}")
            
            execution_time = time.time() - start_time
            end_memory = psutil.Process().memory_info().rss / 1024 / 1024
            memory_usage = end_memory - start_memory
            
            # è®°å½•æ€§èƒ½æŒ‡æ ‡
            self.monitor.record_request(execution_time, memory_usage, True)
            
            inference_result = InferenceResult(
                request_id=request.request_id,
                agent_type=request.agent_type,
                result=result,
                execution_time=execution_time,
                memory_usage=memory_usage,
                success=True
            )
            
            # ç¼“å­˜ç»“æœ
            async with self.cache_lock:
                self.result_cache[request.request_id] = inference_result
            
            return inference_result
            
        except Exception as e:
            execution_time = time.time() - start_time
            end_memory = psutil.Process().memory_info().rss / 1024 / 1024
            memory_usage = end_memory - start_memory
            
            # è®°å½•æ€§èƒ½æŒ‡æ ‡
            self.monitor.record_request(execution_time, memory_usage, False)
            
            logger.error(f"æ¨ç†è¯·æ±‚å¤„ç†å¤±è´¥: {request.request_id}, é”™è¯¯: {str(e)}")
            
            return InferenceResult(
                request_id=request.request_id,
                agent_type=request.agent_type,
                result={},
                execution_time=execution_time,
                memory_usage=memory_usage,
                success=False,
                error_message=str(e)
            )
    
    async def _process_xiaoai_inference(self, request: InferenceRequest) -> Dict[str, Any]:
        """å¤„ç†å°è‰¾ï¼ˆAIæ¨ç†ï¼‰çš„æ¨ç†è¯·æ±‚"""
        input_data = request.input_data
        
        # CPUå¯†é›†å‹æ¨ç†ä½¿ç”¨è¿›ç¨‹æ± 
        loop = asyncio.get_event_loop()
        
        if "symptoms" in input_data and "weights" in input_data:
            # ä¸­åŒ»è¾¨è¯åˆ†æ
            symptoms = np.array(input_data["symptoms"], dtype=np.float32)
            weights = np.array(input_data["weights"], dtype=np.float32)
            
            syndrome_scores = await loop.run_in_executor(
                self.cpu_pool,
                self.algorithms.tcm_syndrome_analysis,
                symptoms,
                weights
            )
            
            return {
                "syndrome_analysis": syndrome_scores.tolist(),
                "confidence": float(np.max(syndrome_scores)),
                "recommended_treatment": self._generate_treatment_recommendation(syndrome_scores)
            }
        
        # é»˜è®¤å¤„ç†
        return {"message": "å°è‰¾æ¨ç†å®Œæˆ", "timestamp": datetime.now().isoformat()}
    
    async def _process_xiaoke_inference(self, request: InferenceRequest) -> Dict[str, Any]:
        """å¤„ç†å°å…‹ï¼ˆè¯Šæ–­åˆ†æï¼‰çš„æ¨ç†è¯·æ±‚"""
        input_data = request.input_data
        
        loop = asyncio.get_event_loop()
        
        if "biomarkers" in input_data and "risk_factors" in input_data:
            # å¥åº·é£é™©è¯„ä¼°
            biomarkers = np.array(input_data["biomarkers"], dtype=np.float32)
            risk_factors = np.array(input_data["risk_factors"], dtype=np.float32)
            
            risk_score = await loop.run_in_executor(
                self.cpu_pool,
                self.algorithms.health_risk_calculation,
                biomarkers,
                risk_factors
            )
            
            return {
                "health_risk_score": float(risk_score),
                "risk_level": self._categorize_risk_level(risk_score),
                "recommendations": self._generate_health_recommendations(risk_score)
            }
        
        return {"message": "å°å…‹è¯Šæ–­å®Œæˆ", "timestamp": datetime.now().isoformat()}
    
    async def _process_laoke_inference(self, request: InferenceRequest) -> Dict[str, Any]:
        """å¤„ç†è€å…‹ï¼ˆçŸ¥è¯†ç®¡ç†ï¼‰çš„æ¨ç†è¯·æ±‚"""
        input_data = request.input_data
        
        # I/Oå¯†é›†å‹ä»»åŠ¡ä½¿ç”¨å¼‚æ­¥å¤„ç†
        if "query" in input_data:
            knowledge_results = await self._async_knowledge_search(input_data["query"])
            return {
                "knowledge_results": knowledge_results,
                "source_count": len(knowledge_results),
                "timestamp": datetime.now().isoformat()
            }
        
        return {"message": "è€å…‹çŸ¥è¯†æ£€ç´¢å®Œæˆ", "timestamp": datetime.now().isoformat()}
    
    async def _process_soer_inference(self, request: InferenceRequest) -> Dict[str, Any]:
        """å¤„ç†ç´¢å„¿ï¼ˆç”¨æˆ·äº¤äº’ï¼‰çš„æ¨ç†è¯·æ±‚"""
        input_data = request.input_data
        
        if "user_profile" in input_data and "recommendations" in input_data:
            # ä¸ªæ€§åŒ–æ¨è
            loop = asyncio.get_event_loop()
            
            user_profile = np.array(input_data["user_profile"], dtype=np.float32)
            recommendations = np.array(input_data["recommendations"], dtype=np.float32)
            
            scores = await loop.run_in_executor(
                self.cpu_pool,
                self.algorithms.personalized_recommendation_scoring,
                user_profile,
                recommendations
            )
            
            return {
                "recommendation_scores": scores.tolist(),
                "top_recommendations": self._get_top_recommendations(scores, input_data.get("items", [])),
                "personalization_confidence": float(np.std(scores))
            }
        
        return {"message": "ç´¢å„¿ä¸ªæ€§åŒ–æœåŠ¡å®Œæˆ", "timestamp": datetime.now().isoformat()}
    
    async def _async_knowledge_search(self, query: str) -> List[Dict[str, Any]]:
        """å¼‚æ­¥çŸ¥è¯†æœç´¢"""
        # æ¨¡æ‹Ÿå¼‚æ­¥æ•°æ®åº“æŸ¥è¯¢
        await asyncio.sleep(0.1)  # æ¨¡æ‹ŸI/Oå»¶è¿Ÿ
        
        # å¦‚æœæœ‰æ•°æ®åº“è¿æ¥æ± ï¼Œä½¿ç”¨çœŸå®æŸ¥è¯¢
        if self.db_pool:
            try:
                async with self.db_pool.acquire() as conn:
                    results = await conn.fetch(
                        "SELECT * FROM knowledge_base WHERE content ILIKE $1 LIMIT 10",
                        f"%{query}%"
                    )
                    return [dict(row) for row in results]
            except Exception as e:
                logger.error(f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
        
        # æ¨¡æ‹ŸçŸ¥è¯†æœç´¢ç»“æœ
        return [
            {"id": i, "title": f"çŸ¥è¯†æ¡ç›® {i}", "content": f"å…³äº {query} çš„çŸ¥è¯†å†…å®¹ {i}"}
            for i in range(5)
        ]
    
    def _generate_treatment_recommendation(self, syndrome_scores: np.ndarray) -> List[str]:
        """ç”Ÿæˆæ²»ç–—å»ºè®®"""
        max_index = np.argmax(syndrome_scores)
        treatments = [
            "æ¸…çƒ­è§£æ¯’", "å¥è„¾ç›Šæ°”", "æ»‹é˜´æ¶¦ç‡¥", "æ¸©é˜³æ•£å¯’", "æ´»è¡€åŒ–ç˜€"
        ]
        return [treatments[max_index % len(treatments)]]
    
    def _categorize_risk_level(self, risk_score: float) -> str:
        """åˆ†ç±»é£é™©ç­‰çº§"""
        if risk_score < 0.3:
            return "ä½é£é™©"
        elif risk_score < 0.7:
            return "ä¸­é£é™©"
        else:
            return "é«˜é£é™©"
    
    def _generate_health_recommendations(self, risk_score: float) -> List[str]:
        """ç”Ÿæˆå¥åº·å»ºè®®"""
        if risk_score < 0.3:
            return ["ä¿æŒè‰¯å¥½ç”Ÿæ´»ä¹ æƒ¯", "å®šæœŸä½“æ£€"]
        elif risk_score < 0.7:
            return ["è°ƒæ•´é¥®é£Ÿç»“æ„", "å¢åŠ è¿åŠ¨é‡", "å®šæœŸç›‘æµ‹"]
        else:
            return ["ç«‹å³å°±åŒ»", "ä¸¥æ ¼æ§åˆ¶é¥®é£Ÿ", "å¯†åˆ‡ç›‘æµ‹å¥åº·æŒ‡æ ‡"]
    
    def _get_top_recommendations(self, scores: np.ndarray, items: List[Any]) -> List[Dict[str, Any]]:
        """è·å–topæ¨è"""
        if not items:
            return []
        
        top_indices = np.argsort(scores)[-5:][::-1]  # å–å‰5ä¸ª
        return [
            {"item": items[i] if i < len(items) else f"æ¨èé¡¹ç›® {i}", "score": float(scores[i])}
            for i in top_indices
        ]
    
    async def get_performance_metrics(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŒ‡æ ‡"""
        return self.monitor.get_metrics()
    
    async def shutdown(self):
        """å…³é—­æ¨ç†å¼•æ“"""
        logger.info("æ­£åœ¨å…³é—­æ¨ç†å¼•æ“...")
        
        # å…³é—­è¿›ç¨‹æ± å’Œçº¿ç¨‹æ± 
        self.cpu_pool.shutdown(wait=True)
        self.io_pool.shutdown(wait=True)
        
        # å…³é—­æ•°æ®åº“è¿æ¥æ± 
        if self.db_pool:
            await self.db_pool.close()
        
        # æ¸…ç†å…±äº«å†…å­˜
        self.shared_memory.cleanup()
        
        logger.info("æ¨ç†å¼•æ“å·²å…³é—­")


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•å‡½æ•°
async def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºä¼˜åŒ–åçš„æ¨ç†å¼•æ“"""
    # åˆå§‹åŒ–æ¨ç†å¼•æ“
    engine = OptimizedInferenceEngine(
        max_workers=multiprocessing.cpu_count(),
        enable_gpu=False,  # æ ¹æ®éœ€è¦å¯ç”¨GPU
        database_url=None  # å¦‚æœæœ‰æ•°æ®åº“ï¼Œæä¾›è¿æ¥å­—ç¬¦ä¸²
    )
    
    await engine.initialize()
    
    try:
        # åˆ›å»ºæµ‹è¯•è¯·æ±‚
        requests = [
            InferenceRequest(
                request_id=str(uuid.uuid4()),
                agent_type="xiaoai",
                input_data={
                    "symptoms": np.random.rand(10, 20).tolist(),
                    "weights": np.random.rand(20).tolist()
                }
            ),
            InferenceRequest(
                request_id=str(uuid.uuid4()),
                agent_type="xiaoke",
                input_data={
                    "biomarkers": np.random.rand(15).tolist(),
                    "risk_factors": np.random.rand(15).tolist()
                }
            ),
            InferenceRequest(
                request_id=str(uuid.uuid4()),
                agent_type="laoke",
                input_data={"query": "ä¸­åŒ»å…»ç”Ÿ"}
            ),
            InferenceRequest(
                request_id=str(uuid.uuid4()),
                agent_type="soer",
                input_data={
                    "user_profile": np.random.rand(10).tolist(),
                    "recommendations": np.random.rand(20, 10).tolist(),
                    "items": [f"æ¨èé¡¹ç›® {i}" for i in range(20)]
                }
            )
        ]
        
        # æ‰¹é‡å¤„ç†æ¨ç†è¯·æ±‚
        print("ğŸš€ å¼€å§‹æ‰¹é‡æ¨ç†æµ‹è¯•...")
        start_time = time.time()
        
        results = await engine.process_inference_batch(requests)
        
        end_time = time.time()
        
        # æ‰“å°ç»“æœ
        print(f"âœ… æ‰¹é‡æ¨ç†å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.3f}s")
        for result in results:
            print(f"  ğŸ“Š {result.agent_type}: {result.success}, è€—æ—¶: {result.execution_time:.3f}s")
        
        # è·å–æ€§èƒ½æŒ‡æ ‡
        metrics = await engine.get_performance_metrics()
        print(f"\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
        for key, value in metrics.items():
            print(f"  {key}: {value}")
    
    finally:
        await engine.shutdown()


if __name__ == "__main__":
    asyncio.run(main()) 