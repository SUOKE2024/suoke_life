#!/usr/bin/env python3
"""
ç´¢å…‹ç”Ÿæ´» - æ™ºèƒ½å†—ä½™æ–‡ä»¶æ¸…ç†å™¨
åŸºäºservicesç°æœ‰ä»£ç ç»“æ„åŠå…·ä½“å®ç°ï¼Œæ´å¯Ÿå†—ä½™æ–‡ä»¶å¹¶æ¸…ç†

åŠŸèƒ½ï¼š
1. è¯†åˆ«å¤‡ä»½æ–‡ä»¶ (.backup, .backup_advanced, .backup_priority)
2. è¯†åˆ«ç©ºå ä½ç¬¦æ–‡ä»¶
3. è¯†åˆ«é‡å¤çš„é…ç½®æ–‡ä»¶
4. è¯†åˆ«æœªä½¿ç”¨çš„ä¸´æ—¶æ–‡ä»¶
5. ç”Ÿæˆæ¸…ç†æŠ¥å‘Š
"""

import hashlib
import json
import os
import shutil
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Set, Tuple


class IntelligentRedundancyCleaner:
    """æ™ºèƒ½å†—ä½™æ–‡ä»¶æ¸…ç†å™¨"""

    def __init__(self, services_root: str = "services"):
        self.services_root = Path(services_root)
        self.redundant_files = {
            "backup_files": [],
            "placeholder_files": [],
            "duplicate_files": [],
            "empty_files": [],
            "temp_files": [],
            "cache_files": [],
        }
        self.file_hashes = {}
        self.cleanup_stats = {
            "total_files_scanned": 0,
            "total_size_saved": 0,
            "files_removed": 0,
        }

    def scan_services(self) -> Dict:
        """æ‰«æservicesç›®å½•è¯†åˆ«å†—ä½™æ–‡ä»¶"""
        print("ğŸ” å¼€å§‹æ‰«æservicesç›®å½•...")

        for root, dirs, files in os.walk(self.services_root):
            # è·³è¿‡è™šæ‹Ÿç¯å¢ƒå’Œç¼“å­˜ç›®å½•
            dirs[:] = [
                d
                for d in dirs
                if not d.startswith(
                    (
                        ".venv",
                        "__pycache__",
                        ".pytest_cache",
                        ".mypy_cache",
                        ".ruff_cache",
                        "htmlcov",
                        ".benchmarks",
                    )
                )
            ]

            for file in files:
                file_path = Path(root) / file
                self.cleanup_stats["total_files_scanned"] += 1

                # æ£€æŸ¥å¤‡ä»½æ–‡ä»¶
                if self._is_backup_file(file):
                    self.redundant_files["backup_files"].append(str(file_path))

                # æ£€æŸ¥å ä½ç¬¦æ–‡ä»¶
                elif self._is_placeholder_file(file_path):
                    self.redundant_files["placeholder_files"].append(str(file_path))

                # æ£€æŸ¥ç©ºæ–‡ä»¶
                elif self._is_empty_file(file_path):
                    self.redundant_files["empty_files"].append(str(file_path))

                # æ£€æŸ¥ä¸´æ—¶æ–‡ä»¶
                elif self._is_temp_file(file):
                    self.redundant_files["temp_files"].append(str(file_path))

                # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶
                elif self._is_cache_file(file):
                    self.redundant_files["cache_files"].append(str(file_path))

                # è®¡ç®—æ–‡ä»¶å“ˆå¸Œç”¨äºé‡å¤æ£€æµ‹
                else:
                    self._calculate_file_hash(file_path)

        # è¯†åˆ«é‡å¤æ–‡ä»¶
        self._find_duplicate_files()

        return self.redundant_files

    def _is_backup_file(self, filename: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºå¤‡ä»½æ–‡ä»¶"""
        backup_patterns = [
            ".backup",
            ".backup_advanced",
            ".backup_priority",
            ".bak",
            ".orig",
            ".old",
            ".tmp",
        ]
        return any(filename.endswith(pattern) for pattern in backup_patterns)

    def _is_placeholder_file(self, file_path: Path) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºå ä½ç¬¦æ–‡ä»¶"""
        try:
            if file_path.stat().st_size > 1000:  # å¤§äº1KBçš„æ–‡ä»¶ä¸å¤ªå¯èƒ½æ˜¯å ä½ç¬¦
                return False

            with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                content = f.read().strip()

            # æ£€æŸ¥å¸¸è§çš„å ä½ç¬¦æ¨¡å¼
            placeholder_patterns = [
                '"""Module placeholder"""',
                'def main():\n    """Main function placeholder"""',
                "pass",
                "# TODO",
                "# PLACEHOLDER",
            ]

            return (
                any(pattern in content for pattern in placeholder_patterns)
                and len(content) < 200
            )

        except Exception:
            return False

    def _is_empty_file(self, file_path: Path) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºç©ºæ–‡ä»¶"""
        try:
            return file_path.stat().st_size == 0
        except Exception:
            return False

    def _is_temp_file(self, filename: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºä¸´æ—¶æ–‡ä»¶"""
        temp_patterns = [
            ".tmp",
            ".temp",
            ".swp",
            ".swo",
            "~",
            "fix_",
            "test_quick",
            "demo_",
        ]
        return any(
            filename.startswith(pattern) or filename.endswith(pattern)
            for pattern in temp_patterns
        )

    def _is_cache_file(self, filename: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºç¼“å­˜æ–‡ä»¶"""
        cache_patterns = [
            ".coverage",
            "coverage.xml",
            ".pytest_cache",
            ".mypy_cache",
            ".ruff_cache",
            "htmlcov",
        ]
        return any(pattern in filename for pattern in cache_patterns)

    def _calculate_file_hash(self, file_path: Path):
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        try:
            if file_path.stat().st_size > 10 * 1024 * 1024:  # è·³è¿‡å¤§äº10MBçš„æ–‡ä»¶
                return

            with open(file_path, "rb") as f:
                content = f.read()
                file_hash = hashlib.md5(content).hexdigest()

                if file_hash not in self.file_hashes:
                    self.file_hashes[file_hash] = []
                self.file_hashes[file_hash].append(str(file_path))

        except Exception:
            pass

    def _find_duplicate_files(self):
        """æŸ¥æ‰¾é‡å¤æ–‡ä»¶"""
        for file_hash, file_paths in self.file_hashes.items():
            if len(file_paths) > 1:
                # ä¿ç•™ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼Œå…¶ä½™æ ‡è®°ä¸ºé‡å¤
                self.redundant_files["duplicate_files"].extend(file_paths[1:])

    def generate_cleanup_report(self) -> str:
        """ç”Ÿæˆæ¸…ç†æŠ¥å‘Š"""
        report = []
        report.append("# ç´¢å…‹ç”Ÿæ´» Services å†—ä½™æ–‡ä»¶æ¸…ç†æŠ¥å‘Š")
        report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")

        # ç»Ÿè®¡ä¿¡æ¯
        total_redundant = sum(len(files) for files in self.redundant_files.values())
        report.append("## ğŸ“Š æ¸…ç†ç»Ÿè®¡")
        report.append(f"- æ‰«ææ–‡ä»¶æ€»æ•°: {self.cleanup_stats['total_files_scanned']}")
        report.append(f"- å‘ç°å†—ä½™æ–‡ä»¶: {total_redundant}")
        report.append("")

        # è¯¦ç»†åˆ†ç±»
        for category, files in self.redundant_files.items():
            if files:
                report.append(
                    f"### {self._get_category_name(category)} ({len(files)}ä¸ª)"
                )
                for file_path in files[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                    report.append(f"- {file_path}")
                if len(files) > 10:
                    report.append(f"- ... è¿˜æœ‰ {len(files) - 10} ä¸ªæ–‡ä»¶")
                report.append("")

        return "\n".join(report)

    def _get_category_name(self, category: str) -> str:
        """è·å–åˆ†ç±»ä¸­æ–‡åç§°"""
        names = {
            "backup_files": "ğŸ”„ å¤‡ä»½æ–‡ä»¶",
            "placeholder_files": "ğŸ“ å ä½ç¬¦æ–‡ä»¶",
            "duplicate_files": "ğŸ” é‡å¤æ–‡ä»¶",
            "empty_files": "ğŸ“„ ç©ºæ–‡ä»¶",
            "temp_files": "ğŸ—‚ï¸ ä¸´æ—¶æ–‡ä»¶",
            "cache_files": "ğŸ’¾ ç¼“å­˜æ–‡ä»¶",
        }
        return names.get(category, category)

    def execute_cleanup(self, dry_run: bool = True) -> Dict:
        """æ‰§è¡Œæ¸…ç†æ“ä½œ"""
        cleanup_results = {
            "removed_files": [],
            "failed_removals": [],
            "total_size_saved": 0,
        }

        if dry_run:
            print("ğŸ” æ‰§è¡Œæ¨¡æ‹Ÿæ¸…ç† (dry-runæ¨¡å¼)")
        else:
            print("ğŸ—‘ï¸ æ‰§è¡Œå®é™…æ¸…ç†")

        for category, files in self.redundant_files.items():
            for file_path in files:
                try:
                    file_path_obj = Path(file_path)
                    if file_path_obj.exists():
                        file_size = file_path_obj.stat().st_size

                        if not dry_run:
                            if file_path_obj.is_file():
                                file_path_obj.unlink()
                            elif file_path_obj.is_dir():
                                shutil.rmtree(file_path_obj)

                        cleanup_results["removed_files"].append(
                            {"path": file_path, "category": category, "size": file_size}
                        )
                        cleanup_results["total_size_saved"] += file_size

                except Exception as e:
                    cleanup_results["failed_removals"].append(
                        {"path": file_path, "error": str(e)}
                    )

        return cleanup_results

    def save_cleanup_record(self, cleanup_results: Dict):
        """ä¿å­˜æ¸…ç†è®°å½•"""
        record = {
            "timestamp": datetime.now().isoformat(),
            "scan_results": self.redundant_files,
            "cleanup_results": cleanup_results,
            "stats": self.cleanup_stats,
        }

        record_file = self.services_root / "redundancy_cleanup_record.json"
        with open(record_file, "w", encoding="utf-8") as f:
            json.dump(record, f, ensure_ascii=False, indent=2)

        print(f"ğŸ“ æ¸…ç†è®°å½•å·²ä¿å­˜åˆ°: {record_file}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ç´¢å…‹ç”Ÿæ´» Services æ™ºèƒ½å†—ä½™æ–‡ä»¶æ¸…ç†å™¨")
    print("=" * 50)

    cleaner = IntelligentRedundancyCleaner()

    # æ‰«æå†—ä½™æ–‡ä»¶
    redundant_files = cleaner.scan_services()

    # ç”ŸæˆæŠ¥å‘Š
    report = cleaner.generate_cleanup_report()
    print(report)

    # ä¿å­˜æŠ¥å‘Š
    report_file = Path("services/REDUNDANCY_CLEANUP_REPORT.md")
    with open(report_file, "w", encoding="utf-8") as f:
        f.write(report)
    print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

    # è¯¢é—®æ˜¯å¦æ‰§è¡Œæ¸…ç†
    total_files = sum(len(files) for files in redundant_files.values())
    if total_files > 0:
        print(f"\nå‘ç° {total_files} ä¸ªå†—ä½™æ–‡ä»¶")

        # å…ˆæ‰§è¡Œæ¨¡æ‹Ÿæ¸…ç†
        print("\næ‰§è¡Œæ¨¡æ‹Ÿæ¸…ç†...")
        dry_run_results = cleaner.execute_cleanup(dry_run=True)

        size_mb = dry_run_results["total_size_saved"] / (1024 * 1024)
        print(f"é¢„è®¡å¯èŠ‚çœç©ºé—´: {size_mb:.2f} MB")

        # è¯¢é—®æ˜¯å¦æ‰§è¡Œå®é™…æ¸…ç†
        response = input("\næ˜¯å¦æ‰§è¡Œå®é™…æ¸…ç†? (y/N): ").strip().lower()
        if response == "y":
            cleanup_results = cleaner.execute_cleanup(dry_run=False)
            cleaner.save_cleanup_record(cleanup_results)
            print("âœ… æ¸…ç†å®Œæˆ!")
        else:
            print("â„¹ï¸ ä»…ç”ŸæˆæŠ¥å‘Šï¼Œæœªæ‰§è¡Œæ¸…ç†")
    else:
        print("âœ¨ æœªå‘ç°å†—ä½™æ–‡ä»¶ï¼Œservicesç›®å½•å¾ˆå¹²å‡€!")


if __name__ == "__main__":
    main()
