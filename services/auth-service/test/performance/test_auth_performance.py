#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®¤è¯æœåŠ¡æ€§èƒ½åŸºå‡†æµ‹è¯•
"""
import asyncio
import time
import statistics
from typing import List
import pytest
from unittest.mock import AsyncMock, MagicMock, patch

from internal.service.auth_service import authenticate_user, create_tokens, verify_token
from internal.model.user import User


class TestAuthPerformance:
    """è®¤è¯æœåŠ¡æ€§èƒ½æµ‹è¯•"""
    
    @pytest.fixture
    def mock_user(self):
        """æ¨¡æ‹Ÿç”¨æˆ·å¯¹è±¡"""
        user = MagicMock(spec=User)
        user.id = "test-user-id"
        user.username = "testuser"
        user.email = "test@example.com"
        user.is_active = True
        return user
    
    @pytest.fixture
    def performance_settings(self):
        """æ€§èƒ½æµ‹è¯•é…ç½®"""
        return {
            'concurrent_users': 100,
            'requests_per_user': 10,
            'max_response_time': 0.1,  # 100ms
            'target_throughput': 1000,  # 1000 RPS
        }
    
    async def measure_execution_time(self, func, *args, **kwargs):
        """æµ‹é‡å‡½æ•°æ‰§è¡Œæ—¶é—´"""
        start_time = time.perf_counter()
        result = await func(*args, **kwargs)
        end_time = time.perf_counter()
        return result, end_time - start_time
    
    async def run_concurrent_requests(self, func, args_list: List, concurrency: int = 10):
        """è¿è¡Œå¹¶å‘è¯·æ±‚"""
        semaphore = asyncio.Semaphore(concurrency)
        
        async def limited_request(args):
            async with semaphore:
                return await self.measure_execution_time(func, *args)
        
        tasks = [limited_request(args) for args in args_list]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # åˆ†ç¦»æˆåŠŸå’Œå¤±è´¥çš„ç»“æœ
        successful_results = []
        execution_times = []
        errors = []
        
        for result in results:
            if isinstance(result, Exception):
                errors.append(result)
            else:
                _, exec_time = result
                successful_results.append(result)
                execution_times.append(exec_time)
        
        return successful_results, execution_times, errors
    
    @pytest.mark.asyncio
    @pytest.mark.performance
    async def test_authenticate_user_performance(self, mock_user, performance_settings):
        """æµ‹è¯•ç”¨æˆ·è®¤è¯æ€§èƒ½"""
        with patch('internal.service.auth_service.UserRepository') as mock_repo_class, \
             patch('internal.service.auth_service.PasswordManager') as mock_password_class, \
             patch('internal.service.auth_service.cache') as mock_cache, \
             patch('internal.service.auth_service.query_optimizer') as mock_optimizer:
            
            # è®¾ç½®æ¨¡æ‹Ÿ
            mock_cache.get.return_value = None
            mock_optimizer.execute_optimized_query.return_value = [dict(mock_user.__dict__)]
            
            mock_password_manager = AsyncMock()
            mock_password_manager.verify_password.return_value = True
            mock_password_class.return_value = mock_password_manager
            
            # å‡†å¤‡æµ‹è¯•æ•°æ®
            test_args = [("testuser", "password123") for _ in range(performance_settings['concurrent_users'])]
            
            # æ‰§è¡Œå¹¶å‘æµ‹è¯•
            results, execution_times, errors = await self.run_concurrent_requests(
                authenticate_user, 
                test_args, 
                concurrency=50
            )
            
            # æ€§èƒ½åˆ†æ
            if execution_times:
                avg_time = statistics.mean(execution_times)
                p95_time = statistics.quantiles(execution_times, n=20)[18]  # 95th percentile
                p99_time = statistics.quantiles(execution_times, n=100)[98]  # 99th percentile
                max_time = max(execution_times)
                min_time = min(execution_times)
                
                print(f"\nğŸ” ç”¨æˆ·è®¤è¯æ€§èƒ½æµ‹è¯•ç»“æœ:")
                print(f"   ğŸ“Š æ€»è¯·æ±‚æ•°: {len(test_args)}")
                print(f"   âœ… æˆåŠŸè¯·æ±‚: {len(results)}")
                print(f"   âŒ å¤±è´¥è¯·æ±‚: {len(errors)}")
                print(f"   â±ï¸  å¹³å‡å“åº”æ—¶é—´: {avg_time:.3f}s")
                print(f"   ğŸ“ˆ P95å“åº”æ—¶é—´: {p95_time:.3f}s")
                print(f"   ğŸ“ˆ P99å“åº”æ—¶é—´: {p99_time:.3f}s")
                print(f"   ğŸ”º æœ€å¤§å“åº”æ—¶é—´: {max_time:.3f}s")
                print(f"   ğŸ”» æœ€å°å“åº”æ—¶é—´: {min_time:.3f}s")
                
                # æ€§èƒ½æ–­è¨€
                assert avg_time < performance_settings['max_response_time'], \
                    f"å¹³å‡å“åº”æ—¶é—´ {avg_time:.3f}s è¶…è¿‡ç›®æ ‡ {performance_settings['max_response_time']}s"
                assert p95_time < performance_settings['max_response_time'] * 2, \
                    f"P95å“åº”æ—¶é—´ {p95_time:.3f}s è¶…è¿‡ç›®æ ‡ {performance_settings['max_response_time'] * 2}s"
                assert len(errors) == 0, f"å­˜åœ¨ {len(errors)} ä¸ªé”™è¯¯"
    
    @pytest.mark.asyncio
    @pytest.mark.performance
    async def test_create_tokens_performance(self, mock_user, performance_settings):
        """æµ‹è¯•åˆ›å»ºä»¤ç‰Œæ€§èƒ½"""
        with patch('internal.service.auth_service.create_jwt_token') as mock_create_jwt, \
             patch('internal.service.auth_service.settings') as mock_settings:
            
            mock_create_jwt.side_effect = lambda *args, **kwargs: f"token_{time.time()}"
            mock_settings.jwt_access_token_expire_minutes = 30
            mock_settings.jwt_refresh_token_expire_days = 7
            
            # å‡†å¤‡æµ‹è¯•æ•°æ®
            test_args = [(mock_user,) for _ in range(performance_settings['concurrent_users'])]
            
            # æ‰§è¡Œå¹¶å‘æµ‹è¯•
            results, execution_times, errors = await self.run_concurrent_requests(
                create_tokens, 
                test_args, 
                concurrency=50
            )
            
            # æ€§èƒ½åˆ†æ
            if execution_times:
                avg_time = statistics.mean(execution_times)
                throughput = len(results) / sum(execution_times) if execution_times else 0
                
                print(f"\nğŸ” åˆ›å»ºä»¤ç‰Œæ€§èƒ½æµ‹è¯•ç»“æœ:")
                print(f"   ğŸ“Š æ€»è¯·æ±‚æ•°: {len(test_args)}")
                print(f"   âœ… æˆåŠŸè¯·æ±‚: {len(results)}")
                print(f"   âŒ å¤±è´¥è¯·æ±‚: {len(errors)}")
                print(f"   â±ï¸  å¹³å‡å“åº”æ—¶é—´: {avg_time:.3f}s")
                print(f"   ğŸš€ ååé‡: {throughput:.1f} RPS")
                
                # æ€§èƒ½æ–­è¨€
                assert avg_time < 0.05, f"ä»¤ç‰Œåˆ›å»ºå¹³å‡æ—¶é—´ {avg_time:.3f}s è¿‡é•¿"
                assert len(errors) == 0, f"å­˜åœ¨ {len(errors)} ä¸ªé”™è¯¯"
    
    @pytest.mark.asyncio
    @pytest.mark.performance
    async def test_verify_token_performance(self, mock_user, performance_settings):
        """æµ‹è¯•éªŒè¯ä»¤ç‰Œæ€§èƒ½"""
        with patch('jwt.decode') as mock_decode, \
             patch('internal.service.auth_service.jwt_key_manager') as mock_jwt_manager:
            
            mock_decode.return_value = {
                "sub": "test-user-id",
                "type": "access",
                "exp": time.time() + 3600
            }
            
            mock_user_repo = AsyncMock()
            mock_user_repo.get_by_id.return_value = mock_user
            
            # å‡†å¤‡æµ‹è¯•æ•°æ®
            test_args = [("test_token", mock_user_repo) for _ in range(performance_settings['concurrent_users'])]
            
            # æ‰§è¡Œå¹¶å‘æµ‹è¯•
            results, execution_times, errors = await self.run_concurrent_requests(
                verify_token, 
                test_args, 
                concurrency=100  # ä»¤ç‰ŒéªŒè¯åº”è¯¥æ”¯æŒæ›´é«˜å¹¶å‘
            )
            
            # æ€§èƒ½åˆ†æ
            if execution_times:
                avg_time = statistics.mean(execution_times)
                throughput = len(results) / sum(execution_times) if execution_times else 0
                
                print(f"\nğŸ” éªŒè¯ä»¤ç‰Œæ€§èƒ½æµ‹è¯•ç»“æœ:")
                print(f"   ğŸ“Š æ€»è¯·æ±‚æ•°: {len(test_args)}")
                print(f"   âœ… æˆåŠŸè¯·æ±‚: {len(results)}")
                print(f"   âŒ å¤±è´¥è¯·æ±‚: {len(errors)}")
                print(f"   â±ï¸  å¹³å‡å“åº”æ—¶é—´: {avg_time:.3f}s")
                print(f"   ğŸš€ ååé‡: {throughput:.1f} RPS")
                
                # æ€§èƒ½æ–­è¨€
                assert avg_time < 0.02, f"ä»¤ç‰ŒéªŒè¯å¹³å‡æ—¶é—´ {avg_time:.3f}s è¿‡é•¿"
                assert throughput > 1000, f"ååé‡ {throughput:.1f} RPS ä½äºç›®æ ‡ 1000 RPS"
                assert len(errors) == 0, f"å­˜åœ¨ {len(errors)} ä¸ªé”™è¯¯"
    
    @pytest.mark.asyncio
    @pytest.mark.performance
    async def test_memory_usage(self, mock_user):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        with patch('internal.service.auth_service.create_jwt_token') as mock_create_jwt:
            mock_create_jwt.side_effect = lambda *args, **kwargs: f"token_{time.time()}"
            
            # æ‰§è¡Œå¤§é‡ä»¤ç‰Œåˆ›å»ºæ“ä½œ
            tasks = []
            for i in range(1000):
                task = create_tokens(mock_user)
                tasks.append(task)
            
            await asyncio.gather(*tasks)
            
            final_memory = process.memory_info().rss / 1024 / 1024  # MB
            memory_increase = final_memory - initial_memory
            
            print(f"\nğŸ” å†…å­˜ä½¿ç”¨æµ‹è¯•ç»“æœ:")
            print(f"   ğŸ“Š åˆå§‹å†…å­˜: {initial_memory:.1f} MB")
            print(f"   ğŸ“Š æœ€ç»ˆå†…å­˜: {final_memory:.1f} MB")
            print(f"   ğŸ“ˆ å†…å­˜å¢é•¿: {memory_increase:.1f} MB")
            
            # å†…å­˜ä½¿ç”¨æ–­è¨€
            assert memory_increase < 100, f"å†…å­˜å¢é•¿ {memory_increase:.1f} MB è¿‡å¤š"
    
    @pytest.mark.asyncio
    @pytest.mark.performance
    async def test_stress_test(self, mock_user, performance_settings):
        """å‹åŠ›æµ‹è¯•"""
        with patch('internal.service.auth_service.UserRepository') as mock_repo_class, \
             patch('internal.service.auth_service.PasswordManager') as mock_password_class, \
             patch('internal.service.auth_service.cache') as mock_cache, \
             patch('internal.service.auth_service.query_optimizer') as mock_optimizer:
            
            # è®¾ç½®æ¨¡æ‹Ÿ
            mock_cache.get.return_value = None
            mock_optimizer.execute_optimized_query.return_value = [dict(mock_user.__dict__)]
            
            mock_password_manager = AsyncMock()
            mock_password_manager.verify_password.return_value = True
            mock_password_class.return_value = mock_password_manager
            
            # å‹åŠ›æµ‹è¯•å‚æ•°
            stress_users = 500
            stress_requests = 2000
            
            print(f"\nğŸ”¥ å¼€å§‹å‹åŠ›æµ‹è¯•: {stress_users} å¹¶å‘ç”¨æˆ·, {stress_requests} æ€»è¯·æ±‚")
            
            # å‡†å¤‡æµ‹è¯•æ•°æ®
            test_args = [("testuser", "password123") for _ in range(stress_requests)]
            
            start_time = time.perf_counter()
            
            # æ‰§è¡Œå‹åŠ›æµ‹è¯•
            results, execution_times, errors = await self.run_concurrent_requests(
                authenticate_user, 
                test_args, 
                concurrency=stress_users
            )
            
            total_time = time.perf_counter() - start_time
            
            # å‹åŠ›æµ‹è¯•åˆ†æ
            success_rate = len(results) / len(test_args) * 100
            throughput = len(results) / total_time
            
            print(f"\nğŸ” å‹åŠ›æµ‹è¯•ç»“æœ:")
            print(f"   ğŸ“Š æ€»è¯·æ±‚æ•°: {len(test_args)}")
            print(f"   âœ… æˆåŠŸè¯·æ±‚: {len(results)}")
            print(f"   âŒ å¤±è´¥è¯·æ±‚: {len(errors)}")
            print(f"   ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")
            print(f"   â±ï¸  æ€»è€—æ—¶: {total_time:.2f}s")
            print(f"   ğŸš€ ååé‡: {throughput:.1f} RPS")
            
            if execution_times:
                avg_time = statistics.mean(execution_times)
                p95_time = statistics.quantiles(execution_times, n=20)[18]
                print(f"   â±ï¸  å¹³å‡å“åº”æ—¶é—´: {avg_time:.3f}s")
                print(f"   ğŸ“ˆ P95å“åº”æ—¶é—´: {p95_time:.3f}s")
            
            # å‹åŠ›æµ‹è¯•æ–­è¨€
            assert success_rate >= 95, f"æˆåŠŸç‡ {success_rate:.1f}% ä½äº 95%"
            assert throughput >= 500, f"ååé‡ {throughput:.1f} RPS ä½äº 500 RPS"


class TestCachePerformance:
    """ç¼“å­˜æ€§èƒ½æµ‹è¯•"""
    
    @pytest.mark.asyncio
    @pytest.mark.performance
    async def test_cache_hit_performance(self):
        """æµ‹è¯•ç¼“å­˜å‘½ä¸­æ€§èƒ½"""
        with patch('internal.service.auth_service.cache') as mock_cache:
            # æ¨¡æ‹Ÿç¼“å­˜å‘½ä¸­
            mock_cache.get.return_value = {
                "id": "test-user-id",
                "username": "testuser",
                "email": "test@example.com",
                "is_active": True
            }
            
            start_time = time.perf_counter()
            
            # æ‰§è¡Œå¤šæ¬¡ç¼“å­˜æŸ¥è¯¢
            for _ in range(1000):
                await mock_cache.get("user_auth:testuser")
            
            end_time = time.perf_counter()
            avg_time = (end_time - start_time) / 1000
            
            print(f"\nğŸ” ç¼“å­˜æ€§èƒ½æµ‹è¯•ç»“æœ:")
            print(f"   â±ï¸  å¹³å‡ç¼“å­˜æŸ¥è¯¢æ—¶é—´: {avg_time:.6f}s")
            
            # ç¼“å­˜æ€§èƒ½æ–­è¨€
            assert avg_time < 0.001, f"ç¼“å­˜æŸ¥è¯¢æ—¶é—´ {avg_time:.6f}s è¿‡é•¿"


if __name__ == "__main__":
    # è¿è¡Œæ€§èƒ½æµ‹è¯•
 