"""
åº”ç”¨å¯åŠ¨åˆå§‹åŒ–

åˆå§‹åŒ–æ‰€æœ‰æ€§èƒ½ä¼˜åŒ–ç»„ä»¶å’ŒæœåŠ¡ã€‚
"""
import asyncio
import logging
from contextlib import asynccontextmanager
from typing import AsyncGenerator, Dict, Any

from fastapi import FastAPI

from .config.settings import get_settings
from .database.connection_manager import init_connection_manager, close_connection_manager
from .cache.redis_cache import init_cache, close_cache
from .async_tasks.task_manager import init_task_manager, shutdown_task_manager
from .middleware.rate_limiter import init_rate_limiter
from .database.query_optimizer import get_query_optimizer
from .discovery.service_registry import init_service_registry, shutdown_service_registry
from .grpc_client.client_manager import init_grpc_client_manager, shutdown_grpc_client_manager
from .ha.failover_manager import init_failover_manager, shutdown_failover_manager, NodeRole

logger = logging.getLogger(__name__)
settings = get_settings()


@asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:
    """
    åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†
    
    Args:
        app: FastAPIåº”ç”¨å®ä¾‹
    """
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    logger.info("ğŸš€ å¼€å§‹åˆå§‹åŒ–ç´¢å…‹ç”Ÿæ´»è®¤è¯æœåŠ¡...")
    
    try:
        # 1. åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± 
        logger.info("ğŸ“Š åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± ...")
        await init_connection_manager()
        
        # 2. åˆå§‹åŒ–Redisç¼“å­˜
        if settings.redis_enabled:
            logger.info("ğŸ—„ï¸ åˆå§‹åŒ–Redisç¼“å­˜...")
            await init_cache()
        
        # 3. åˆå§‹åŒ–ä»»åŠ¡ç®¡ç†å™¨
        if settings.task_manager_enabled:
            logger.info("âš¡ åˆå§‹åŒ–å¼‚æ­¥ä»»åŠ¡ç®¡ç†å™¨...")
            await init_task_manager()
        
        # 4. åˆå§‹åŒ–é€Ÿç‡é™åˆ¶å™¨
        if settings.rate_limit_enabled:
            logger.info("ğŸ›¡ï¸ åˆå§‹åŒ–é€Ÿç‡é™åˆ¶å™¨...")
            await init_rate_limiter()
        
        # 5. åˆå§‹åŒ–æŸ¥è¯¢ä¼˜åŒ–å™¨
        logger.info("ğŸ”§ åˆå§‹åŒ–æŸ¥è¯¢ä¼˜åŒ–å™¨...")
        query_optimizer = get_query_optimizer()
        
        # 6. åˆå§‹åŒ–æœåŠ¡æ³¨å†Œä¸­å¿ƒ
        if settings.service_discovery_enabled:
            logger.info("ğŸ” åˆå§‹åŒ–æœåŠ¡æ³¨å†Œä¸­å¿ƒ...")
            await init_service_registry()
        
        # 7. åˆå§‹åŒ–gRPCå®¢æˆ·ç«¯ç®¡ç†å™¨
        if settings.grpc_enabled:
            logger.info("ğŸŒ åˆå§‹åŒ–gRPCå®¢æˆ·ç«¯ç®¡ç†å™¨...")
            await init_grpc_client_manager()
        
        # 8. åˆå§‹åŒ–æ•…éšœè½¬ç§»ç®¡ç†å™¨
        if settings.ha_enabled:
            logger.info("ğŸ”„ åˆå§‹åŒ–é«˜å¯ç”¨æ•…éšœè½¬ç§»ç®¡ç†å™¨...")
            # æ ¹æ®é…ç½®ç¡®å®šèŠ‚ç‚¹è§’è‰²
            node_role = NodeRole.PRIMARY if settings.ha_node_role == "primary" else NodeRole.SECONDARY
            node_priority = getattr(settings, 'ha_node_priority', 1)
            await init_failover_manager(role=node_role, priority=node_priority)
        
        # 9. åˆ›å»ºæ¨èçš„æ•°æ®åº“ç´¢å¼•ï¼ˆè¯•è¿è¡Œæ¨¡å¼ï¼‰
        logger.info("ğŸ“ˆ æ£€æŸ¥æ•°æ®åº“ç´¢å¼•ä¼˜åŒ–...")
        try:
            index_results = await query_optimizer.create_recommended_indexes(dry_run=True)
            missing_indexes = [r for r in index_results if r["status"] == "would_create"]
            if missing_indexes:
                logger.warning(f"å‘ç° {len(missing_indexes)} ä¸ªç¼ºå¤±çš„æ¨èç´¢å¼•ï¼Œå»ºè®®åˆ›å»ºä»¥æå‡æ€§èƒ½")
                for idx in missing_indexes:
                    logger.info(f"  - {idx['index']} (è¡¨: {idx['table']})")
        except Exception as e:
            logger.warning(f"ç´¢å¼•æ£€æŸ¥å¤±è´¥: {str(e)}")
        
        logger.info("âœ… ç´¢å…‹ç”Ÿæ´»è®¤è¯æœåŠ¡åˆå§‹åŒ–å®Œæˆï¼")
        logger.info("ğŸ¯ æœåŠ¡å·²å°±ç»ªï¼Œå¼€å§‹å¤„ç†è¯·æ±‚...")
        
        # åº”ç”¨è¿è¡ŒæœŸé—´
        yield
        
    except Exception as e:
        logger.error(f"âŒ æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        raise
    
    finally:
        # å…³é—­æ—¶æ¸…ç†
        logger.info("ğŸ”„ å¼€å§‹å…³é—­ç´¢å…‹ç”Ÿæ´»è®¤è¯æœåŠ¡...")
        
        try:
            # å…³é—­æ•…éšœè½¬ç§»ç®¡ç†å™¨
            if settings.ha_enabled:
                logger.info("ğŸ”„ å…³é—­æ•…éšœè½¬ç§»ç®¡ç†å™¨...")
                await shutdown_failover_manager()
            
            # å…³é—­gRPCå®¢æˆ·ç«¯ç®¡ç†å™¨
            if settings.grpc_enabled:
                logger.info("ğŸŒ å…³é—­gRPCå®¢æˆ·ç«¯ç®¡ç†å™¨...")
                await shutdown_grpc_client_manager()
            
            # å…³é—­æœåŠ¡æ³¨å†Œä¸­å¿ƒ
            if settings.service_discovery_enabled:
                logger.info("ğŸ” å…³é—­æœåŠ¡æ³¨å†Œä¸­å¿ƒ...")
                await shutdown_service_registry()
            
            # å…³é—­ä»»åŠ¡ç®¡ç†å™¨
            if settings.task_manager_enabled:
                logger.info("âš¡ å…³é—­å¼‚æ­¥ä»»åŠ¡ç®¡ç†å™¨...")
                await shutdown_task_manager()
            
            # å…³é—­Redisç¼“å­˜
            if settings.redis_enabled:
                logger.info("ğŸ—„ï¸ å…³é—­Redisç¼“å­˜è¿æ¥...")
                await close_cache()
            
            # å…³é—­æ•°æ®åº“è¿æ¥æ± 
            logger.info("ğŸ“Š å…³é—­æ•°æ®åº“è¿æ¥æ± ...")
            await close_connection_manager()
            
            logger.info("âœ… ç´¢å…‹ç”Ÿæ´»è®¤è¯æœåŠ¡å·²å®‰å…¨å…³é—­")
            
        except Exception as e:
            logger.error(f"âŒ æœåŠ¡å…³é—­æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")


async def health_check_startup() -> bool:
    """
    å¯åŠ¨æ—¶å¥åº·æ£€æŸ¥
    
    Returns:
        bool: å¥åº·æ£€æŸ¥æ˜¯å¦é€šè¿‡
    """
    logger.info("ğŸ” æ‰§è¡Œå¯åŠ¨å¥åº·æ£€æŸ¥...")
    
    try:
        from .database.connection_manager import get_connection_manager
        from .cache.redis_cache import get_redis_cache
        
        # æ£€æŸ¥æ•°æ®åº“è¿æ¥
        db_manager = get_connection_manager()
        db_health = await db_manager.health_check()
        
        if db_health.get("status") != "healthy":
            logger.error("âŒ æ•°æ®åº“å¥åº·æ£€æŸ¥å¤±è´¥")
            return False
        
        logger.info(f"âœ… æ•°æ®åº“è¿æ¥æ­£å¸¸ (å“åº”æ—¶é—´: {db_health.get('response_time_ms', 0):.2f}ms)")
        
        # æ£€æŸ¥Redisè¿æ¥ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if settings.redis_enabled:
            cache = get_redis_cache()
            cache_health = await cache.health_check()
            
            if cache_health.get("status") != "healthy":
                logger.warning("âš ï¸ Redisç¼“å­˜å¥åº·æ£€æŸ¥å¤±è´¥ï¼Œå°†ä»¥é™çº§æ¨¡å¼è¿è¡Œ")
            else:
                logger.info(f"âœ… Redisç¼“å­˜è¿æ¥æ­£å¸¸ (å“åº”æ—¶é—´: {cache_health.get('response_time_ms', 0):.2f}ms)")
        
        logger.info("âœ… å¯åŠ¨å¥åº·æ£€æŸ¥é€šè¿‡")
        return True
        
    except Exception as e:
        logger.error(f"âŒ å¯åŠ¨å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")
        return False


def setup_logging() -> None:
    """
    è®¾ç½®æ—¥å¿—é…ç½®
    """
    # é…ç½®æ—¥å¿—æ ¼å¼
    log_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    
    # æ ¹æ®ç¯å¢ƒè®¾ç½®æ—¥å¿—çº§åˆ«
    if settings.environment == "development":
        log_level = logging.DEBUG
    elif settings.environment == "testing":
        log_level = logging.INFO
    else:
        log_level = logging.WARNING
    
    # é…ç½®æ ¹æ—¥å¿—å™¨
    logging.basicConfig(
        level=log_level,
        format=log_format,
        handlers=[
            logging.StreamHandler(),  # æ§åˆ¶å°è¾“å‡º
        ]
    )
    
    # è®¾ç½®ç‰¹å®šæ¨¡å—çš„æ—¥å¿—çº§åˆ«
    logging.getLogger("asyncpg").setLevel(logging.WARNING)
    logging.getLogger("redis").setLevel(logging.WARNING)
    logging.getLogger("fastapi").setLevel(logging.INFO)
    
    logger.info(f"ğŸ“ æ—¥å¿—ç³»ç»Ÿå·²é…ç½® (çº§åˆ«: {logging.getLevelName(log_level)})")


async def performance_optimization_check() -> Dict[str, Any]:
    """
    æ€§èƒ½ä¼˜åŒ–æ£€æŸ¥
    
    Returns:
        Dict[str, Any]: ä¼˜åŒ–å»ºè®®
    """
    logger.info("ğŸ” æ‰§è¡Œæ€§èƒ½ä¼˜åŒ–æ£€æŸ¥...")
    
    try:
        from .database.connection_manager import get_connection_manager
        from .cache.redis_cache import get_redis_cache
        from .database.query_optimizer import get_query_optimizer
        
        recommendations = []
        
        # æ£€æŸ¥æ•°æ®åº“é…ç½®
        db_manager = get_connection_manager()
        pool_metrics = await db_manager.get_pool_metrics()
        
        if pool_metrics.get("max_size", 0) < 10:
            recommendations.append("å»ºè®®å¢åŠ æ•°æ®åº“è¿æ¥æ± å¤§å°ä»¥æå‡å¹¶å‘æ€§èƒ½")
        
        # æ£€æŸ¥ç¼“å­˜é…ç½®
        if not settings.redis_enabled:
            recommendations.append("å»ºè®®å¯ç”¨Redisç¼“å­˜ä»¥æå‡æŸ¥è¯¢æ€§èƒ½")
        
        # æ£€æŸ¥æŸ¥è¯¢ä¼˜åŒ–
        query_optimizer = get_query_optimizer()
        table_stats = await query_optimizer.get_table_statistics()
        
        for table_name, stats in table_stats.items():
            dead_rows = stats.get('dead_rows', 0)
            live_rows = stats.get('live_rows', 0)
            
            if dead_rows > 0 and live_rows > 0:
                dead_ratio = dead_rows / (live_rows + dead_rows)
                if dead_ratio > 0.1:
                    recommendations.append(f"è¡¨ {table_name} éœ€è¦æ‰§è¡Œ VACUUM æ¸…ç†æ­»è¡Œ")
        
        # æ£€æŸ¥ä»»åŠ¡é˜Ÿåˆ—é…ç½®
        if not settings.task_manager_enabled:
            recommendations.append("å»ºè®®å¯ç”¨å¼‚æ­¥ä»»åŠ¡ç®¡ç†å™¨ä»¥æå‡å“åº”æ€§èƒ½")
        
        result = {
            "total_recommendations": len(recommendations),
            "recommendations": recommendations,
            "status": "optimal" if len(recommendations) == 0 else "needs_optimization"
        }
        
        if recommendations:
            logger.info(f"ğŸ“Š å‘ç° {len(recommendations)} ä¸ªæ€§èƒ½ä¼˜åŒ–å»ºè®®")
            for i, rec in enumerate(recommendations, 1):
                logger.info(f"  {i}. {rec}")
        else:
            logger.info("âœ… ç³»ç»Ÿæ€§èƒ½é…ç½®è‰¯å¥½")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ æ€§èƒ½ä¼˜åŒ–æ£€æŸ¥å¤±è´¥: {str(e)}")
        return {
            "total_recommendations": 0,
            "recommendations": [],
            "status": "error",
            "error": str(e)
        }