#!/usr/bin/env python3
"""
é«˜çº§æœºå™¨å­¦ä¹ å¼‚å¸¸æ£€æµ‹æ¨¡å—
ç‰¹æ€§ï¼š
1. å¤šç®—æ³•èåˆæ£€æµ‹
2. è‡ªé€‚åº”é˜ˆå€¼è°ƒæ•´
3. æ—¶åºæ¨¡å¼è¯†åˆ«
4. å®æ—¶æµå¤„ç†
5. å¼‚å¸¸æ ¹å› åˆ†æ
6. é¢„æµ‹æ€§å¼‚å¸¸æ£€æµ‹
"""

import asyncio
import logging
import threading
import time
from collections import defaultdict, deque
from dataclasses import dataclass, field
from enum import Enum
from typing import Any

logger = logging.getLogger(__name__)


class AnomalyType(Enum):
    """å¼‚å¸¸ç±»å‹"""

    POINT_ANOMALY = "point"  # ç‚¹å¼‚å¸¸
    CONTEXTUAL_ANOMALY = "contextual"  # ä¸Šä¸‹æ–‡å¼‚å¸¸
    COLLECTIVE_ANOMALY = "collective"  # é›†ä½“å¼‚å¸¸
    TREND_ANOMALY = "trend"  # è¶‹åŠ¿å¼‚å¸¸
    SEASONAL_ANOMALY = "seasonal"  # å­£èŠ‚æ€§å¼‚å¸¸


class AnomalySeverity(Enum):
    """å¼‚å¸¸ä¸¥é‡ç¨‹åº¦"""

    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


@dataclass
class AnomalyResult:
    """å¼‚å¸¸æ£€æµ‹ç»“æœ"""

    timestamp: float
    metric_name: str
    value: float
    is_anomaly: bool
    anomaly_type: AnomalyType
    severity: AnomalySeverity
    confidence: float
    expected_range: tuple[float, float]
    deviation_score: float
    context: dict[str, Any] = field(default_factory=dict)
    root_cause: str | None = None


@dataclass
class MetricData:
    """æŒ‡æ ‡æ•°æ®"""

    name: str
    values: deque
    timestamps: deque
    max_size: int = 1000

    def add_value(self, value: float, timestamp: float = None):
        """æ·»åŠ æ•°å€¼"""
        if timestamp is None:
            timestamp = time.time()

        self.values.append(value)
        self.timestamps.append(timestamp)

        # ä¿æŒå›ºå®šå¤§å°
        while len(self.values) > self.max_size:
            self.values.popleft()
            self.timestamps.popleft()

    def get_recent_values(self, count: int = None) -> list[float]:
        """è·å–æœ€è¿‘çš„æ•°å€¼"""
        if count is None:
            return list(self.values)
        return list(self.values)[-count:]

    def get_values_in_window(self, window_seconds: int) -> list[tuple[float, float]]:
        """è·å–æ—¶é—´çª—å£å†…çš„æ•°å€¼"""
        current_time = time.time()
        cutoff_time = current_time - window_seconds

        result = []
        for value, timestamp in zip(self.values, self.timestamps, strict=False):
            if timestamp >= cutoff_time:
                result.append((value, timestamp))

        return result


class StatisticalAnomalyDetector:
    """ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹å™¨"""

    def __init__(self, window_size: int = 50, z_threshold: float = 3.0):
        self.window_size = window_size
        self.z_threshold = z_threshold

    def detect(self, data: MetricData) -> AnomalyResult | None:
        """æ£€æµ‹å¼‚å¸¸"""
        values = data.get_recent_values(self.window_size)

        if len(values) < 10:  # éœ€è¦è¶³å¤Ÿçš„æ•°æ®
            return None

        current_value = values[-1]
        historical_values = values[:-1]

        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        mean = np.mean(historical_values)
        std = np.std(historical_values)

        if std == 0:  # é¿å…é™¤é›¶
            return None

        # Z-scoreæ£€æµ‹
        z_score = abs(current_value - mean) / std
        is_anomaly = z_score > self.z_threshold

        if is_anomaly:
            # è®¡ç®—æœŸæœ›èŒƒå›´
            expected_range = (
                mean - self.z_threshold * std,
                mean + self.z_threshold * std,
            )

            # ç¡®å®šä¸¥é‡ç¨‹åº¦
            if z_score > 5:
                severity = AnomalySeverity.CRITICAL
            elif z_score > 4:
                severity = AnomalySeverity.HIGH
            elif z_score > 3.5:
                severity = AnomalySeverity.MEDIUM
            else:
                severity = AnomalySeverity.LOW

            return AnomalyResult(
                timestamp=time.time(),
                metric_name=data.name,
                value=current_value,
                is_anomaly=True,
                anomaly_type=AnomalyType.POINT_ANOMALY,
                severity=severity,
                confidence=min(z_score / 5.0, 1.0),
                expected_range=expected_range,
                deviation_score=z_score,
                context={
                    "mean": mean,
                    "std": std,
                    "z_score": z_score,
                    "method": "statistical",
                },
            )

        return None


class IsolationForestDetector:
    """å­¤ç«‹æ£®æ—å¼‚å¸¸æ£€æµ‹å™¨"""

    def __init__(self, contamination: float = 0.1, n_estimators: int = 100):
        self.contamination = contamination
        self.n_estimators = n_estimators
        self.model = None
        self.is_fitted = False
        self.feature_scaler = None

        try:
            from sklearn.ensemble import IsolationForest
            from sklearn.preprocessing import StandardScaler

            self.IsolationForest = IsolationForest
            self.StandardScaler = StandardScaler
            self.sklearn_available = True
        except ImportError:
            logger.warning("scikit-learnä¸å¯ç”¨ï¼Œå­¤ç«‹æ£®æ—æ£€æµ‹å™¨å°†è¢«ç¦ç”¨")
            self.sklearn_available = False

    def _prepare_features(self, data: MetricData) -> np.ndarray | None:
        """å‡†å¤‡ç‰¹å¾"""
        values = data.get_recent_values(100)

        if len(values) < 20:
            return None

        # æ„å»ºç‰¹å¾ï¼šå½“å‰å€¼ã€ç§»åŠ¨å¹³å‡ã€è¶‹åŠ¿ç­‰
        features = []
        window_sizes = [5, 10, 20]

        for i in range(len(values)):
            feature_vector = [values[i]]  # å½“å‰å€¼

            # ç§»åŠ¨å¹³å‡ç‰¹å¾
            for window in window_sizes:
                if i >= window - 1:
                    ma = np.mean(values[i - window + 1 : i + 1])
                    feature_vector.append(ma)
                else:
                    feature_vector.append(values[i])

            # è¶‹åŠ¿ç‰¹å¾
            if i >= 5:
                trend = np.polyfit(range(5), values[i - 4 : i + 1], 1)[0]
                feature_vector.append(trend)
            else:
                feature_vector.append(0)

            # æ³¢åŠ¨æ€§ç‰¹å¾
            if i >= 10:
                volatility = np.std(values[i - 9 : i + 1])
                feature_vector.append(volatility)
            else:
                feature_vector.append(0)

            features.append(feature_vector)

        return np.array(features)

    def detect(self, data: MetricData) -> AnomalyResult | None:
        """æ£€æµ‹å¼‚å¸¸"""
        if not self.sklearn_available:
            return None

        features = self._prepare_features(data)
        if features is None:
            return None

        try:
            # è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if not self.is_fitted or len(features) > 200:  # å®šæœŸé‡è®­ç»ƒ
                self.feature_scaler = self.StandardScaler()
                scaled_features = self.feature_scaler.fit_transform(features)

                self.model = self.IsolationForest(
                    contamination=self.contamination,
                    n_estimators=self.n_estimators,
                    random_state=42,
                )
                self.model.fit(scaled_features)
                self.is_fitted = True

            # é¢„æµ‹å½“å‰ç‚¹
            current_features = features[-1:]
            scaled_current = self.feature_scaler.transform(current_features)

            anomaly_score = self.model.decision_function(scaled_current)[0]
            is_anomaly = self.model.predict(scaled_current)[0] == -1

            if is_anomaly:
                # è®¡ç®—ç½®ä¿¡åº¦å’Œä¸¥é‡ç¨‹åº¦
                confidence = abs(anomaly_score)

                if confidence > 0.5:
                    severity = AnomalySeverity.CRITICAL
                elif confidence > 0.3:
                    severity = AnomalySeverity.HIGH
                elif confidence > 0.1:
                    severity = AnomalySeverity.MEDIUM
                else:
                    severity = AnomalySeverity.LOW

                current_value = data.get_recent_values(1)[0]

                return AnomalyResult(
                    timestamp=time.time(),
                    metric_name=data.name,
                    value=current_value,
                    is_anomaly=True,
                    anomaly_type=AnomalyType.CONTEXTUAL_ANOMALY,
                    severity=severity,
                    confidence=min(confidence, 1.0),
                    expected_range=(
                        float("-inf"),
                        float("inf"),
                    ),  # å­¤ç«‹æ£®æ—ä¸æä¾›å…·ä½“èŒƒå›´
                    deviation_score=abs(anomaly_score),
                    context={
                        "anomaly_score": anomaly_score,
                        "method": "isolation_forest",
                        "features_count": len(current_features[0]),
                    },
                )

        except Exception as e:
            logger.error(f"å­¤ç«‹æ£®æ—æ£€æµ‹å¤±è´¥: {e}")

        return None


class TrendAnomalyDetector:
    """è¶‹åŠ¿å¼‚å¸¸æ£€æµ‹å™¨"""

    def __init__(self, window_size: int = 30, trend_threshold: float = 0.1):
        self.window_size = window_size
        self.trend_threshold = trend_threshold

    def detect(self, data: MetricData) -> AnomalyResult | None:
        """æ£€æµ‹è¶‹åŠ¿å¼‚å¸¸"""
        values = data.get_recent_values(self.window_size)

        if len(values) < self.window_size:
            return None

        try:
            # è®¡ç®—çº¿æ€§è¶‹åŠ¿
            x = np.arange(len(values))
            coeffs = np.polyfit(x, values, 1)
            trend_slope = coeffs[0]

            # è®¡ç®—è¶‹åŠ¿å¼ºåº¦
            y_pred = np.polyval(coeffs, x)
            r_squared = 1 - (
                np.sum((values - y_pred) ** 2) / np.sum((values - np.mean(values)) ** 2)
            )

            # æ£€æµ‹å¼‚å¸¸è¶‹åŠ¿
            is_anomaly = abs(trend_slope) > self.trend_threshold and r_squared > 0.7

            if is_anomaly:
                # ç¡®å®šä¸¥é‡ç¨‹åº¦
                trend_magnitude = abs(trend_slope)
                if trend_magnitude > self.trend_threshold * 5:
                    severity = AnomalySeverity.CRITICAL
                elif trend_magnitude > self.trend_threshold * 3:
                    severity = AnomalySeverity.HIGH
                elif trend_magnitude > self.trend_threshold * 2:
                    severity = AnomalySeverity.MEDIUM
                else:
                    severity = AnomalySeverity.LOW

                current_value = values[-1]

                return AnomalyResult(
                    timestamp=time.time(),
                    metric_name=data.name,
                    value=current_value,
                    is_anomaly=True,
                    anomaly_type=AnomalyType.TREND_ANOMALY,
                    severity=severity,
                    confidence=r_squared,
                    expected_range=(float("-inf"), float("inf")),
                    deviation_score=trend_magnitude,
                    context={
                        "trend_slope": trend_slope,
                        "r_squared": r_squared,
                        "trend_direction": (
                            "increasing" if trend_slope > 0 else "decreasing"
                        ),
                        "method": "trend_analysis",
                    },
                )

        except Exception as e:
            logger.error(f"è¶‹åŠ¿æ£€æµ‹å¤±è´¥: {e}")

        return None


class SeasonalAnomalyDetector:
    """å­£èŠ‚æ€§å¼‚å¸¸æ£€æµ‹å™¨"""

    def __init__(self, period: int = 60, seasonal_threshold: float = 2.0):
        self.period = period  # å­£èŠ‚å‘¨æœŸï¼ˆåˆ†é’Ÿï¼‰
        self.seasonal_threshold = seasonal_threshold
        self.seasonal_patterns = {}

    def detect(self, data: MetricData) -> AnomalyResult | None:
        """æ£€æµ‹å­£èŠ‚æ€§å¼‚å¸¸"""
        # è·å–è¶³å¤Ÿçš„å†å²æ•°æ®
        window_data = data.get_values_in_window(self.period * 60 * 3)  # 3ä¸ªå‘¨æœŸçš„æ•°æ®

        if len(window_data) < self.period * 2:
            return None

        try:
            current_time = time.time()
            current_value = data.get_recent_values(1)[0]

            # è®¡ç®—å½“å‰æ—¶é—´åœ¨å‘¨æœŸä¸­çš„ä½ç½®
            time_in_period = int((current_time % (self.period * 60)) / 60)

            # æ”¶é›†åŒä¸€æ—¶é—´ç‚¹çš„å†å²æ•°æ®
            historical_values = []
            for value, timestamp in window_data[:-1]:  # æ’é™¤å½“å‰å€¼
                hist_time_in_period = int((timestamp % (self.period * 60)) / 60)
                if abs(hist_time_in_period - time_in_period) <= 2:  # å…è®¸2åˆ†é’Ÿè¯¯å·®
                    historical_values.append(value)

            if len(historical_values) < 3:
                return None

            # è®¡ç®—å­£èŠ‚æ€§æœŸæœ›å€¼å’Œæ ‡å‡†å·®
            seasonal_mean = np.mean(historical_values)
            seasonal_std = np.std(historical_values)

            if seasonal_std == 0:
                return None

            # è®¡ç®—å­£èŠ‚æ€§åå·®
            seasonal_deviation = abs(current_value - seasonal_mean) / seasonal_std
            is_anomaly = seasonal_deviation > self.seasonal_threshold

            if is_anomaly:
                # ç¡®å®šä¸¥é‡ç¨‹åº¦
                if seasonal_deviation > 4:
                    severity = AnomalySeverity.CRITICAL
                elif seasonal_deviation > 3:
                    severity = AnomalySeverity.HIGH
                elif seasonal_deviation > 2.5:
                    severity = AnomalySeverity.MEDIUM
                else:
                    severity = AnomalySeverity.LOW

                expected_range = (
                    seasonal_mean - self.seasonal_threshold * seasonal_std,
                    seasonal_mean + self.seasonal_threshold * seasonal_std,
                )

                return AnomalyResult(
                    timestamp=current_time,
                    metric_name=data.name,
                    value=current_value,
                    is_anomaly=True,
                    anomaly_type=AnomalyType.SEASONAL_ANOMALY,
                    severity=severity,
                    confidence=min(seasonal_deviation / 4.0, 1.0),
                    expected_range=expected_range,
                    deviation_score=seasonal_deviation,
                    context={
                        "seasonal_mean": seasonal_mean,
                        "seasonal_std": seasonal_std,
                        "time_in_period": time_in_period,
                        "historical_count": len(historical_values),
                        "method": "seasonal_analysis",
                    },
                )

        except Exception as e:
            logger.error(f"å­£èŠ‚æ€§æ£€æµ‹å¤±è´¥: {e}")

        return None


class EnsembleAnomalyDetector:
    """é›†æˆå¼‚å¸¸æ£€æµ‹å™¨"""

    def __init__(self) -> None:
        self.detectors = [
            StatisticalAnomalyDetector(),
            IsolationForestDetector(),
            TrendAnomalyDetector(),
            SeasonalAnomalyDetector(),
        ]
        self.weights = [0.3, 0.3, 0.2, 0.2]  # æ£€æµ‹å™¨æƒé‡

    def detect(self, data: MetricData) -> list[AnomalyResult]:
        """é›†æˆæ£€æµ‹"""
        results = []

        for detector in self.detectors:
            try:
                result = detector.detect(data)
                if result:
                    results.append(result)
            except Exception as e:
                logger.error(f"æ£€æµ‹å™¨ {type(detector).__name__} å¤±è´¥: {e}")

        return results

    def get_consensus_result(self, data: MetricData) -> AnomalyResult | None:
        """è·å–å…±è¯†ç»“æœ"""
        results = self.detect(data)

        if not results:
            return None

        # å¦‚æœå¤šä¸ªæ£€æµ‹å™¨éƒ½è®¤ä¸ºæ˜¯å¼‚å¸¸ï¼Œåˆ™æé«˜ç½®ä¿¡åº¦
        if len(results) >= 2:
            # é€‰æ‹©æœ€é«˜ä¸¥é‡ç¨‹åº¦çš„ç»“æœ
            critical_results = [
                r for r in results if r.severity == AnomalySeverity.CRITICAL
            ]
            if critical_results:
                best_result = critical_results[0]
            else:
                best_result = max(results, key=lambda r: r.confidence)

            # æé«˜ç½®ä¿¡åº¦
            best_result.confidence = min(best_result.confidence * 1.5, 1.0)
            best_result.context["ensemble_count"] = len(results)
            best_result.context["consensus"] = True

            return best_result

        # å•ä¸ªæ£€æµ‹å™¨ç»“æœ
        return results[0]


class AdvancedMLAnomalyManager:
    """é«˜çº§æœºå™¨å­¦ä¹ å¼‚å¸¸ç®¡ç†å™¨"""

    def __init__(self, max_metrics: int = 100):
        self.metrics_data: dict[str, MetricData] = {}
        self.ensemble_detector = EnsembleAnomalyDetector()
        self.max_metrics = max_metrics
        self.anomaly_history: deque = deque(maxlen=1000)
        self.lock = threading.RLock()

        # è‡ªé€‚åº”å‚æ•°
        self.adaptive_thresholds = {}
        self.false_positive_rates = defaultdict(float)

        # æ€§èƒ½ç»Ÿè®¡
        self.detection_count = 0
        self.anomaly_count = 0
        self.processing_times = deque(maxlen=100)

    def add_metric_value(self, metric_name: str, value: float, timestamp: float = None):
        """æ·»åŠ æŒ‡æ ‡å€¼"""
        with self.lock:
            if metric_name not in self.metrics_data:
                if len(self.metrics_data) >= self.max_metrics:
                    # ç§»é™¤æœ€æ—§çš„æŒ‡æ ‡
                    oldest_metric = min(self.metrics_data.keys())
                    del self.metrics_data[oldest_metric]

                self.metrics_data[metric_name] = MetricData(
                    name=metric_name, values=deque(), timestamps=deque()
                )

            self.metrics_data[metric_name].add_value(value, timestamp)

    async def detect_anomalies(self, metric_name: str = None) -> list[AnomalyResult]:
        """æ£€æµ‹å¼‚å¸¸"""
        start_time = time.time()
        results = []

        try:
            with self.lock:
                metrics_to_check = (
                    [metric_name] if metric_name else list(self.metrics_data.keys())
                )

                for name in metrics_to_check:
                    if name in self.metrics_data:
                        data = self.metrics_data[name]

                        # æ‰§è¡Œæ£€æµ‹
                        result = self.ensemble_detector.get_consensus_result(data)

                        if result:
                            # åº”ç”¨è‡ªé€‚åº”é˜ˆå€¼
                            result = self._apply_adaptive_threshold(result)

                            if result.is_anomaly:
                                # æ·»åŠ æ ¹å› åˆ†æ
                                result.root_cause = self._analyze_root_cause(
                                    result, data
                                )

                                results.append(result)
                                self.anomaly_history.append(result)
                                self.anomaly_count += 1

                        self.detection_count += 1

            # è®°å½•å¤„ç†æ—¶é—´
            processing_time = time.time() - start_time
            self.processing_times.append(processing_time)

        except Exception as e:
            logger.error(f"å¼‚å¸¸æ£€æµ‹å¤±è´¥: {e}")

        return results

    def _apply_adaptive_threshold(self, result: AnomalyResult) -> AnomalyResult:
        """åº”ç”¨è‡ªé€‚åº”é˜ˆå€¼"""
        metric_name = result.metric_name

        # è·å–å†å²è¯¯æŠ¥ç‡
        false_positive_rate = self.false_positive_rates.get(metric_name, 0.1)

        # æ ¹æ®è¯¯æŠ¥ç‡è°ƒæ•´ç½®ä¿¡åº¦é˜ˆå€¼
        if false_positive_rate > 0.3:  # è¯¯æŠ¥ç‡è¿‡é«˜
            result.confidence *= 0.8  # é™ä½ç½®ä¿¡åº¦
        elif false_positive_rate < 0.05:  # è¯¯æŠ¥ç‡å¾ˆä½
            result.confidence *= 1.2  # æé«˜ç½®ä¿¡åº¦

        result.confidence = min(result.confidence, 1.0)

        # æ›´æ–°æ˜¯å¦ä¸ºå¼‚å¸¸çš„åˆ¤æ–­
        confidence_threshold = 0.7 if false_positive_rate > 0.2 else 0.5
        result.is_anomaly = result.confidence >= confidence_threshold

        return result

    def _analyze_root_cause(self, result: AnomalyResult, data: MetricData) -> str:
        """åˆ†ææ ¹å› """
        try:
            # ç®€å•çš„æ ¹å› åˆ†æ
            if result.anomaly_type == AnomalyType.TREND_ANOMALY:
                trend_direction = result.context.get("trend_direction", "unknown")
                return f"æ£€æµ‹åˆ°{trend_direction}è¶‹åŠ¿å¼‚å¸¸ï¼Œå¯èƒ½åŸå› ï¼šè´Ÿè½½å˜åŒ–ã€èµ„æºæ³„æ¼æˆ–ç³»ç»Ÿé…ç½®å˜æ›´"

            elif result.anomaly_type == AnomalyType.SEASONAL_ANOMALY:
                return "æ£€æµ‹åˆ°å­£èŠ‚æ€§æ¨¡å¼å¼‚å¸¸ï¼Œå¯èƒ½åŸå› ï¼šä¸šåŠ¡æ¨¡å¼å˜åŒ–ã€å®šæ—¶ä»»åŠ¡å¼‚å¸¸æˆ–å¤–éƒ¨ä¾èµ–é—®é¢˜"

            elif result.anomaly_type == AnomalyType.POINT_ANOMALY:
                if result.severity in [AnomalySeverity.HIGH, AnomalySeverity.CRITICAL]:
                    return "æ£€æµ‹åˆ°çªå‘å¼‚å¸¸å³°å€¼ï¼Œå¯èƒ½åŸå› ï¼šç³»ç»Ÿæ•…éšœã€æ”»å‡»æˆ–é…ç½®é”™è¯¯"
                else:
                    return "æ£€æµ‹åˆ°è½»å¾®å¼‚å¸¸ï¼Œå¯èƒ½åŸå› ï¼šæ­£å¸¸æ³¢åŠ¨æˆ–ä¸´æ—¶è´Ÿè½½å˜åŒ–"

            elif result.anomaly_type == AnomalyType.CONTEXTUAL_ANOMALY:
                return "æ£€æµ‹åˆ°ä¸Šä¸‹æ–‡å¼‚å¸¸ï¼Œå¯èƒ½åŸå› ï¼šç³»ç»Ÿè¡Œä¸ºæ¨¡å¼å˜åŒ–æˆ–å¤šæŒ‡æ ‡å…³è”å¼‚å¸¸"

            else:
                return "å¼‚å¸¸åŸå› éœ€è¦è¿›ä¸€æ­¥åˆ†æ"

        except Exception as e:
            logger.error(f"æ ¹å› åˆ†æå¤±è´¥: {e}")
            return "æ ¹å› åˆ†æå¤±è´¥"

    def update_false_positive_feedback(self, metric_name: str, is_false_positive: bool):
        """æ›´æ–°è¯¯æŠ¥åé¦ˆ"""
        current_rate = self.false_positive_rates.get(metric_name, 0.1)

        # ä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡æ›´æ–°è¯¯æŠ¥ç‡
        alpha = 0.1
        if is_false_positive:
            new_rate = current_rate * (1 - alpha) + alpha
        else:
            new_rate = current_rate * (1 - alpha)

        self.false_positive_rates[metric_name] = max(0.01, min(0.5, new_rate))

    def get_anomaly_summary(self) -> dict[str, Any]:
        """è·å–å¼‚å¸¸æ‘˜è¦"""
        recent_anomalies = [
            a for a in self.anomaly_history if time.time() - a.timestamp < 3600
        ]  # æœ€è¿‘1å°æ—¶

        severity_counts = defaultdict(int)
        type_counts = defaultdict(int)

        for anomaly in recent_anomalies:
            severity_counts[anomaly.severity.value] += 1
            type_counts[anomaly.anomaly_type.value] += 1

        avg_processing_time = (
            np.mean(self.processing_times) if self.processing_times else 0
        )

        return {
            "total_detections": self.detection_count,
            "total_anomalies": self.anomaly_count,
            "anomaly_rate": (self.anomaly_count / max(self.detection_count, 1)) * 100,
            "recent_anomalies": len(recent_anomalies),
            "severity_distribution": dict(severity_counts),
            "type_distribution": dict(type_counts),
            "avg_processing_time": round(avg_processing_time, 4),
            "monitored_metrics": len(self.metrics_data),
            "false_positive_rates": dict(self.false_positive_rates),
        }

    def get_metric_health_score(self, metric_name: str) -> float:
        """è·å–æŒ‡æ ‡å¥åº·åˆ†æ•°"""
        if metric_name not in self.metrics_data:
            return 1.0

        # è®¡ç®—æœ€è¿‘å¼‚å¸¸çš„å½±å“
        recent_anomalies = [
            a
            for a in self.anomaly_history
            if a.metric_name == metric_name
            and time.time() - a.timestamp < 1800  # æœ€è¿‘30åˆ†é’Ÿ
        ]

        if not recent_anomalies:
            return 1.0

        # æ ¹æ®å¼‚å¸¸ä¸¥é‡ç¨‹åº¦å’Œæ•°é‡è®¡ç®—å¥åº·åˆ†æ•°
        severity_weights = {
            AnomalySeverity.LOW: 0.1,
            AnomalySeverity.MEDIUM: 0.3,
            AnomalySeverity.HIGH: 0.6,
            AnomalySeverity.CRITICAL: 1.0,
        }

        total_impact = sum(
            severity_weights.get(a.severity, 0.5) for a in recent_anomalies
        )
        health_score = max(
            0.0, 1.0 - (total_impact / 10.0)
        )  # æœ€å¤š10ä¸ªä¸¥é‡å¼‚å¸¸ä¼šä½¿åˆ†æ•°é™åˆ°0

        return round(health_score, 2)


# å…¨å±€é«˜çº§å¼‚å¸¸ç®¡ç†å™¨
advanced_ml_anomaly_manager = AdvancedMLAnomalyManager()


async def demo_advanced_ml_anomaly() -> None:
    """æ¼”ç¤ºé«˜çº§æœºå™¨å­¦ä¹ å¼‚å¸¸æ£€æµ‹"""
    print("ğŸš€ é«˜çº§æœºå™¨å­¦ä¹ å¼‚å¸¸æ£€æµ‹æ¼”ç¤º")

    # æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆ
    import random

    print("\nğŸ“Š ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®...")

    # æ­£å¸¸æ•°æ®
    for i in range(100):
        timestamp = time.time() - (100 - i) * 60  # è¿‡å»100åˆ†é’Ÿçš„æ•°æ®

        # CPUä½¿ç”¨ç‡ - æ­£å¸¸æ³¢åŠ¨
        cpu_value = 30 + 10 * np.sin(i * 0.1) + random.gauss(0, 2)
        advanced_ml_anomaly_manager.add_metric_value(
            "cpu_percent", cpu_value, timestamp
        )

        # å†…å­˜ä½¿ç”¨ç‡ - ç¼“æ…¢å¢é•¿è¶‹åŠ¿
        memory_value = 40 + i * 0.1 + random.gauss(0, 1)
        advanced_ml_anomaly_manager.add_metric_value(
            "memory_percent", memory_value, timestamp
        )

        # å“åº”æ—¶é—´ - å­£èŠ‚æ€§æ¨¡å¼
        response_time = 100 + 20 * np.sin(i * 0.2) + random.gauss(0, 5)
        advanced_ml_anomaly_manager.add_metric_value(
            "response_time", response_time, timestamp
        )

    # æ·»åŠ å¼‚å¸¸æ•°æ®
    print("ğŸ”¥ æ³¨å…¥å¼‚å¸¸æ•°æ®...")

    # CPUçªå‘å¼‚å¸¸
    advanced_ml_anomaly_manager.add_metric_value("cpu_percent", 95.0)

    # å†…å­˜å¼‚å¸¸è¶‹åŠ¿
    for i in range(5):
        memory_value = 50 + i * 10 + random.gauss(0, 1)
        advanced_ml_anomaly_manager.add_metric_value("memory_percent", memory_value)
        await asyncio.sleep(0.1)

    # å“åº”æ—¶é—´å­£èŠ‚æ€§å¼‚å¸¸
    advanced_ml_anomaly_manager.add_metric_value("response_time", 300.0)

    print("\nğŸ” æ‰§è¡Œå¼‚å¸¸æ£€æµ‹...")

    # æ£€æµ‹å¼‚å¸¸
    anomalies = await advanced_ml_anomaly_manager.detect_anomalies()

    print("\nğŸ“‹ æ£€æµ‹ç»“æœ:")
    print(f"å‘ç°å¼‚å¸¸æ•°é‡: {len(anomalies)}")

    for anomaly in anomalies:
        print("\nğŸš¨ å¼‚å¸¸è¯¦æƒ…:")
        print(f"  æŒ‡æ ‡: {anomaly.metric_name}")
        print(f"  ç±»å‹: {anomaly.anomaly_type.value}")
        print(f"  ä¸¥é‡ç¨‹åº¦: {anomaly.severity.value}")
        print(f"  ç½®ä¿¡åº¦: {anomaly.confidence:.2f}")
        print(f"  å½“å‰å€¼: {anomaly.value:.2f}")
        print(f"  æœŸæœ›èŒƒå›´: {anomaly.expected_range}")
        print(f"  åå·®åˆ†æ•°: {anomaly.deviation_score:.2f}")
        print(f"  æ ¹å› : {anomaly.root_cause}")
        print(f"  æ£€æµ‹æ–¹æ³•: {anomaly.context.get('method', 'unknown')}")

    # æ˜¾ç¤ºæ‘˜è¦ç»Ÿè®¡
    summary = advanced_ml_anomaly_manager.get_anomaly_summary()
    print("\nğŸ“Š å¼‚å¸¸æ£€æµ‹æ‘˜è¦:")
    print(f"æ€»æ£€æµ‹æ¬¡æ•°: {summary['total_detections']}")
    print(f"æ€»å¼‚å¸¸æ•°é‡: {summary['total_anomalies']}")
    print(f"å¼‚å¸¸ç‡: {summary['anomaly_rate']:.1f}%")
    print(f"å¹³å‡å¤„ç†æ—¶é—´: {summary['avg_processing_time']:.4f}s")
    print(f"ç›‘æ§æŒ‡æ ‡æ•°: {summary['monitored_metrics']}")

    # æ˜¾ç¤ºæŒ‡æ ‡å¥åº·åˆ†æ•°
    print("\nğŸ’š æŒ‡æ ‡å¥åº·åˆ†æ•°:")
    for metric in ["cpu_percent", "memory_percent", "response_time"]:
        score = advanced_ml_anomaly_manager.get_metric_health_score(metric)
        status = "å¥åº·" if score > 0.8 else "è­¦å‘Š" if score > 0.5 else "å¼‚å¸¸"
        print(f"  {metric}: {score:.2f} ({status})")

    return {
        "anomalies_detected": len(anomalies),
        "summary": summary,
        "health_scores": {
            metric: advanced_ml_anomaly_manager.get_metric_health_score(metric)
            for metric in ["cpu_percent", "memory_percent", "response_time"]
        },
    }


if __name__ == "__main__":
    asyncio.run(demo_advanced_ml_anomaly())
