from typing import Dict, List, Any, Optional, Union

"""
test_benchmark_service - 索克生活项目模块
"""

from datetime import datetime
from internal.benchmark.benchmark_service import BenchmarkExecutor, BenchmarkTask, BenchmarkResult
from internal.benchmark.model_interface import ModelInterface, ModelPrediction
from internal.suokebench.config import BenchConfig
from unittest.mock import Mock, patch, AsyncMock
import asyncio
import pytest

"""基准测试引擎单元测试"""




@pytest.fixture
def mock_config() - > None:
    """模拟配置"""
    config = Mock(spec = BenchConfig)
    config.benchmark = Mock()
    config.benchmark.max_concurrent_tasks = 4
    return config


@pytest.fixture
def mock_model() - > None:
    """模拟模型"""
    model = Mock(spec = ModelInterface)
    model.predict_batch.return_value = [
        ModelPrediction(
            input_data = {"text": "test"},
            prediction = "positive",
            confidence = 0.95,
            latency = 100.0,
            metadata = {}
        )
    ]
    return model


@pytest.fixture
def benchmark_executor(mock_config):
    """基准测试执行器"""
    with patch('internal.benchmark.benchmark_service.get_global_cache'), \
        patch('internal.benchmark.benchmark_service.get_global_metrics'):
        executor = BenchmarkExecutor(mock_config)
        return executor


class TestBenchmarkTask:
    """基准测试任务测试"""

    def test_task_creation(self) - > None:
        """测试任务创建"""
        task = BenchmarkTask(
            task_id = "test - 123",
            benchmark_id = "sentiment - analysis",
            model_id = "bert - base",
            model_version = "v1.0",
            test_data = [{"text": "test", "label": "positive"}],
            config = {"batch_size": 32},
            created_at = datetime.now()
        )

        assert task.task_id == "test - 123"
        assert task.benchmark_id == "sentiment - analysis"
        assert task.status == "pending"
        assert task.progress == 0.0
        assert task.results is None
        assert task.error_message is None

    def test_task_status_update(self) - > None:
        """测试任务状态更新"""
        task = BenchmarkTask(
            task_id = "test - 123",
            benchmark_id = "sentiment - analysis",
            model_id = "bert - base",
            model_version = "v1.0",
            test_data = [],
            config = {},
            created_at = datetime.now()
        )

        task.status = "running"
        task.progress = 0.5

        assert task.status == "running"
        assert task.progress == 0.5


class TestBenchmarkResult:
    """基准测试结果测试"""

    def test_result_creation(self) - > None:
        """测试结果创建"""
        predictions = [
            ModelPrediction(
                input_data = {"text": "test"},
                prediction = "positive",
                confidence = 0.95,
                latency = 100.0,
                metadata = {}
            )
        ]

        result = BenchmarkResult(
            task_id = "test - 123",
            benchmark_id = "sentiment - analysis",
            model_id = "bert - base",
            model_version = "v1.0",
            metrics = {"accuracy": 0.95, "f1": 0.93},
            predictions = predictions,
            execution_time = 120.5,
            timestamp = datetime.now(),
            metadata = {"batch_size": 32}
        )

        assert result.task_id == "test - 123"
        assert result.metrics["accuracy"] == 0.95
        assert len(result.predictions) == 1
        assert result.execution_time == 120.5


class TestBenchmarkExecutor:
    """基准测试执行器测试"""

    @pytest.mark.asyncio
    async def test_submit_benchmark(self, benchmark_executor):
        """测试提交基准测试"""
        test_data = [
            {"text": "I love this movie", "label": "positive"},
            {"text": "This is terrible", "label": "negative"}
        ]

        with patch.object(benchmark_executor, '_execute_benchmark_async') as mock_execute:
            task_id = await benchmark_executor.submit_benchmark(
                benchmark_id = "sentiment - analysis",
                model_id = "bert - base",
                model_version = "v1.0",
                test_data = test_data,
                config = {"batch_size": 32}
            )

            assert task_id in benchmark_executor.active_tasks
            task = benchmark_executor.active_tasks[task_id]
            assert task.benchmark_id == "sentiment - analysis"
            assert task.model_id == "bert - base"
            assert len(task.test_data) == 2
            mock_execute.assert_called_once()

    @pytest.mark.asyncio
    async def test_execute_benchmark_success(self, benchmark_executor, mock_model):
        """测试成功执行基准测试"""
        task = BenchmarkTask(
            task_id = "test - 123",
            benchmark_id = "sentiment - analysis",
            model_id = "bert - base",
            model_version = "v1.0",
            test_data = [{"text": "test", "label": "positive"}],
            config = {},
            created_at = datetime.now()
        )

        with patch.object(benchmark_executor, '_get_model', return_value = mock_model), \
            patch.object(benchmark_executor, '_calculate_metrics', return_value = {"accuracy": 0.95}):

            await benchmark_executor._execute_benchmark_async(task)

            assert task.status == "completed"
            assert task.progress == 1.0
            assert task.results is not None
            assert task.task_id in benchmark_executor.completed_tasks

    @pytest.mark.asyncio
    async def test_execute_benchmark_failure(self, benchmark_executor):
        """测试基准测试执行失败"""
        task = BenchmarkTask(
            task_id = "test - 123",
            benchmark_id = "sentiment - analysis",
            model_id = "bert - base",
            model_version = "v1.0",
            test_data = [{"text": "test", "label": "positive"}],
            config = {},
            created_at = datetime.now()
        )

        with patch.object(benchmark_executor, '_get_model', side_effect = Exception("模型加载失败")):
            await benchmark_executor._execute_benchmark_async(task)

            assert task.status == "failed"
            assert task.error_message == "模型加载失败"

    @pytest.mark.asyncio
    async def test_batch_predict(self, benchmark_executor, mock_model):
        """测试批量预测"""
        task = BenchmarkTask(
            task_id = "test - 123",
            benchmark_id = "sentiment - analysis",
            model_id = "bert - base",
            model_version = "v1.0",
            test_data = [
                {"text": "test1", "label": "positive"},
                {"text": "test2", "label": "negative"}
            ],
            config = {"batch_size": 1},
            created_at = datetime.now()
        )

        predictions = await benchmark_executor._batch_predict(
            model = mock_model,
            batch_data = task.test_data,
            task = task
        )

        assert len(predictions) == 2
        assert all(isinstance(p, ModelPrediction) for p in predictions)
        mock_model.predict_batch.assert_called()

    @pytest.mark.asyncio
    async def test_calculate_metrics_tcm(self, benchmark_executor):
        """测试计算中医指标"""
        predictions = [
            ModelPrediction(
                input_data = {"image": "tongue.jpg"},
                prediction = "pale",
                confidence = 0.9,
                latency = 150.0,
                metadata = {}
            )
        ]

        test_data = [
            {"image": "tongue.jpg", "label": "pale"}
        ]

        with patch.object(benchmark_executor, '_calculate_tcm_metrics', return_value = {"accuracy": 1.0}):
            metrics = await benchmark_executor._calculate_metrics(
                benchmark_id = "tongue_recognition",
                predictions = predictions,
                test_data = test_data
            )

            assert "accuracy" in metrics
            assert metrics["accuracy"] == 1.0

    @pytest.mark.asyncio
    async def test_calculate_metrics_health_plan(self, benchmark_executor):
        """测试计算健康方案指标"""
        predictions = [
            ModelPrediction(
                input_data = {"symptoms": ["fatigue", "insomnia"]},
                prediction = "qi_deficiency_plan",
                confidence = 0.85,
                latency = 200.0,
                metadata = {}
            )
        ]

        test_data = [
            {"symptoms": ["fatigue", "insomnia"], "plan": "qi_deficiency_plan"}
        ]

        with patch.object(benchmark_executor, '_calculate_health_plan_metrics', return_value = {"rouge_l": 0.8}):
            metrics = await benchmark_executor._calculate_metrics(
                benchmark_id = "health_plan_generation",
                predictions = predictions,
                test_data = test_data
            )

            assert "rouge_l" in metrics
            assert metrics["rouge_l"] == 0.8

    def test_get_task_status(self, benchmark_executor):
        """测试获取任务状态"""
        task = BenchmarkTask(
            task_id = "test - 123",
            benchmark_id = "sentiment - analysis",
            model_id = "bert - base",
            model_version = "v1.0",
            test_data = [],
            config = {},
            created_at = datetime.now(),
            status = "running",
            progress = 0.5
        )

        benchmark_executor.active_tasks["test - 123"] = task

        status = benchmark_executor.get_task_status("test - 123")

        assert status is not None
        assert status["task_id"] == "test - 123"
        assert status["status"] == "running"
        assert status["progress"] == 0.5

    def test_get_task_status_not_found(self, benchmark_executor):
        """测试获取不存在的任务状态"""
        status = benchmark_executor.get_task_status("nonexistent")
        assert status is None

    def test_get_task_result(self, benchmark_executor):
        """测试获取任务结果"""
        result = BenchmarkResult(
            task_id = "test - 123",
            benchmark_id = "sentiment - analysis",
            model_id = "bert - base",
            model_version = "v1.0",
            metrics = {"accuracy": 0.95},
            predictions = [],
            execution_time = 120.0,
            timestamp = datetime.now(),
            metadata = {}
        )

        benchmark_executor.completed_tasks["test - 123"] = result

        retrieved_result = benchmark_executor.get_task_result("test - 123")

        assert retrieved_result is not None
        assert retrieved_result.task_id == "test - 123"
        assert retrieved_result.metrics["accuracy"] == 0.95

    def test_list_tasks_all(self, benchmark_executor):
        """测试列出所有任务"""
        # 添加活跃任务
        active_task = BenchmarkTask(
            task_id = "active - 123",
            benchmark_id = "test",
            model_id = "model",
            model_version = "v1.0",
            test_data = [],
            config = {},
            created_at = datetime.now(),
            status = "running"
        )
        benchmark_executor.active_tasks["active - 123"] = active_task

        # 添加完成任务
        completed_result = BenchmarkResult(
            task_id = "completed - 123",
            benchmark_id = "test",
            model_id = "model",
            model_version = "v1.0",
            metrics = {},
            predictions = [],
            execution_time = 100.0,
            timestamp = datetime.now(),
            metadata = {}
        )
        benchmark_executor.completed_tasks["completed - 123"] = completed_result

        tasks = benchmark_executor.list_tasks()

        assert len(tasks) == 2
        task_ids = [task["task_id"] for task in tasks]
        assert "active - 123" in task_ids
        assert "completed - 123" in task_ids

    def test_list_tasks_filtered(self, benchmark_executor):
        """测试按状态过滤任务"""
        # 添加运行中任务
        running_task = BenchmarkTask(
            task_id = "running - 123",
            benchmark_id = "test",
            model_id = "model",
            model_version = "v1.0",
            test_data = [],
            config = {},
            created_at = datetime.now(),
            status = "running"
        )
        benchmark_executor.active_tasks["running - 123"] = running_task

        # 添加失败任务
        failed_task = BenchmarkTask(
            task_id = "failed - 123",
            benchmark_id = "test",
            model_id = "model",
            model_version = "v1.0",
            test_data = [],
            config = {},
            created_at = datetime.now(),
            status = "failed"
        )
        benchmark_executor.active_tasks["failed - 123"] = failed_task

        running_tasks = benchmark_executor.list_tasks(status = "running")

        assert len(running_tasks) == 1
        assert running_tasks[0]["task_id"] == "running - 123"
        assert running_tasks[0]["status"] == "running"

    def test_cleanup_old_tasks(self, benchmark_executor):
        """测试清理旧任务"""
        # 添加旧任务
        old_time = datetime.now().timestamp() - 25 * 3600  # 25小时前
        old_task = BenchmarkTask(
            task_id = "old - 123",
            benchmark_id = "test",
            model_id = "model",
            model_version = "v1.0",
            test_data = [],
            config = {},
            created_at = datetime.fromtimestamp(old_time),
            status = "completed"
        )
        benchmark_executor.active_tasks["old - 123"] = old_task

        # 添加新任务
        new_task = BenchmarkTask(
            task_id = "new - 123",
            benchmark_id = "test",
            model_id = "model",
            model_version = "v1.0",
            test_data = [],
            config = {},
            created_at = datetime.now(),
            status = "running"
        )
        benchmark_executor.active_tasks["new - 123"] = new_task

        benchmark_executor.cleanup_old_tasks(max_age_hours = 24)

        # 旧任务应该被清理
        assert "old - 123" not in benchmark_executor.active_tasks
        # 新任务应该保留
        assert "new - 123" in benchmark_executor.active_tasks

    @pytest.mark.asyncio
    async def test_generate_report(self, benchmark_executor):
        """测试生成报告"""
        result = BenchmarkResult(
            task_id = "test - 123",
            benchmark_id = "sentiment - analysis",
            model_id = "bert - base",
            model_version = "v1.0",
            metrics = {"accuracy": 0.95, "f1": 0.93},
            predictions = [],
            execution_time = 120.0,
            timestamp = datetime.now(),
            metadata = {}
        )

        benchmark_executor.completed_tasks["test - 123"] = result

        with patch.object(benchmark_executor, '_generate_html_report', return_value = "report.html"):
            report_path = await benchmark_executor.generate_report("test - 123")

            assert report_path == "report.html"

    @pytest.mark.asyncio
    async def test_generate_report_not_found(self, benchmark_executor):
        """测试生成不存在任务的报告"""
        report_path = await benchmark_executor.generate_report("nonexistent")
        assert report_path is None


class TestRetryAndCircuitBreaker:
    """重试和熔断器测试"""

    @pytest.mark.asyncio
    async def test_retry_mechanism(self, benchmark_executor):
        """测试重试机制"""
        task = BenchmarkTask(
            task_id = "test - 123",
            benchmark_id = "test",
            model_id = "model",
            model_version = "v1.0",
            test_data = [{"input": "test"}],
            config = {},
            created_at = datetime.now()
        )

        call_count = 0

        async def failing_function() - > None:
            nonlocal call_count
            call_count + = 1
            if call_count < 3:
                raise Exception("临时失败")
            return {"metrics": {"accuracy": 0.9}, "predictions": [], "metadata": {}}

        with patch.object(benchmark_executor, '_run_benchmark', side_effect = failing_function):
            result = await benchmark_executor._run_benchmark_with_retry(task)

            assert call_count == 3  # 重试了2次
            assert result["metrics"]["accuracy"] == 0.9


if __name__ == "__main__":
    pytest.main([__file__, " - v"])