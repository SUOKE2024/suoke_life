"""
voice_processor - 索克生活项目模块
"""

from ..models.audio_models import AudioFeature, VoiceFeatures, AudioMetadata
from ..utils.performance import async_timer
from typing import Dict, List, Optional, Tuple
import asyncio
import numpy as np
import structlog

"""
语音处理器

专门处理语音信号的预处理、特征提取和质量评估。
"""



logger = structlog.get_logger(__name__)


class VoiceProcessor:
    """语音处理器"""

    def __init__(self) - > None:
        """TODO: 添加文档字符串"""
        self.sample_rate = 16000  # 标准采样率
        self.frame_length = 1024
        self.hop_length = 512

    @async_timer
    async def process_voice(
        self,
        audio_data: np.ndarray,
        sample_rate: int = 16000
    ) - > VoiceFeatures:
        """
        处理语音数据并提取特征

        Args:
            audio_data: 音频数据
            sample_rate: 采样率

        Returns:
            语音特征
        """
        try:
            # 预处理音频
            processed_audio = await self._preprocess_audio(audio_data, sample_rate)

            # 提取各类特征
            mfcc_features = await self._extract_mfcc_features(processed_audio)
            spectral_features = await self._extract_spectral_features(processed_audio)
            prosodic_features = await self._extract_prosodic_features(processed_audio)
            pitch_features = await self._extract_pitch_features(processed_audio)
            intensity_features = await self._extract_intensity_features(processed_audio)
            rhythm_features = await self._extract_rhythm_features(processed_audio)
            clarity_features = await self._extract_clarity_features(processed_audio)

            return VoiceFeatures(
                mfcc_features = mfcc_features,
                spectral_features = spectral_features,
                prosodic_features = prosodic_features,
                pitch_features = pitch_features,
                intensity_features = intensity_features,
                rhythm_features = rhythm_features,
                clarity_features = clarity_features
            )

        except Exception as e:
            logger.error("语音处理失败", error = str(e), exc_info = True)
            # 返回默认特征
            return VoiceFeatures(
                mfcc_features = [0.0] * 13,
                spectral_features = {},
                prosodic_features = {},
                pitch_features = {},
                intensity_features = {},
                rhythm_features = {},
                clarity_features = {}
            )

    async def _preprocess_audio(
        self,
        audio_data: np.ndarray,
        sample_rate: int
    ) - > np.ndarray:
        """预处理音频数据"""
        # 重采样到标准采样率
        if sample_rate ! = self.sample_rate:
            audio_data = self._resample(audio_data, sample_rate, self.sample_rate)

        # 归一化
        audio_data = self._normalize(audio_data)

        # 去除静音段
        audio_data = self._remove_silence(audio_data)

        # 预加重
        audio_data = self._preemphasis(audio_data)

        return audio_data

    def _resample(
        self,
        audio_data: np.ndarray,
        orig_sr: int,
        target_sr: int
    ) - > np.ndarray:
        """重采样音频"""
        # 简化的重采样实现
        if orig_sr == target_sr:
            return audio_data

        ratio = target_sr / orig_sr
        new_length = int(len(audio_data) * ratio)

        # 线性插值重采样
        old_indices = np.linspace(0, len(audio_data) - 1, len(audio_data))
        new_indices = np.linspace(0, len(audio_data) - 1, new_length)

        return np.interp(new_indices, old_indices, audio_data)

    def _normalize(self, audio_data: np.ndarray) - > np.ndarray:
        """归一化音频"""
        max_val = np.max(np.abs(audio_data))
        if max_val > 0:
            return audio_data / max_val
        return audio_data

    def _remove_silence(
        self,
        audio_data: np.ndarray,
        threshold: float = 0.01
    ) - > np.ndarray:
        """去除静音段"""
        # 计算能量
        frame_length = 1024
        energy = []

        for i in range(0, len(audio_data) - frame_length, frame_length / / 2):
            frame = audio_data[i:i + frame_length]
            energy.append(np.sum(frame * * 2))

        # 找到非静音段
        energy = np.array(energy)
        non_silent = energy > threshold * np.max(energy)

        if np.any(non_silent):
            start_idx = np.where(non_silent)[0][0] * (frame_length / / 2)
            end_idx = np.where(non_silent)[0][ - 1] * (frame_length / / 2) + frame_length
            return audio_data[start_idx:end_idx]

        return audio_data

    def _preemphasis(
        self,
        audio_data: np.ndarray,
        coeff: float = 0.97
    ) - > np.ndarray:
        """预加重滤波"""
        return np.append(audio_data[0], audio_data[1:] - coeff * audio_data[: - 1])

    async def _extract_mfcc_features(
        self,
        audio_data: np.ndarray
    ) - > List[float]:
        """提取MFCC特征"""
        try:
            # 简化的MFCC实现
            # 实际应用中应使用librosa.feature.mfcc

            # 计算功率谱
            fft = np.fft.fft(audio_data, n = 2048)
            power_spectrum = np.abs(fft) * * 2

            # Mel滤波器组
            mel_filters = self._create_mel_filters(power_spectrum.shape[0])

            # 应用Mel滤波器
            mel_spectrum = np.dot(mel_filters, power_spectrum[:len(mel_filters[0])])

            # 对数变换
            log_mel = np.log(mel_spectrum + 1e - 10)

            # DCT变换
            mfcc = self._dct(log_mel)[:13]

            return mfcc.tolist()

        except Exception as e:
            logger.error("MFCC特征提取失败", error = str(e))
            return [0.0] * 13

    def _create_mel_filters(self, n_fft: int, n_mels: int = 26) - > np.ndarray:
        """创建Mel滤波器组"""
        # 简化的Mel滤波器实现
        mel_filters = np.zeros((n_mels, n_fft / / 2 + 1))

        # Mel频率点
        mel_points = np.linspace(0, 2595 * np.log10(1 + self.sample_rate / 2 / 700), n_mels + 2)
        hz_points = 700 * (10 * * (mel_points / 2595) - 1)
        bin_points = np.floor((n_fft + 1) * hz_points / self.sample_rate).astype(int)

        for i in range(1, n_mels + 1):
            left = bin_points[i - 1]
            center = bin_points[i]
            right = bin_points[i + 1]

            for j in range(left, center):
                if center ! = left:
                    mel_filters[i - 1, j] = (j - left) / (center - left)

            for j in range(center, right):
                if right ! = center:
                    mel_filters[i - 1, j] = (right - j) / (right - center)

        return mel_filters

    def _dct(self, x: np.ndarray) - > np.ndarray:
        """离散余弦变换"""
        N = len(x)
        result = np.zeros(N)

        for k in range(N):
            sum_val = 0
            for n in range(N):
                sum_val + = x[n] * np.cos(np.pi * k * (2 * n + 1) / (2 * N))
            result[k] = sum_val

        return result

    async def _extract_spectral_features(
        self,
        audio_data: np.ndarray
    ) - > Dict[str, float]:
        """提取频谱特征"""
        try:
            # 计算频谱
            fft = np.fft.fft(audio_data)
            magnitude = np.abs(fft)
            power = magnitude * * 2

            # 频率轴
            freqs = np.fft.fftfreq(len(audio_data), 1 / self.sample_rate)
            positive_freqs = freqs[:len(freqs) / / 2]
            positive_power = power[:len(power) / / 2]

            # 频谱质心
            spectral_centroid = np.sum(positive_freqs * positive_power) / np.sum(positive_power)

            # 频谱带宽
            spectral_bandwidth = np.sqrt(
                np.sum(((positive_freqs - spectral_centroid) * * 2) * positive_power) /
                np.sum(positive_power)
            )

            # 频谱滚降
            cumsum = np.cumsum(positive_power)
            rolloff_idx = np.where(cumsum > = 0.85 * cumsum[ - 1])[0][0]
            spectral_rolloff = positive_freqs[rolloff_idx]

            # 过零率
            zero_crossings = np.where(np.diff(np.sign(audio_data)))[0]
            zero_crossing_rate = len(zero_crossings) / len(audio_data)

            return {
                "spectral_centroid": float(spectral_centroid),
                "spectral_bandwidth": float(spectral_bandwidth),
                "spectral_rolloff": float(spectral_rolloff),
                "zero_crossing_rate": float(zero_crossing_rate),
                "spectral_flatness": float(self._calculate_spectral_flatness(positive_power)),
                "spectral_flux": float(self._calculate_spectral_flux(positive_power))
            }

        except Exception as e:
            logger.error("频谱特征提取失败", error = str(e))
            return {}

    def _calculate_spectral_flatness(self, power_spectrum: np.ndarray) - > float:
        """计算频谱平坦度"""
        geometric_mean = np.exp(np.mean(np.log(power_spectrum + 1e - 10)))
        arithmetic_mean = np.mean(power_spectrum)
        return geometric_mean / arithmetic_mean if arithmetic_mean > 0 else 0.0

    def _calculate_spectral_flux(self, power_spectrum: np.ndarray) - > float:
        """计算频谱通量"""
        # 简化实现，实际需要时间序列
        return float(np.std(power_spectrum))

    async def _extract_prosodic_features(
        self,
        audio_data: np.ndarray
    ) - > Dict[str, float]:
        """提取韵律特征"""
        try:
            # 基音频率
            pitch = self._estimate_pitch(audio_data)

            # 强度
            intensity = self._calculate_intensity(audio_data)

            # 语速估计
            speaking_rate = self._estimate_speaking_rate(audio_data)

            return {
                "pitch_mean": float(np.mean(pitch[pitch > 0])) if np.any(pitch > 0) else 0.0,
                "pitch_std": float(np.std(pitch[pitch > 0])) if np.any(pitch > 0) else 0.0,
                "pitch_range": float(np.max(pitch) - np.min(pitch[pitch > 0])) if np.any(pitch > 0) else 0.0,
                "intensity_mean": float(np.mean(intensity)),
                "intensity_std": float(np.std(intensity)),
                "speaking_rate": float(speaking_rate)
            }

        except Exception as e:
            logger.error("韵律特征提取失败", error = str(e))
            return {}

    def _estimate_pitch(self, audio_data: np.ndarray) - > np.ndarray:
        """估计基音频率"""
        # 简化的基音估计算法
        frame_length = 1024
        hop_length = 512
        pitch = []

        for i in range(0, len(audio_data) - frame_length, hop_length):
            frame = audio_data[i:i + frame_length]

            # 自相关法估计基音
            autocorr = np.correlate(frame, frame, mode = 'full')
            autocorr = autocorr[len(autocorr) / / 2:]

            # 寻找峰值
            min_period = int(self.sample_rate / 500)  # 最高500Hz
            max_period = int(self.sample_rate / 50)   # 最低50Hz

            if len(autocorr) > max_period:
                peak_idx = np.argmax(autocorr[min_period:max_period]) + min_period
                if autocorr[peak_idx] > 0.3 * autocorr[0]:  # 阈值判断
                    pitch.append(self.sample_rate / peak_idx)
                else:
                    pitch.append(0.0)
            else:
                pitch.append(0.0)

        return np.array(pitch)

    def _calculate_intensity(self, audio_data: np.ndarray) - > np.ndarray:
        """计算强度"""
        frame_length = 1024
        hop_length = 512
        intensity = []

        for i in range(0, len(audio_data) - frame_length, hop_length):
            frame = audio_data[i:i + frame_length]
            rms = np.sqrt(np.mean(frame * * 2))
            intensity.append(20 * np.log10(rms + 1e - 10))  # 转换为dB

        return np.array(intensity)

    def _estimate_speaking_rate(self, audio_data: np.ndarray) - > float:
        """估计语速"""
        # 简化的语速估计
        # 基于音节检测
        frame_length = 1024
        hop_length = 512

        # 计算能量包络
        energy = []
        for i in range(0, len(audio_data) - frame_length, hop_length):
            frame = audio_data[i:i + frame_length]
            energy.append(np.sum(frame * * 2))

        energy = np.array(energy)

        # 平滑处理
        smoothed_energy = np.convolve(energy, np.ones(5) / 5, mode = 'same')

        # 检测峰值（音节）
        threshold = 0.1 * np.max(smoothed_energy)
        peaks = []

        for i in range(1, len(smoothed_energy) - 1):
            if (smoothed_energy[i] > smoothed_energy[i - 1] and
                smoothed_energy[i] > smoothed_energy[i + 1] and
                smoothed_energy[i] > threshold):
                peaks.append(i)

        # 计算语速（音节 / 秒）
        duration = len(audio_data) / self.sample_rate
        syllables_per_second = len(peaks) / duration if duration > 0 else 0.0

        return syllables_per_second

    async def _extract_pitch_features(
        self,
        audio_data: np.ndarray
    ) - > Dict[str, float]:
        """提取基音特征"""
        try:
            pitch = self._estimate_pitch(audio_data)
            valid_pitch = pitch[pitch > 0]

            if len(valid_pitch) == 0:
                return {}

            return {
                "f0_mean": float(np.mean(valid_pitch)),
                "f0_std": float(np.std(valid_pitch)),
                "f0_min": float(np.min(valid_pitch)),
                "f0_max": float(np.max(valid_pitch)),
                "f0_range": float(np.max(valid_pitch) - np.min(valid_pitch)),
                "f0_median": float(np.median(valid_pitch)),
                "voiced_ratio": float(len(valid_pitch) / len(pitch))
            }

        except Exception as e:
            logger.error("基音特征提取失败", error = str(e))
            return {}

    async def _extract_intensity_features(
        self,
        audio_data: np.ndarray
    ) - > Dict[str, float]:
        """提取强度特征"""
        try:
            intensity = self._calculate_intensity(audio_data)

            return {
                "intensity_mean": float(np.mean(intensity)),
                "intensity_std": float(np.std(intensity)),
                "intensity_min": float(np.min(intensity)),
                "intensity_max": float(np.max(intensity)),
                "intensity_range": float(np.max(intensity) - np.min(intensity)),
                "intensity_median": float(np.median(intensity))
            }

        except Exception as e:
            logger.error("强度特征提取失败", error = str(e))
            return {}

    async def _extract_rhythm_features(
        self,
        audio_data: np.ndarray
    ) - > Dict[str, float]:
        """提取节律特征"""
        try:
            # 计算节拍和节律
            tempo = self._estimate_tempo(audio_data)
            rhythm_regularity = self._calculate_rhythm_regularity(audio_data)

            return {
                "tempo": float(tempo),
                "rhythm_regularity": float(rhythm_regularity),
                "speaking_rate": float(self._estimate_speaking_rate(audio_data))
            }

        except Exception as e:
            logger.error("节律特征提取失败", error = str(e))
            return {}

    def _estimate_tempo(self, audio_data: np.ndarray) - > float:
        """估计节拍"""
        # 简化的节拍估计
        onset_strength = self._calculate_onset_strength(audio_data)

        # 自相关分析
        autocorr = np.correlate(onset_strength, onset_strength, mode = 'full')
        autocorr = autocorr[len(autocorr) / / 2:]

        # 寻找周期性
        min_tempo = 60  # BPM
        max_tempo = 200  # BPM

        hop_length = 512
        sr = self.sample_rate

        min_period = int(60 * sr / hop_length / max_tempo)
        max_period = int(60 * sr / hop_length / min_tempo)

        if len(autocorr) > max_period:
            peak_idx = np.argmax(autocorr[min_period:max_period]) + min_period
            tempo = 60 * sr / hop_length / peak_idx
            return tempo

        return 120.0  # 默认节拍

    def _calculate_onset_strength(self, audio_data: np.ndarray) - > np.ndarray:
        """计算起始强度"""
        frame_length = 1024
        hop_length = 512

        # 计算频谱差异
        prev_spectrum = None
        onset_strength = []

        for i in range(0, len(audio_data) - frame_length, hop_length):
            frame = audio_data[i:i + frame_length]
            spectrum = np.abs(np.fft.fft(frame))

            if prev_spectrum is not None:
                diff = np.sum(np.maximum(0, spectrum - prev_spectrum))
                onset_strength.append(diff)
            else:
                onset_strength.append(0.0)

            prev_spectrum = spectrum

        return np.array(onset_strength)

    def _calculate_rhythm_regularity(self, audio_data: np.ndarray) - > float:
        """计算节律规律性"""
        onset_strength = self._calculate_onset_strength(audio_data)

        # 计算间隔的变异系数
        peaks = []
        threshold = 0.3 * np.max(onset_strength)

        for i in range(1, len(onset_strength) - 1):
            if (onset_strength[i] > onset_strength[i - 1] and
                onset_strength[i] > onset_strength[i + 1] and
                onset_strength[i] > threshold):
                peaks.append(i)

        if len(peaks) < 2:
            return 0.0

        intervals = np.diff(peaks)
        if len(intervals) == 0:
            return 0.0

        cv = np.std(intervals) / np.mean(intervals) if np.mean(intervals) > 0 else 1.0
        regularity = 1.0 / (1.0 + cv)  # 变异系数越小，规律性越高

        return regularity

    async def _extract_clarity_features(
        self,
        audio_data: np.ndarray
    ) - > Dict[str, float]:
        """提取清晰度特征"""
        try:
            # 计算语音质量指标
            snr = self._calculate_snr(audio_data)
            jitter = self._calculate_jitter(audio_data)
            shimmer = self._calculate_shimmer(audio_data)
            hnr = self._calculate_hnr(audio_data)

            return {
                "snr": float(snr),
                "jitter": float(jitter),
                "shimmer": float(shimmer),
                "hnr": float(hnr),
                "clarity_score": float((snr + hnr) / 2)
            }

        except Exception as e:
            logger.error("清晰度特征提取失败", error = str(e))
            return {}

    def _calculate_snr(self, audio_data: np.ndarray) - > float:
        """计算信噪比"""
        # 简化的SNR计算
        signal_power = np.mean(audio_data * * 2)

        # 估计噪声（假设前后10%为噪声）
        noise_length = len(audio_data) / / 10
        noise_start = audio_data[:noise_length]
        noise_end = audio_data[ - noise_length:]
        noise_power = np.mean(np.concatenate([noise_start, noise_end]) * * 2)

        if noise_power > 0:
            snr = 10 * np.log10(signal_power / noise_power)
            return max(snr, 0.0)

        return 40.0  # 默认高SNR

    def _calculate_jitter(self, audio_data: np.ndarray) - > float:
        """计算基音抖动"""
        pitch = self._estimate_pitch(audio_data)
        valid_pitch = pitch[pitch > 0]

        if len(valid_pitch) < 2:
            return 0.0

        # 计算相邻周期的差异
        period_diffs = np.abs(np.diff(1.0 / valid_pitch))
        mean_period = np.mean(1.0 / valid_pitch)

        jitter = np.mean(period_diffs) / mean_period if mean_period > 0 else 0.0
        return min(jitter, 1.0)

    def _calculate_shimmer(self, audio_data: np.ndarray) - > float:
        """计算振幅抖动"""
        intensity = self._calculate_intensity(audio_data)

        if len(intensity) < 2:
            return 0.0

        # 计算相邻帧的振幅差异
        amplitude_diffs = np.abs(np.diff(intensity))
        mean_amplitude = np.mean(intensity)

        shimmer = np.mean(amplitude_diffs) / abs(mean_amplitude) if mean_amplitude ! = 0 else 0.0
        return min(shimmer, 1.0)

    def _calculate_hnr(self, audio_data: np.ndarray) - > float:
        """计算谐噪比"""
        # 简化的HNR计算
        frame_length = 1024
        hnr_values = []

        for i in range(0, len(audio_data) - frame_length, frame_length / / 2):
            frame = audio_data[i:i + frame_length]

            # 自相关
            autocorr = np.correlate(frame, frame, mode = 'full')
            autocorr = autocorr[len(autocorr) / / 2:]

            if len(autocorr) > 1:
                # 谐波能量 vs 噪声能量
                harmonic_energy = autocorr[0]
                noise_energy = np.sum(autocorr[1:]) / (len(autocorr) - 1)

                if noise_energy > 0:
                    hnr = 10 * np.log10(harmonic_energy / noise_energy)
                    hnr_values.append(max(hnr, 0.0))

        return np.mean(hnr_values) if hnr_values else 20.0