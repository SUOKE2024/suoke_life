#!/usr/bin/env python3
"""
é‡å¤ä»£ç æ£€æµ‹å·¥å…·
æ£€æµ‹é¡¹ç›®ä¸­çš„é‡å¤ä»£ç å—ï¼Œæä¾›é‡æ„å»ºè®®
"""

import ast
import difflib
import hashlib
import json
import os
import sys
from collections import Counter, defaultdict
from datetime import datetime
from typing import Any, Dict, List, Set, Tuple


class CodeBlock:
    """ä»£ç å—ç±»"""

    def __init__(
        self,
        content: str,
        file_path: str,
        start_line: int,
        end_line: int,
        block_type: str = "function",
    ):
        self.content = content.strip()
        self.file_path = file_path
        self.start_line = start_line
        self.end_line = end_line
        self.block_type = block_type
        self.hash = self._calculate_hash()
        self.normalized_content = self._normalize_content()

    def _calculate_hash(self) -> str:
        """è®¡ç®—ä»£ç å—å“ˆå¸Œå€¼"""
        return hashlib.md5(self.content.encode()).hexdigest()

    def _normalize_content(self) -> str:
        """æ ‡å‡†åŒ–ä»£ç å†…å®¹ï¼ˆç§»é™¤ç©ºç™½ã€æ³¨é‡Šç­‰ï¼‰"""
        lines = []
        for line in self.content.split("\n"):
            line = line.strip()
            if line and not line.startswith("#"):
                # ç§»é™¤å¤šä½™ç©ºæ ¼
                line = " ".join(line.split())
                lines.append(line)
        return "\n".join(lines)


class DuplicateCodeDetector:
    """é‡å¤ä»£ç æ£€æµ‹å™¨"""

    def __init__(self, project_root: str):
        self.project_root = project_root
        self.code_blocks: List[CodeBlock] = []
        self.duplicates: Dict[str, List[CodeBlock]] = defaultdict(list)
        self.similarity_threshold = 0.8
        self.min_lines = 5  # æœ€å°è¡Œæ•°é˜ˆå€¼

    def scan_project(self) -> Dict[str, Any]:
        """æ‰«ææ•´ä¸ªé¡¹ç›®"""
        print("ğŸ” å¼€å§‹æ‰«æé¡¹ç›®ä¸­çš„é‡å¤ä»£ç ...")

        stats = {
            "total_files": 0,
            "scanned_files": 0,
            "total_blocks": 0,
            "duplicate_groups": 0,
            "total_duplicates": 0,
            "potential_savings": 0,
        }

        # æ‰«æPythonæ–‡ä»¶
        for root, dirs, files in os.walk(self.project_root):
            # è·³è¿‡ç‰¹å®šç›®å½•
            dirs[:] = [
                d
                for d in dirs
                if not d.startswith(".")
                and d
                not in ["__pycache__", "node_modules", "venv", "env", "build", "dist"]
            ]

            for file in files:
                if file.endswith(".py"):
                    stats["total_files"] += 1
                    file_path = os.path.join(root, file)
                    try:
                        self._analyze_file(file_path)
                        stats["scanned_files"] += 1
                    except Exception as e:
                        print(f"âš ï¸  åˆ†ææ–‡ä»¶å¤±è´¥ {file_path}: {e}")

        # æ£€æµ‹é‡å¤ä»£ç 
        self._detect_duplicates()

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        stats["total_blocks"] = len(self.code_blocks)
        stats["duplicate_groups"] = len(self.duplicates)
        stats["total_duplicates"] = sum(
            len(blocks) for blocks in self.duplicates.values()
        )
        stats["potential_savings"] = self._calculate_potential_savings()

        return stats

    def _analyze_file(self, file_path: str):
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            # è§£æAST
            tree = ast.parse(content)

            # æå–å‡½æ•°å’Œç±»
            for node in ast.walk(tree):
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    self._extract_function_block(node, content, file_path)
                elif isinstance(node, ast.ClassDef):
                    self._extract_class_block(node, content, file_path)

        except Exception as e:
            # å¦‚æœASTè§£æå¤±è´¥ï¼Œå°è¯•ç®€å•çš„è¡Œåˆ†æ
            self._analyze_file_by_lines(file_path)

    def _extract_function_block(
        self, node: ast.FunctionDef, content: str, file_path: str
    ):
        """æå–å‡½æ•°ä»£ç å—"""
        lines = content.split("\n")
        start_line = node.lineno
        end_line = node.end_lineno if hasattr(node, "end_lineno") else start_line + 10

        if end_line - start_line >= self.min_lines:
            block_content = "\n".join(lines[start_line - 1 : end_line])
            block = CodeBlock(
                block_content, file_path, start_line, end_line, "function"
            )
            self.code_blocks.append(block)

    def _extract_class_block(self, node: ast.ClassDef, content: str, file_path: str):
        """æå–ç±»ä»£ç å—"""
        lines = content.split("\n")
        start_line = node.lineno
        end_line = node.end_lineno if hasattr(node, "end_lineno") else start_line + 20

        if end_line - start_line >= self.min_lines:
            block_content = "\n".join(lines[start_line - 1 : end_line])
            block = CodeBlock(block_content, file_path, start_line, end_line, "class")
            self.code_blocks.append(block)

    def _analyze_file_by_lines(self, file_path: str):
        """æŒ‰è¡Œåˆ†ææ–‡ä»¶ï¼ˆASTè§£æå¤±è´¥æ—¶çš„å¤‡é€‰æ–¹æ¡ˆï¼‰"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                lines = f.readlines()

            # æŸ¥æ‰¾è¿ç»­çš„ä»£ç å—
            current_block = []
            start_line = 0

            for i, line in enumerate(lines):
                line = line.strip()
                if line and not line.startswith("#"):
                    if not current_block:
                        start_line = i + 1
                    current_block.append(line)
                else:
                    if len(current_block) >= self.min_lines:
                        block_content = "\n".join(current_block)
                        block = CodeBlock(
                            block_content, file_path, start_line, i, "block"
                        )
                        self.code_blocks.append(block)
                    current_block = []

            # å¤„ç†æ–‡ä»¶æœ«å°¾çš„ä»£ç å—
            if len(current_block) >= self.min_lines:
                block_content = "\n".join(current_block)
                block = CodeBlock(
                    block_content, file_path, start_line, len(lines), "block"
                )
                self.code_blocks.append(block)

        except Exception as e:
            print(f"âš ï¸  æŒ‰è¡Œåˆ†ææ–‡ä»¶å¤±è´¥ {file_path}: {e}")

    def _detect_duplicates(self):
        """æ£€æµ‹é‡å¤ä»£ç """
        print("ğŸ” æ£€æµ‹é‡å¤ä»£ç ...")

        # æŒ‰å“ˆå¸Œå€¼åˆ†ç»„
        hash_groups = defaultdict(list)
        for block in self.code_blocks:
            hash_groups[block.hash].append(block)

        # æ‰¾å‡ºé‡å¤çš„å“ˆå¸Œç»„
        for hash_value, blocks in hash_groups.items():
            if len(blocks) > 1:
                self.duplicates[hash_value] = blocks

        # æ£€æµ‹ç›¸ä¼¼ä»£ç ï¼ˆä¸å®Œå…¨ç›¸åŒä½†ç›¸ä¼¼åº¦é«˜ï¼‰
        self._detect_similar_code()

    def _detect_similar_code(self):
        """æ£€æµ‹ç›¸ä¼¼ä»£ç """
        print("ğŸ” æ£€æµ‹ç›¸ä¼¼ä»£ç ...")

        processed = set()

        for i, block1 in enumerate(self.code_blocks):
            if block1.hash in processed:
                continue

            similar_blocks = [block1]

            for j, block2 in enumerate(self.code_blocks[i + 1 :], i + 1):
                if block2.hash in processed:
                    continue

                similarity = self._calculate_similarity(
                    block1.normalized_content, block2.normalized_content
                )
                if similarity >= self.similarity_threshold:
                    similar_blocks.append(block2)

            if len(similar_blocks) > 1:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªå—çš„å“ˆå¸Œä½œä¸ºç»„æ ‡è¯†
                group_key = f"similar_{block1.hash}"
                self.duplicates[group_key] = similar_blocks
                for block in similar_blocks:
                    processed.add(block.hash)

    def _calculate_similarity(self, content1: str, content2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªä»£ç å—çš„ç›¸ä¼¼åº¦"""
        if not content1 or not content2:
            return 0.0

        # ä½¿ç”¨difflibè®¡ç®—ç›¸ä¼¼åº¦
        similarity = difflib.SequenceMatcher(None, content1, content2).ratio()
        return similarity

    def _calculate_potential_savings(self) -> int:
        """è®¡ç®—æ½œåœ¨çš„ä»£ç è¡Œæ•°èŠ‚çœ"""
        savings = 0
        for blocks in self.duplicates.values():
            if len(blocks) > 1:
                # å‡è®¾å¯ä»¥å°†é‡å¤ä»£ç é‡æ„ä¸ºä¸€ä¸ªå‡½æ•°
                max_lines = max(block.end_line - block.start_line for block in blocks)
                savings += (len(blocks) - 1) * max_lines
        return savings

    def generate_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆé‡å¤ä»£ç æ£€æµ‹æŠ¥å‘Š"""
        print("ğŸ“Š ç”Ÿæˆé‡å¤ä»£ç æ£€æµ‹æŠ¥å‘Š...")

        report = {
            "scan_time": datetime.now().isoformat(),
            "summary": {
                "total_files": len(set(block.file_path for block in self.code_blocks)),
                "total_blocks": len(self.code_blocks),
                "duplicate_groups": len(self.duplicates),
                "total_duplicates": sum(
                    len(blocks) for blocks in self.duplicates.values()
                ),
                "potential_savings": self._calculate_potential_savings(),
            },
            "duplicate_groups": [],
            "recommendations": [],
        }

        # æŒ‰é‡å¤æ¬¡æ•°æ’åº
        sorted_duplicates = sorted(
            self.duplicates.items(), key=lambda x: len(x[1]), reverse=True
        )

        for group_id, blocks in sorted_duplicates:
            group_info = {
                "group_id": group_id,
                "duplicate_count": len(blocks),
                "block_type": blocks[0].block_type,
                "lines_per_block": blocks[0].end_line - blocks[0].start_line,
                "total_duplicate_lines": sum(
                    block.end_line - block.start_line for block in blocks
                ),
                "locations": [],
            }

            for block in blocks:
                location = {
                    "file": os.path.relpath(block.file_path, self.project_root),
                    "start_line": block.start_line,
                    "end_line": block.end_line,
                    "preview": (
                        block.content[:200] + "..."
                        if len(block.content) > 200
                        else block.content
                    ),
                }
                group_info["locations"].append(location)

            report["duplicate_groups"].append(group_info)

        # ç”Ÿæˆé‡æ„å»ºè®®
        report["recommendations"] = self._generate_recommendations()

        return report

    def _generate_recommendations(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆé‡æ„å»ºè®®"""
        recommendations = []

        # é«˜ä¼˜å…ˆçº§ï¼šå¤§é‡é‡å¤çš„ä»£ç å—
        high_priority = [
            (group_id, blocks)
            for group_id, blocks in self.duplicates.items()
            if len(blocks) >= 3 and (blocks[0].end_line - blocks[0].start_line) >= 10
        ]

        if high_priority:
            recommendations.append(
                {
                    "priority": "HIGH",
                    "title": "æå–å…¬å…±å‡½æ•°",
                    "description": f"å‘ç° {len(high_priority)} ç»„é«˜åº¦é‡å¤çš„ä»£ç å—ï¼Œå»ºè®®æå–ä¸ºå…¬å…±å‡½æ•°",
                    "action": "å°†é‡å¤ä»£ç æå–ä¸ºç‹¬ç«‹å‡½æ•°ï¼Œå¹¶åœ¨åŸä½ç½®è°ƒç”¨",
                    "estimated_savings": sum(
                        (len(blocks) - 1) * (blocks[0].end_line - blocks[0].start_line)
                        for _, blocks in high_priority
                    ),
                }
            )

        # ä¸­ä¼˜å…ˆçº§ï¼šä¸­ç­‰é‡å¤çš„ä»£ç å—
        medium_priority = [
            (group_id, blocks)
            for group_id, blocks in self.duplicates.items()
            if len(blocks) == 2 and (blocks[0].end_line - blocks[0].start_line) >= 5
        ]

        if medium_priority:
            recommendations.append(
                {
                    "priority": "MEDIUM",
                    "title": "é‡æ„ç›¸ä¼¼ä»£ç ",
                    "description": f"å‘ç° {len(medium_priority)} ç»„ç›¸ä¼¼ä»£ç å—ï¼Œå»ºè®®é‡æ„",
                    "action": "åˆ†æä»£ç å·®å¼‚ï¼Œæå–å…¬å…±éƒ¨åˆ†ï¼Œå‚æ•°åŒ–å·®å¼‚éƒ¨åˆ†",
                    "estimated_savings": sum(
                        (blocks[0].end_line - blocks[0].start_line) // 2
                        for _, blocks in medium_priority
                    ),
                }
            )

        # ä½ä¼˜å…ˆçº§ï¼šå°çš„é‡å¤ä»£ç å—
        low_priority = [
            (group_id, blocks)
            for group_id, blocks in self.duplicates.items()
            if len(blocks) >= 2 and (blocks[0].end_line - blocks[0].start_line) < 5
        ]

        if low_priority:
            recommendations.append(
                {
                    "priority": "LOW",
                    "title": "åˆ›å»ºå·¥å…·å‡½æ•°",
                    "description": f"å‘ç° {len(low_priority)} ç»„å°çš„é‡å¤ä»£ç ç‰‡æ®µ",
                    "action": "è€ƒè™‘åˆ›å»ºå·¥å…·å‡½æ•°æˆ–å¸¸é‡æ¥å‡å°‘é‡å¤",
                    "estimated_savings": len(low_priority) * 2,
                }
            )

        return recommendations


def main():
    """ä¸»å‡½æ•°"""
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

    print("ğŸš€ å¯åŠ¨é‡å¤ä»£ç æ£€æµ‹å·¥å…·...")
    print(f"ğŸ“ é¡¹ç›®æ ¹ç›®å½•: {project_root}")

    detector = DuplicateCodeDetector(project_root)

    # æ‰«æé¡¹ç›®
    stats = detector.scan_project()

    # ç”ŸæˆæŠ¥å‘Š
    report = detector.generate_report()

    # ä¿å­˜æŠ¥å‘Š
    report_file = os.path.join(project_root, "duplicate_code_report.md")

    with open(report_file, "w", encoding="utf-8") as f:
        f.write("# é‡å¤ä»£ç æ£€æµ‹æŠ¥å‘Š\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {report['scan_time']}\n\n")

        # æ‘˜è¦
        f.write("## ğŸ“Š æ£€æµ‹æ‘˜è¦\n\n")
        summary = report["summary"]
        f.write(f"- **æ‰«ææ–‡ä»¶æ•°**: {summary['total_files']}\n")
        f.write(f"- **ä»£ç å—æ€»æ•°**: {summary['total_blocks']}\n")
        f.write(f"- **é‡å¤ç»„æ•°**: {summary['duplicate_groups']}\n")
        f.write(f"- **é‡å¤ä»£ç å—æ•°**: {summary['total_duplicates']}\n")
        f.write(f"- **æ½œåœ¨èŠ‚çœè¡Œæ•°**: {summary['potential_savings']}\n\n")

        # é‡å¤ä»£ç ç»„
        if report["duplicate_groups"]:
            f.write("## ğŸ” é‡å¤ä»£ç ç»„\n\n")
            for i, group in enumerate(
                report["duplicate_groups"][:10], 1
            ):  # åªæ˜¾ç¤ºå‰10ç»„
                f.write(f"### ç»„ {i}: {group['duplicate_count']} ä¸ªé‡å¤\n\n")
                f.write(f"- **ç±»å‹**: {group['block_type']}\n")
                f.write(f"- **æ¯å—è¡Œæ•°**: {group['lines_per_block']}\n")
                f.write(f"- **æ€»é‡å¤è¡Œæ•°**: {group['total_duplicate_lines']}\n\n")

                f.write("**ä½ç½®**:\n")
                for location in group["locations"]:
                    f.write(
                        f"- `{location['file']}` (è¡Œ {location['start_line']}-{location['end_line']})\n"
                    )

                f.write(
                    f"\n**ä»£ç é¢„è§ˆ**:\n```python\n{group['locations'][0]['preview']}\n```\n\n"
                )

        # é‡æ„å»ºè®®
        if report["recommendations"]:
            f.write("## ğŸ’¡ é‡æ„å»ºè®®\n\n")
            for rec in report["recommendations"]:
                f.write(f"### {rec['priority']} ä¼˜å…ˆçº§: {rec['title']}\n\n")
                f.write(f"**æè¿°**: {rec['description']}\n\n")
                f.write(f"**å»ºè®®è¡ŒåŠ¨**: {rec['action']}\n\n")
                f.write(f"**é¢„è®¡èŠ‚çœè¡Œæ•°**: {rec['estimated_savings']}\n\n")

    print(f"âœ… é‡å¤ä»£ç æ£€æµ‹å®Œæˆ!")
    print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    print(
        f"ğŸ“Š å‘ç° {summary['duplicate_groups']} ç»„é‡å¤ä»£ç ï¼Œæ½œåœ¨èŠ‚çœ {summary['potential_savings']} è¡Œä»£ç "
    )

    return report


if __name__ == "__main__":
    main()
