#!/usr/bin/env python3
"""
æ€§èƒ½åˆ†æå·¥å…·
åˆ†æä»£ç å¤æ‚åº¦ã€æ£€æµ‹æ€§èƒ½ç“¶é¢ˆå¹¶æä¾›ä¼˜åŒ–å»ºè®®
"""

import ast
import logging
import os
import re
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Set, Tuple

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class PerformanceAnalyzer:
    """æ€§èƒ½åˆ†æå™¨"""

    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.complexity_issues: List[Dict] = []
        self.performance_issues: List[Dict] = []
        self.optimization_suggestions: List[Dict] = []

        # å¿½ç•¥çš„ç›®å½•å’Œæ–‡ä»¶
        self.ignore_dirs = {
            "__pycache__",
            ".git",
            ".pytest_cache",
            "node_modules",
            ".venv",
            "venv",
            "env",
            ".env",
            "build",
            "dist",
            ".idea",
            ".vscode",
            "logs",
            "temp",
            "tmp",
        }
        self.ignore_files = {"__init__.py", "conftest.py", "setup.py", "manage.py"}

        # æ€§èƒ½é—®é¢˜æ¨¡å¼
        self.performance_patterns = {
            "nested_loops": {
                "pattern": r"for\s+\w+\s+in\s+.*:\s*\n\s*for\s+\w+\s+in\s+.*:",
                "severity": "HIGH",
                "description": "åµŒå¥—å¾ªç¯å¯èƒ½å¯¼è‡´æ€§èƒ½é—®é¢˜",
            },
            "string_concatenation": {
                "pattern": r'\w+\s*\+=\s*["\'].*["\']',
                "severity": "MEDIUM",
                "description": "å­—ç¬¦ä¸²æ‹¼æ¥åº”ä½¿ç”¨join()æˆ–f-string",
            },
            "list_comprehension_in_loop": {
                "pattern": r"for\s+\w+\s+in\s+.*:\s*\n\s*.*\[.*for.*in.*\]",
                "severity": "MEDIUM",
                "description": "å¾ªç¯ä¸­çš„åˆ—è¡¨æ¨å¯¼å¼å¯èƒ½å½±å“æ€§èƒ½",
            },
            "global_variable_access": {
                "pattern": r"global\s+\w+",
                "severity": "LOW",
                "description": "å…¨å±€å˜é‡è®¿é—®å¯èƒ½å½±å“æ€§èƒ½",
            },
        }

    def should_ignore_path(self, path: Path) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥å¿½ç•¥è·¯å¾„"""
        # æ£€æŸ¥ç›®å½•
        for part in path.parts:
            if part in self.ignore_dirs or part.startswith("."):
                return True

        # æ£€æŸ¥æ–‡ä»¶
        if path.name in self.ignore_files:
            return True

        # åªå¤„ç†Pythonæ–‡ä»¶
        if path.suffix != ".py":
            return True

        return False

    def calculate_cyclomatic_complexity(self, node: ast.AST) -> int:
        """è®¡ç®—åœˆå¤æ‚åº¦"""
        complexity = 1  # åŸºç¡€å¤æ‚åº¦

        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.While, ast.For, ast.AsyncFor)):
                complexity += 1
            elif isinstance(child, ast.ExceptHandler):
                complexity += 1
            elif isinstance(child, (ast.And, ast.Or)):
                complexity += 1
            elif isinstance(child, ast.comprehension):
                complexity += 1

        return complexity

    def analyze_function_complexity(
        self, node: ast.FunctionDef, file_path: str
    ) -> Dict:
        """åˆ†æå‡½æ•°å¤æ‚åº¦"""
        complexity = self.calculate_cyclomatic_complexity(node)
        lines_count = (node.end_lineno or node.lineno) - node.lineno + 1

        # è®¡ç®—åµŒå¥—æ·±åº¦
        max_depth = self.calculate_nesting_depth(node)

        # è®¡ç®—å‚æ•°æ•°é‡
        param_count = len(node.args.args)
        if node.args.vararg:
            param_count += 1
        if node.args.kwarg:
            param_count += 1

        return {
            "name": node.name,
            "file_path": file_path,
            "line_number": node.lineno,
            "cyclomatic_complexity": complexity,
            "lines_count": lines_count,
            "max_nesting_depth": max_depth,
            "parameter_count": param_count,
            "is_async": isinstance(node, ast.AsyncFunctionDef),
        }

    def calculate_nesting_depth(self, node: ast.AST, current_depth: int = 0) -> int:
        """è®¡ç®—æœ€å¤§åµŒå¥—æ·±åº¦"""
        max_depth = current_depth

        for child in ast.iter_child_nodes(node):
            if isinstance(
                child,
                (
                    ast.If,
                    ast.While,
                    ast.For,
                    ast.AsyncFor,
                    ast.With,
                    ast.AsyncWith,
                    ast.Try,
                ),
            ):
                child_depth = self.calculate_nesting_depth(child, current_depth + 1)
                max_depth = max(max_depth, child_depth)
            else:
                child_depth = self.calculate_nesting_depth(child, current_depth)
                max_depth = max(max_depth, child_depth)

        return max_depth

    def detect_performance_patterns(self, file_path: Path) -> List[Dict]:
        """æ£€æµ‹æ€§èƒ½é—®é¢˜æ¨¡å¼"""
        issues = []

        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            lines = content.split("\n")

            for pattern_name, pattern_info in self.performance_patterns.items():
                matches = re.finditer(pattern_info["pattern"], content, re.MULTILINE)

                for match in matches:
                    # è®¡ç®—è¡Œå·
                    line_number = content[: match.start()].count("\n") + 1

                    issues.append(
                        {
                            "type": pattern_name,
                            "file_path": str(file_path),
                            "line_number": line_number,
                            "severity": pattern_info["severity"],
                            "description": pattern_info["description"],
                            "code_snippet": (
                                lines[line_number - 1].strip()
                                if line_number <= len(lines)
                                else ""
                            ),
                        }
                    )

        except Exception as e:
            logger.warning(f"æ— æ³•åˆ†ææ–‡ä»¶ {file_path}: {e}")

        return issues

    def analyze_imports(self, file_path: Path) -> List[Dict]:
        """åˆ†æå¯¼å…¥æ€§èƒ½é—®é¢˜"""
        issues = []

        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            tree = ast.parse(content)

            for node in ast.walk(tree):
                if isinstance(node, ast.ImportFrom):
                    # æ£€æµ‹é€šé…ç¬¦å¯¼å…¥
                    for alias in node.names:
                        if alias.name == "*":
                            issues.append(
                                {
                                    "type": "wildcard_import",
                                    "file_path": str(file_path),
                                    "line_number": node.lineno,
                                    "severity": "MEDIUM",
                                    "description": f"é€šé…ç¬¦å¯¼å…¥ from {node.module} import * å¯èƒ½å½±å“æ€§èƒ½",
                                    "module": node.module,
                                }
                            )

                elif isinstance(node, ast.Import):
                    # æ£€æµ‹å¤§å‹åº“å¯¼å…¥
                    large_libraries = {
                        "pandas",
                        "numpy",
                        "matplotlib",
                        "tensorflow",
                        "torch",
                    }
                    for alias in node.names:
                        if alias.name in large_libraries:
                            issues.append(
                                {
                                    "type": "large_library_import",
                                    "file_path": str(file_path),
                                    "line_number": node.lineno,
                                    "severity": "LOW",
                                    "description": f"å¤§å‹åº“ {alias.name} å¯¼å…¥å¯èƒ½å½±å“å¯åŠ¨æ€§èƒ½",
                                    "library": alias.name,
                                }
                            )

        except Exception as e:
            logger.warning(f"æ— æ³•åˆ†æå¯¼å…¥ {file_path}: {e}")

        return issues

    def analyze_file(self, file_path: Path) -> Dict:
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            tree = ast.parse(content)

            file_analysis = {
                "path": str(file_path),
                "functions": [],
                "classes": [],
                "performance_issues": [],
                "import_issues": [],
            }

            # åˆ†æå‡½æ•°å¤æ‚åº¦
            for node in ast.walk(tree):
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    func_analysis = self.analyze_function_complexity(
                        node, str(file_path)
                    )
                    file_analysis["functions"].append(func_analysis)

                elif isinstance(node, ast.ClassDef):
                    class_analysis = {
                        "name": node.name,
                        "line_number": node.lineno,
                        "methods_count": len(
                            [
                                n
                                for n in node.body
                                if isinstance(
                                    n, (ast.FunctionDef, ast.AsyncFunctionDef)
                                )
                            ]
                        ),
                        "lines_count": (node.end_lineno or node.lineno)
                        - node.lineno
                        + 1,
                    }
                    file_analysis["classes"].append(class_analysis)

            # æ£€æµ‹æ€§èƒ½é—®é¢˜æ¨¡å¼
            file_analysis["performance_issues"] = self.detect_performance_patterns(
                file_path
            )

            # åˆ†æå¯¼å…¥é—®é¢˜
            file_analysis["import_issues"] = self.analyze_imports(file_path)

            return file_analysis

        except Exception as e:
            logger.warning(f"æ— æ³•åˆ†ææ–‡ä»¶ {file_path}: {e}")
            return None

    def scan_project(self):
        """æ‰«æé¡¹ç›®ä¸­çš„æ‰€æœ‰Pythonæ–‡ä»¶"""
        logger.info("ğŸ” æ‰«æé¡¹ç›®è¿›è¡Œæ€§èƒ½åˆ†æ...")

        python_files = []
        for root, dirs, files in os.walk(self.project_root):
            # è¿‡æ»¤å¿½ç•¥çš„ç›®å½•
            dirs[:] = [
                d for d in dirs if d not in self.ignore_dirs and not d.startswith(".")
            ]

            for file in files:
                file_path = Path(root) / file
                if not self.should_ignore_path(file_path):
                    python_files.append(file_path)

        logger.info(f"æ‰¾åˆ° {len(python_files)} ä¸ªPythonæ–‡ä»¶")

        for file_path in python_files:
            file_analysis = self.analyze_file(file_path)
            if file_analysis:
                # æ”¶é›†å¤æ‚åº¦é—®é¢˜
                for func in file_analysis["functions"]:
                    if func["cyclomatic_complexity"] > 10:
                        self.complexity_issues.append(
                            {
                                "type": "high_complexity",
                                "severity": (
                                    "HIGH"
                                    if func["cyclomatic_complexity"] > 15
                                    else "MEDIUM"
                                ),
                                "function_name": func["name"],
                                "file_path": func["file_path"],
                                "line_number": func["line_number"],
                                "complexity": func["cyclomatic_complexity"],
                                "description": f"å‡½æ•° {func['name']} åœˆå¤æ‚åº¦è¿‡é«˜ ({func['cyclomatic_complexity']})",
                            }
                        )

                    if func["lines_count"] > 50:
                        self.complexity_issues.append(
                            {
                                "type": "long_function",
                                "severity": "MEDIUM",
                                "function_name": func["name"],
                                "file_path": func["file_path"],
                                "line_number": func["line_number"],
                                "lines_count": func["lines_count"],
                                "description": f"å‡½æ•° {func['name']} è¿‡é•¿ ({func['lines_count']} è¡Œ)",
                            }
                        )

                    if func["parameter_count"] > 7:
                        self.complexity_issues.append(
                            {
                                "type": "too_many_parameters",
                                "severity": "MEDIUM",
                                "function_name": func["name"],
                                "file_path": func["file_path"],
                                "line_number": func["line_number"],
                                "parameter_count": func["parameter_count"],
                                "description": f"å‡½æ•° {func['name']} å‚æ•°è¿‡å¤š ({func['parameter_count']} ä¸ª)",
                            }
                        )

                # æ”¶é›†æ€§èƒ½é—®é¢˜
                self.performance_issues.extend(file_analysis["performance_issues"])
                self.performance_issues.extend(file_analysis["import_issues"])

        logger.info(f"å‘ç° {len(self.complexity_issues)} ä¸ªå¤æ‚åº¦é—®é¢˜")
        logger.info(f"å‘ç° {len(self.performance_issues)} ä¸ªæ€§èƒ½é—®é¢˜")

    def generate_optimization_suggestions(self) -> List[Dict]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        suggestions = []

        # åŸºäºå¤æ‚åº¦é—®é¢˜çš„å»ºè®®
        high_complexity_funcs = [
            issue
            for issue in self.complexity_issues
            if issue["type"] == "high_complexity"
        ]
        if high_complexity_funcs:
            suggestions.append(
                {
                    "category": "ä»£ç å¤æ‚åº¦ä¼˜åŒ–",
                    "priority": "HIGH",
                    "title": "é™ä½å‡½æ•°å¤æ‚åº¦",
                    "description": f"å‘ç° {len(high_complexity_funcs)} ä¸ªé«˜å¤æ‚åº¦å‡½æ•°",
                    "recommendations": [
                        "å°†å¤æ‚å‡½æ•°æ‹†åˆ†ä¸ºå¤šä¸ªå°å‡½æ•°",
                        "ä½¿ç”¨æ—©æœŸè¿”å›å‡å°‘åµŒå¥—",
                        "æå–æ¡ä»¶åˆ¤æ–­åˆ°ç‹¬ç«‹å‡½æ•°",
                        "ä½¿ç”¨ç­–ç•¥æ¨¡å¼æ›¿æ¢å¤æ‚çš„if-else",
                    ],
                }
            )

        # åŸºäºæ€§èƒ½é—®é¢˜çš„å»ºè®®
        nested_loops = [
            issue
            for issue in self.performance_issues
            if issue["type"] == "nested_loops"
        ]
        if nested_loops:
            suggestions.append(
                {
                    "category": "æ€§èƒ½ä¼˜åŒ–",
                    "priority": "HIGH",
                    "title": "ä¼˜åŒ–åµŒå¥—å¾ªç¯",
                    "description": f"å‘ç° {len(nested_loops)} ä¸ªåµŒå¥—å¾ªç¯",
                    "recommendations": [
                        "è€ƒè™‘ä½¿ç”¨å­—å…¸æˆ–é›†åˆä¼˜åŒ–æŸ¥æ‰¾",
                        "ä½¿ç”¨ç”Ÿæˆå™¨å‡å°‘å†…å­˜ä½¿ç”¨",
                        "è€ƒè™‘ä½¿ç”¨numpyç­‰ä¼˜åŒ–åº“",
                        "ç¼“å­˜é‡å¤è®¡ç®—çš„ç»“æœ",
                    ],
                }
            )

        string_concat = [
            issue
            for issue in self.performance_issues
            if issue["type"] == "string_concatenation"
        ]
        if string_concat:
            suggestions.append(
                {
                    "category": "æ€§èƒ½ä¼˜åŒ–",
                    "priority": "MEDIUM",
                    "title": "ä¼˜åŒ–å­—ç¬¦ä¸²æ“ä½œ",
                    "description": f"å‘ç° {len(string_concat)} ä¸ªå­—ç¬¦ä¸²æ‹¼æ¥é—®é¢˜",
                    "recommendations": [
                        "ä½¿ç”¨f-stringæ›¿æ¢å­—ç¬¦ä¸²æ‹¼æ¥",
                        "ä½¿ç”¨join()æ–¹æ³•è¿æ¥å¤šä¸ªå­—ç¬¦ä¸²",
                        "é¿å…åœ¨å¾ªç¯ä¸­è¿›è¡Œå­—ç¬¦ä¸²æ‹¼æ¥",
                        "ä½¿ç”¨StringIOå¤„ç†å¤§é‡å­—ç¬¦ä¸²æ“ä½œ",
                    ],
                }
            )

        return suggestions

    def create_performance_guide(self) -> str:
        """åˆ›å»ºæ€§èƒ½ä¼˜åŒ–æŒ‡å—"""
        guide_content = """# Pythonæ€§èƒ½ä¼˜åŒ–æŒ‡å—

## ä»£ç å¤æ‚åº¦ä¼˜åŒ–

### 1. é™ä½åœˆå¤æ‚åº¦
- **ç›®æ ‡**: ä¿æŒå‡½æ•°åœˆå¤æ‚åº¦ < 10
- **æ–¹æ³•**: 
  - æå–å­å‡½æ•°
  - ä½¿ç”¨æ—©æœŸè¿”å›
  - ç®€åŒ–æ¡ä»¶åˆ¤æ–­
  - ä½¿ç”¨è®¾è®¡æ¨¡å¼

### 2. æ§åˆ¶å‡½æ•°é•¿åº¦
- **ç›®æ ‡**: ä¿æŒå‡½æ•°é•¿åº¦ < 50 è¡Œ
- **æ–¹æ³•**:
  - å•ä¸€èŒè´£åŸåˆ™
  - æå–é‡å¤ä»£ç 
  - åˆ†ç¦»ä¸šåŠ¡é€»è¾‘

### 3. å‡å°‘å‚æ•°æ•°é‡
- **ç›®æ ‡**: å‡½æ•°å‚æ•° < 7 ä¸ª
- **æ–¹æ³•**:
  - ä½¿ç”¨æ•°æ®ç±»æˆ–å­—å…¸
  - å‚æ•°å¯¹è±¡æ¨¡å¼
  - é…ç½®å¯¹è±¡

## æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 1. å¾ªç¯ä¼˜åŒ–
```python
# é¿å…
for i in range(len(items)):
    for j in range(len(other_items)):
        if items[i] == other_items[j]:
            # å¤„ç†

# æ¨è
item_set = set(items)
for item in other_items:
    if item in item_set:
        # å¤„ç†
```

### 2. å­—ç¬¦ä¸²ä¼˜åŒ–
```python
# é¿å…
result = ""
for item in items:
    result += str(item)

# æ¨è
result = "".join(str(item) for item in items)
# æˆ–
result = f"{''.join(str(item) for item in items)}"
```

### 3. æ•°æ®ç»“æ„é€‰æ‹©
- **æŸ¥æ‰¾æ“ä½œ**: ä½¿ç”¨setæˆ–dictè€Œä¸æ˜¯list
- **é¢‘ç¹æ’å…¥/åˆ é™¤**: ä½¿ç”¨dequeè€Œä¸æ˜¯list
- **å¤§é‡æ•°å€¼è®¡ç®—**: ä½¿ç”¨numpyæ•°ç»„

### 4. å†…å­˜ä¼˜åŒ–
```python
# ä½¿ç”¨ç”Ÿæˆå™¨
def process_large_data():
    for item in large_dataset:
        yield process(item)

# ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨
with open('file.txt') as f:
    data = f.read()
```

## å¯¼å…¥ä¼˜åŒ–

### 1. é¿å…é€šé…ç¬¦å¯¼å…¥
```python
# é¿å…
from module import *

# æ¨è
from module import specific_function
```

### 2. å»¶è¿Ÿå¯¼å…¥
```python
def heavy_function():
    import heavy_library  # åªåœ¨éœ€è¦æ—¶å¯¼å…¥
    return heavy_library.process()
```

## æ€§èƒ½æµ‹è¯•

### 1. ä½¿ç”¨cProfile
```python
import cProfile
cProfile.run('your_function()')
```

### 2. ä½¿ç”¨timeit
```python
import timeit
time_taken = timeit.timeit('your_function()', number=1000)
```

### 3. å†…å­˜åˆ†æ
```python
from memory_profiler import profile

@profile
def your_function():
    # å‡½æ•°ä»£ç 
```

## æœ€ä½³å®è·µ

1. **æµ‹é‡ä¼˜å…ˆ**: å…ˆæµ‹é‡å†ä¼˜åŒ–
2. **å…³æ³¨ç“¶é¢ˆ**: ä¼˜åŒ–æœ€è€—æ—¶çš„éƒ¨åˆ†
3. **ä¿æŒç®€å•**: ä¸è¦è¿‡åº¦ä¼˜åŒ–
4. **å®šæœŸæ£€æŸ¥**: ä½¿ç”¨å·¥å…·å®šæœŸæ£€æŸ¥æ€§èƒ½
"""
        return guide_content

    def generate_report(self) -> str:
        """ç”Ÿæˆæ€§èƒ½åˆ†ææŠ¥å‘Š"""
        logger.info("ğŸ“Š ç”Ÿæˆæ€§èƒ½åˆ†ææŠ¥å‘Š...")

        suggestions = self.generate_optimization_suggestions()

        # ç»Ÿè®¡ä¿¡æ¯
        total_issues = len(self.complexity_issues) + len(self.performance_issues)
        high_priority = len(
            [
                issue
                for issue in self.complexity_issues + self.performance_issues
                if issue.get("severity") == "HIGH"
            ]
        )
        medium_priority = len(
            [
                issue
                for issue in self.complexity_issues + self.performance_issues
                if issue.get("severity") == "MEDIUM"
            ]
        )
        low_priority = len(
            [
                issue
                for issue in self.complexity_issues + self.performance_issues
                if issue.get("severity") == "LOW"
            ]
        )

        report = f"""# æ€§èƒ½åˆ†ææŠ¥å‘Š

## ğŸ“Š åˆ†æç»Ÿè®¡

- **æ€»é—®é¢˜æ•°**: {total_issues}
- **é«˜ä¼˜å…ˆçº§**: {high_priority} ä¸ª
- **ä¸­ä¼˜å…ˆçº§**: {medium_priority} ä¸ª
- **ä½ä¼˜å…ˆçº§**: {low_priority} ä¸ª

## ğŸ” å¤æ‚åº¦é—®é¢˜

"""

        if self.complexity_issues:
            # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç»„
            severity_groups = defaultdict(list)
            for issue in self.complexity_issues:
                severity_groups[issue["severity"]].append(issue)

            for severity in ["HIGH", "MEDIUM", "LOW"]:
                if severity in severity_groups:
                    report += f"### {severity} ä¼˜å…ˆçº§\n\n"
                    for issue in severity_groups[severity]:
                        report += f"- **{issue['function_name']}** ({issue['file_path']}:{issue['line_number']})\n"
                        report += f"  - {issue['description']}\n\n"
        else:
            report += "âœ… æœªå‘ç°å¤æ‚åº¦é—®é¢˜\n\n"

        report += """## âš¡ æ€§èƒ½é—®é¢˜

"""

        if self.performance_issues:
            # æŒ‰ç±»å‹åˆ†ç»„
            type_groups = defaultdict(list)
            for issue in self.performance_issues:
                type_groups[issue["type"]].append(issue)

            for issue_type, issues in type_groups.items():
                report += f"### {issue_type.replace('_', ' ').title()}\n\n"
                for issue in issues[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    report += f"- **{issue['file_path']}:{issue['line_number']}**\n"
                    report += f"  - {issue['description']}\n"
                    if "code_snippet" in issue and issue["code_snippet"]:
                        report += f"  - ä»£ç : `{issue['code_snippet']}`\n"
                    report += "\n"

                if len(issues) > 5:
                    report += f"  *... è¿˜æœ‰ {len(issues) - 5} ä¸ªç±»ä¼¼é—®é¢˜*\n\n"
        else:
            report += "âœ… æœªå‘ç°æ€§èƒ½é—®é¢˜\n\n"

        report += """## ğŸš€ ä¼˜åŒ–å»ºè®®

"""

        if suggestions:
            for i, suggestion in enumerate(suggestions, 1):
                report += f"### {i}. {suggestion['title']} ({suggestion['priority']} ä¼˜å…ˆçº§)\n\n"
                report += f"**ç±»åˆ«**: {suggestion['category']}\n\n"
                report += f"**é—®é¢˜**: {suggestion['description']}\n\n"
                report += "**å»ºè®®**:\n"
                for rec in suggestion["recommendations"]:
                    report += f"- {rec}\n"
                report += "\n"
        else:
            report += "âœ… å½“å‰ä»£ç æ€§èƒ½è‰¯å¥½\n"

        report += f"""

## ğŸ“‹ æ€»ç»“

æœ¬æ¬¡åˆ†æå‘ç°äº† {total_issues} ä¸ªæ€§èƒ½ç›¸å…³é—®é¢˜ã€‚

### ä¼˜å…ˆçº§å»ºè®®

1. **ç«‹å³å¤„ç†**: {high_priority} ä¸ªé«˜ä¼˜å…ˆçº§é—®é¢˜
2. **çŸ­æœŸå¤„ç†**: {medium_priority} ä¸ªä¸­ä¼˜å…ˆçº§é—®é¢˜  
3. **é•¿æœŸä¼˜åŒ–**: {low_priority} ä¸ªä½ä¼˜å…ˆçº§é—®é¢˜

### ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. æŸ¥çœ‹æ€§èƒ½ä¼˜åŒ–æŒ‡å—: `docs/development/performance_guide.md`
2. æŒ‰ä¼˜å…ˆçº§ä¿®å¤æ€§èƒ½é—®é¢˜
3. å»ºç«‹æ€§èƒ½ç›‘æ§æœºåˆ¶
4. å®šæœŸè¿è¡Œæ€§èƒ½åˆ†æ

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""

        return report


def main():
    """ä¸»å‡½æ•°"""
    project_root = os.getcwd()
    print("âš¡ ç´¢å…‹ç”Ÿæ´»é¡¹ç›® - æ€§èƒ½åˆ†æå·¥å…·")
    print("=" * 60)

    analyzer = PerformanceAnalyzer(project_root)

    # 1. æ‰«æé¡¹ç›®
    analyzer.scan_project()

    # 2. åˆ›å»ºæ€§èƒ½ä¼˜åŒ–æŒ‡å—
    print("ğŸ“ åˆ›å»ºæ€§èƒ½ä¼˜åŒ–æŒ‡å—...")
    guide_content = analyzer.create_performance_guide()

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    docs_dir = Path(project_root) / "docs" / "development"
    docs_dir.mkdir(parents=True, exist_ok=True)

    guide_file = docs_dir / "performance_guide.md"
    with open(guide_file, "w", encoding="utf-8") as f:
        f.write(guide_content)

    print(f"âœ… å·²åˆ›å»ºæ€§èƒ½ä¼˜åŒ–æŒ‡å—: {guide_file}")

    # 3. ç”ŸæˆæŠ¥å‘Š
    print("ğŸ“Š ç”ŸæˆæŠ¥å‘Š...")
    report = analyzer.generate_report()

    # ä¿å­˜æŠ¥å‘Š
    report_file = Path(project_root) / "performance_analysis_report.md"
    with open(report_file, "w", encoding="utf-8") as f:
        f.write(report)

    print(f"ğŸ“Š æ€§èƒ½åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

    total_issues = len(analyzer.complexity_issues) + len(analyzer.performance_issues)
    if total_issues > 0:
        print(f"\nâš ï¸  å‘ç° {total_issues} ä¸ªæ€§èƒ½é—®é¢˜ï¼Œè¯·æŸ¥çœ‹æŠ¥å‘Šè¯¦æƒ…")
    else:
        print("\nâœ… æœªå‘ç°æ€§èƒ½é—®é¢˜")


if __name__ == "__main__":
    main()
