#!/usr/bin/env python3
"""
uv vs pip æ€§èƒ½å¯¹æ¯”æµ‹è¯•è„šæœ¬
ç”¨äºæµ‹è¯•å’Œæ¯”è¾ƒuvä¸pipåœ¨ç´¢å…‹ç”Ÿæ´»é¡¹ç›®ä¸­çš„æ€§èƒ½è¡¨ç°
"""

import time
import subprocess
import tempfile
import shutil
import os
from pathlib import Path
from typing import Dict, List, Tuple
import json
import statistics

class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.results = {}
        self.test_packages = [
            # åŸºç¡€åŒ…
            "fastapi>=0.104.0",
            "uvicorn[standard]>=0.24.0",
            "pydantic>=2.5.0",
            "redis>=5.0.0",
            "httpx>=0.25.0",
            
            # æ•°æ®ç§‘å­¦åŒ…ï¼ˆè¾ƒå¤§ï¼‰
            "numpy>=1.26.0",
            "pandas>=2.2.0",
            "scikit-learn>=1.4.0",
            
            # AI/MLåŒ…ï¼ˆéå¸¸å¤§ï¼‰
            "torch>=2.1.0",
            "transformers>=4.36.0",
        ]
    
    def run_command_with_timing(self, cmd: List[str], cwd: str = None) -> Tuple[float, bool]:
        """æ‰§è¡Œå‘½ä»¤å¹¶æµ‹é‡æ—¶é—´"""
        start_time = time.time()
        try:
            result = subprocess.run(
                cmd,
                cwd=cwd,
                capture_output=True,
                text=True,
                timeout=1800  # 30åˆ†é’Ÿè¶…æ—¶
            )
            end_time = time.time()
            duration = end_time - start_time
            success = result.returncode == 0
            return duration, success
        except subprocess.TimeoutExpired:
            return float('inf'), False
        except Exception:
            return float('inf'), False
    
    def create_test_environment(self) -> str:
        """åˆ›å»ºæµ‹è¯•ç¯å¢ƒ"""
        temp_dir = tempfile.mkdtemp(prefix="uv_benchmark_")
        
        # åˆ›å»ºrequirements.txt
        requirements_path = Path(temp_dir) / "requirements.txt"
        with open(requirements_path, 'w') as f:
            f.write('\n'.join(self.test_packages))
        
        # åˆ›å»ºpyproject.toml
        pyproject_content = f'''[project]
name = "benchmark-test"
version = "1.0.0"
dependencies = {json.dumps(self.test_packages, indent=4)}
requires-python = ">=3.11"
'''
        pyproject_path = Path(temp_dir) / "pyproject.toml"
        with open(pyproject_path, 'w') as f:
            f.write(pyproject_content)
        
        return temp_dir
    
    def benchmark_pip(self, test_dir: str, iterations: int = 3) -> Dict[str, float]:
        """æµ‹è¯•pipæ€§èƒ½"""
        print("ğŸ å¼€å§‹pipæ€§èƒ½æµ‹è¯•...")
        results = {
            'install_times': [],
            'uninstall_times': [],
            'reinstall_times': []
        }
        
        for i in range(iterations):
            print(f"  ç¬¬ {i+1}/{iterations} æ¬¡æµ‹è¯•...")
            
            # åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
            venv_dir = Path(test_dir) / f"venv_pip_{i}"
            subprocess.run([
                "python", "-m", "venv", str(venv_dir)
            ], capture_output=True)
            
            pip_cmd = str(venv_dir / "bin" / "pip")
            if os.name == 'nt':  # Windows
                pip_cmd = str(venv_dir / "Scripts" / "pip.exe")
            
            # å‡çº§pip
            subprocess.run([pip_cmd, "install", "--upgrade", "pip"], capture_output=True)
            
            # æµ‹è¯•å®‰è£…æ—¶é—´
            install_time, success = self.run_command_with_timing([
                pip_cmd, "install", "-r", "requirements.txt"
            ], cwd=test_dir)
            
            if success:
                results['install_times'].append(install_time)
                print(f"    å®‰è£…æ—¶é—´: {install_time:.2f}ç§’")
                
                # æµ‹è¯•å¸è½½æ—¶é—´
                uninstall_time, _ = self.run_command_with_timing([
                    pip_cmd, "uninstall", "-y", "-r", "requirements.txt"
                ], cwd=test_dir)
                results['uninstall_times'].append(uninstall_time)
                
                # æµ‹è¯•é‡æ–°å®‰è£…æ—¶é—´
                reinstall_time, _ = self.run_command_with_timing([
                    pip_cmd, "install", "-r", "requirements.txt"
                ], cwd=test_dir)
                results['reinstall_times'].append(reinstall_time)
            else:
                print(f"    å®‰è£…å¤±è´¥ï¼Œè·³è¿‡æ­¤æ¬¡æµ‹è¯•")
            
            # æ¸…ç†
            shutil.rmtree(venv_dir, ignore_errors=True)
        
        return results
    
    def benchmark_uv(self, test_dir: str, iterations: int = 3) -> Dict[str, float]:
        """æµ‹è¯•uvæ€§èƒ½"""
        print("âš¡ å¼€å§‹uvæ€§èƒ½æµ‹è¯•...")
        results = {
            'install_times': [],
            'uninstall_times': [],
            'reinstall_times': []
        }
        
        for i in range(iterations):
            print(f"  ç¬¬ {i+1}/{iterations} æ¬¡æµ‹è¯•...")
            
            # åˆ›å»ºuvé¡¹ç›®
            project_dir = Path(test_dir) / f"uv_project_{i}"
            project_dir.mkdir(exist_ok=True)
            
            # å¤åˆ¶é…ç½®æ–‡ä»¶
            shutil.copy2(Path(test_dir) / "pyproject.toml", project_dir)
            
            # æµ‹è¯•å®‰è£…æ—¶é—´
            install_time, success = self.run_command_with_timing([
                "uv", "sync"
            ], cwd=str(project_dir))
            
            if success:
                results['install_times'].append(install_time)
                print(f"    å®‰è£…æ—¶é—´: {install_time:.2f}ç§’")
                
                # æµ‹è¯•æ¸…ç†æ—¶é—´ï¼ˆç›¸å½“äºå¸è½½ï¼‰
                uninstall_time, _ = self.run_command_with_timing([
                    "rm", "-rf", ".venv"
                ], cwd=str(project_dir))
                results['uninstall_times'].append(uninstall_time)
                
                # æµ‹è¯•é‡æ–°å®‰è£…æ—¶é—´
                reinstall_time, _ = self.run_command_with_timing([
                    "uv", "sync"
                ], cwd=str(project_dir))
                results['reinstall_times'].append(reinstall_time)
            else:
                print(f"    å®‰è£…å¤±è´¥ï¼Œè·³è¿‡æ­¤æ¬¡æµ‹è¯•")
            
            # æ¸…ç†
            shutil.rmtree(project_dir, ignore_errors=True)
        
        return results
    
    def calculate_stats(self, times: List[float]) -> Dict[str, float]:
        """è®¡ç®—ç»Ÿè®¡æ•°æ®"""
        if not times:
            return {'mean': 0, 'median': 0, 'min': 0, 'max': 0, 'std': 0}
        
        return {
            'mean': statistics.mean(times),
            'median': statistics.median(times),
            'min': min(times),
            'max': max(times),
            'std': statistics.stdev(times) if len(times) > 1 else 0
        }
    
    def generate_report(self, pip_results: Dict, uv_results: Dict) -> str:
        """ç”Ÿæˆæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š"""
        pip_install_stats = self.calculate_stats(pip_results['install_times'])
        uv_install_stats = self.calculate_stats(uv_results['install_times'])
        
        pip_reinstall_stats = self.calculate_stats(pip_results['reinstall_times'])
        uv_reinstall_stats = self.calculate_stats(uv_results['reinstall_times'])
        
        # è®¡ç®—æ€§èƒ½æå‡
        if pip_install_stats['mean'] > 0:
            install_speedup = pip_install_stats['mean'] / uv_install_stats['mean']
        else:
            install_speedup = 0
        
        if pip_reinstall_stats['mean'] > 0:
            reinstall_speedup = pip_reinstall_stats['mean'] / uv_reinstall_stats['mean']
        else:
            reinstall_speedup = 0
        
        report = f"""
# ç´¢å…‹ç”Ÿæ´»é¡¹ç›® - uv vs pip æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š

## æµ‹è¯•ç¯å¢ƒ
- æµ‹è¯•æ—¶é—´: {time.strftime("%Y-%m-%d %H:%M:%S")}
- æµ‹è¯•åŒ…æ•°é‡: {len(self.test_packages)}
- åŒ…å«å¤§å‹MLåŒ…: torch, transformers, scikit-learn

## æ€§èƒ½å¯¹æ¯”ç»“æœ

### é¦–æ¬¡å®‰è£…æ€§èƒ½
| å·¥å…· | å¹³å‡æ—¶é—´ | ä¸­ä½æ•° | æœ€å¿« | æœ€æ…¢ | æ ‡å‡†å·® |
|------|----------|--------|------|------|--------|
| pip  | {pip_install_stats['mean']:.2f}s | {pip_install_stats['median']:.2f}s | {pip_install_stats['min']:.2f}s | {pip_install_stats['max']:.2f}s | {pip_install_stats['std']:.2f}s |
| uv   | {uv_install_stats['mean']:.2f}s | {uv_install_stats['median']:.2f}s | {uv_install_stats['min']:.2f}s | {uv_install_stats['max']:.2f}s | {uv_install_stats['std']:.2f}s |

**uvæ¯”pipå¿« {install_speedup:.1f}x** ğŸš€

### é‡æ–°å®‰è£…æ€§èƒ½ï¼ˆç¼“å­˜åœºæ™¯ï¼‰
| å·¥å…· | å¹³å‡æ—¶é—´ | ä¸­ä½æ•° | æœ€å¿« | æœ€æ…¢ | æ ‡å‡†å·® |
|------|----------|--------|------|------|--------|
| pip  | {pip_reinstall_stats['mean']:.2f}s | {pip_reinstall_stats['median']:.2f}s | {pip_reinstall_stats['min']:.2f}s | {pip_reinstall_stats['max']:.2f}s | {pip_reinstall_stats['std']:.2f}s |
| uv   | {uv_reinstall_stats['mean']:.2f}s | {uv_reinstall_stats['median']:.2f}s | {uv_reinstall_stats['min']:.2f}s | {uv_reinstall_stats['max']:.2f}s | {uv_reinstall_stats['std']:.2f}s |

**uvæ¯”pipå¿« {reinstall_speedup:.1f}x** âš¡

## è¯¦ç»†æµ‹è¯•æ•°æ®

### pipå®‰è£…æ—¶é—´ (ç§’)
{pip_results['install_times']}

### uvå®‰è£…æ—¶é—´ (ç§’)
{uv_results['install_times']}

## ç»“è®º

1. **é¦–æ¬¡å®‰è£…**: uvæ¯”pipå¿« {install_speedup:.1f} å€
2. **é‡æ–°å®‰è£…**: uvæ¯”pipå¿« {reinstall_speedup:.1f} å€
3. **ç¨³å®šæ€§**: uvçš„æ€§èƒ½æ›´ç¨³å®šï¼Œæ ‡å‡†å·®æ›´å°
4. **æ¨è**: å¼ºçƒˆå»ºè®®ç´¢å…‹ç”Ÿæ´»é¡¹ç›®è¿ç§»åˆ°uv

## å¯¹ç´¢å…‹ç”Ÿæ´»é¡¹ç›®çš„å½±å“

- **å¼€å‘æ•ˆç‡**: æ¯æ¬¡ç¯å¢ƒæ­å»ºèŠ‚çœ {pip_install_stats['mean'] - uv_install_stats['mean']:.0f} ç§’
- **CI/CD**: æ„å»ºæ—¶é—´æ˜¾è‘—å‡å°‘ï¼Œç‰¹åˆ«æ˜¯åŒ…å«MLä¾èµ–çš„æœåŠ¡
- **å¼€å‘ä½“éªŒ**: æ›´å¿«çš„ä¾èµ–å®‰è£…å’Œç¯å¢ƒåˆ‡æ¢
"""
        
        return report
    
    def run_benchmark(self, iterations: int = 3) -> str:
        """è¿è¡Œå®Œæ•´çš„æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹uv vs pipæ€§èƒ½å¯¹æ¯”æµ‹è¯•")
        print(f"æµ‹è¯•åŒ…æ•°é‡: {len(self.test_packages)}")
        print(f"æ¯ä¸ªå·¥å…·æµ‹è¯• {iterations} æ¬¡")
        
        # åˆ›å»ºæµ‹è¯•ç¯å¢ƒ
        test_dir = self.create_test_environment()
        print(f"æµ‹è¯•ç›®å½•: {test_dir}")
        
        try:
            # æµ‹è¯•pip
            pip_results = self.benchmark_pip(test_dir, iterations)
            
            # æµ‹è¯•uv
            uv_results = self.benchmark_uv(test_dir, iterations)
            
            # ç”ŸæˆæŠ¥å‘Š
            report = self.generate_report(pip_results, uv_results)
            
            return report
            
        finally:
            # æ¸…ç†æµ‹è¯•ç¯å¢ƒ
            shutil.rmtree(test_dir, ignore_errors=True)


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="uv vs pip æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    parser.add_argument("--iterations", type=int, default=3, help="æµ‹è¯•è¿­ä»£æ¬¡æ•°")
    parser.add_argument("--output", help="è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    benchmark = PerformanceBenchmark()
    report = benchmark.run_benchmark(args.iterations)
    
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output}")
    else:
        print(report)


if __name__ == "__main__":
    main() 