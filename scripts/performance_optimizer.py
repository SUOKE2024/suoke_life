"""
performance_optimizer - ç´¢å…‹ç”Ÿæ´»é¡¹ç›®æ¨¡å—
"""

from pathlib import Path
from typing import Dict, List, Any, Optional
import asyncio
import json
import logging
import os
import re
import subprocess
import time

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç´¢å…‹ç”Ÿæ´» - æ€§èƒ½ä¼˜åŒ–å™¨
è‡ªåŠ¨ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢ã€APIå“åº”æ—¶é—´å’Œå‰ç«¯æ€§èƒ½
"""


logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–å™¨"""
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.optimization_report = {
            "database": {"optimized": 0, "total": 0, "improvements": []},
            "api": {"optimized": 0, "total": 0, "improvements": []},
            "frontend": {"optimized": 0, "total": 0, "improvements": []},
            "overall_score": 0
        }
        
    def optimize_all(self) -> bool:
        """æ‰§è¡Œå…¨é¢æ€§èƒ½ä¼˜åŒ–"""
        logger.info("ğŸš€ å¼€å§‹æ€§èƒ½ä¼˜åŒ–...")
        
        try:
            # æ•°æ®åº“ä¼˜åŒ–
            self.optimize_database()
            
            # APIä¼˜åŒ–
            self.optimize_api()
            
            # å‰ç«¯ä¼˜åŒ–
            self.optimize_frontend()
            
            # ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
            self.generate_report()
            
            logger.info("ğŸ‰ æ€§èƒ½ä¼˜åŒ–å®Œæˆï¼")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ€§èƒ½ä¼˜åŒ–å¤±è´¥: {e}")
            return False
    
    def optimize_database(self):
        """æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–"""
        logger.info("ğŸ“Š å¼€å§‹æ•°æ®åº“ä¼˜åŒ–...")
        
        # æŸ¥æ‰¾æ•°æ®åº“ç›¸å…³æ–‡ä»¶
        db_files = list(self.project_root.glob("**/*.py"))
        db_files = [f for f in db_files if self.is_database_file(f)]
        
        for file_path in db_files:
            try:
                optimized = self.optimize_database_file(file_path)
                if optimized:
                    self.optimization_report["database"]["optimized"] += 1
                self.optimization_report["database"]["total"] += 1
                
            except Exception as e:
                logger.warning(f"æ•°æ®åº“æ–‡ä»¶ä¼˜åŒ–å¤±è´¥ {file_path}: {e}")
        
        logger.info(f"âœ… æ•°æ®åº“ä¼˜åŒ–å®Œæˆ: {self.optimization_report['database']['optimized']}/{self.optimization_report['database']['total']}")
    
    def is_database_file(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ•°æ®åº“ç›¸å…³æ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            db_indicators = [
                'from sqlalchemy import',
                'Session(',
                'query(',
                'filter(',
                'join(',
                'select(',
                'from django.db import',
                'models.Model'
            ]
            
            return any(indicator in content for indicator in db_indicators)
            
        except Exception:
            return False
    
    def optimize_database_file(self, file_path: Path) -> bool:
        """ä¼˜åŒ–å•ä¸ªæ•°æ®åº“æ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            original_content = content
            optimizations = []
            
            # ä¼˜åŒ–1: æ·»åŠ æ•°æ®åº“ç´¢å¼•å»ºè®®
            if 'class ' in content and 'Model' in content:
                content = self.add_index_suggestions(content)
                optimizations.append("æ·»åŠ æ•°æ®åº“ç´¢å¼•å»ºè®®")
            
            # ä¼˜åŒ–2: ä¼˜åŒ–æŸ¥è¯¢è¯­å¥
            content = self.optimize_queries(content)
            if content != original_content:
                optimizations.append("ä¼˜åŒ–æŸ¥è¯¢è¯­å¥")
            
            # ä¼˜åŒ–3: æ·»åŠ æŸ¥è¯¢ç¼“å­˜
            content = self.add_query_cache(content)
            if content != original_content:
                optimizations.append("æ·»åŠ æŸ¥è¯¢ç¼“å­˜")
            
            # ä¼˜åŒ–4: æ‰¹é‡æ“ä½œä¼˜åŒ–
            content = self.optimize_batch_operations(content)
            if content != original_content:
                optimizations.append("æ‰¹é‡æ“ä½œä¼˜åŒ–")
            
            if content != original_content:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                self.optimization_report["database"]["improvements"].extend(optimizations)
                logger.info(f"âœ… ä¼˜åŒ–æ•°æ®åº“æ–‡ä»¶: {file_path.name}")
                return True
            
            return False
            
        except Exception as e:
            logger.warning(f"ä¼˜åŒ–æ•°æ®åº“æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return False
    
    def add_index_suggestions(self, content: str) -> str:
        """æ·»åŠ æ•°æ®åº“ç´¢å¼•å»ºè®®"""
        # æŸ¥æ‰¾æ¨¡å‹ç±»
        model_pattern = r'class\s+(\w+)\s*\([^)]*Model[^)]*\):'
        matches = re.finditer(model_pattern, content)
        
        for match in matches:
            class_name = match.group(1)
            class_start = match.start()
            
            # æŸ¥æ‰¾ç±»çš„ç»“æŸä½ç½®
            class_end = self.find_class_end(content, class_start)
            class_content = content[class_start:class_end]
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰Metaç±»
            if 'class Meta:' not in class_content:
                # æ·»åŠ Metaç±»å’Œç´¢å¼•å»ºè®®
                meta_class = f"""
    class Meta:
        # æ€§èƒ½ä¼˜åŒ–: æ·»åŠ å¸¸ç”¨æŸ¥è¯¢å­—æ®µçš„ç´¢å¼•
        indexes = [
            # æ ¹æ®å®é™…æŸ¥è¯¢éœ€æ±‚æ·»åŠ ç´¢å¼•
            # models.Index(fields=['created_at']),
            # models.Index(fields=['user_id']),
            # models.Index(fields=['status']),
        ]
        # æ•°æ®åº“è¡¨é€‰é¡¹
        db_table = '{class_name.lower()}'
        ordering = ['-created_at']
"""
                
                # åœ¨ç±»çš„æœ€åæ·»åŠ Metaç±»
                insert_pos = class_end - 1
                content = content[:insert_pos] + meta_class + content[insert_pos:]
        
        return content
    
    def find_class_end(self, content: str, class_start: int) -> int:
        """æŸ¥æ‰¾ç±»çš„ç»“æŸä½ç½®"""
        lines = content[class_start:].split('\n')
        indent_level = 0
        
        for i, line in enumerate(lines[1:], 1):
            if line.strip():
                current_indent = len(line) - len(line.lstrip())
                if current_indent <= indent_level and not line.startswith(' '):
                    return class_start + len('\n'.join(lines[:i]))
        
        return len(content)
    
    def optimize_queries(self, content: str) -> str:
        """ä¼˜åŒ–æŸ¥è¯¢è¯­å¥"""
        optimizations = [
            # ä¼˜åŒ–N+1æŸ¥è¯¢é—®é¢˜
            (r'\.filter\(([^)]+)\)\.all\(\)', r'.filter(\1).prefetch_related().all()[:1000]  # é™åˆ¶æŸ¥è¯¢ç»“æœæ•°é‡'),
            
            # æ·»åŠ æŸ¥è¯¢é™åˆ¶
            (r'\.all\(\)(?!\s*\[:)', r'.all()[:1000]  # é™åˆ¶æŸ¥è¯¢ç»“æœæ•°é‡'),
            
            # ä¼˜åŒ–existsæŸ¥è¯¢
            (r'\.filter\(([^)]+)\)\.count\(\)\s*>\s*0', r'.filter(\1).exists()'),
            
            # ä½¿ç”¨select_relatedä¼˜åŒ–å¤–é”®æŸ¥è¯¢
            (r'\.filter\(([^)]+)\)(?=.*__)', r'.select_related().filter(\1)'),
        ]
        
        for pattern, replacement in optimizations:
            content = re.sub(pattern, replacement, content)
        
        return content
    
    def add_query_cache(self, content: str) -> str:
        """æ·»åŠ æŸ¥è¯¢ç¼“å­˜"""
        # æŸ¥æ‰¾æŸ¥è¯¢å‡½æ•°
        query_pattern = r'def\s+(\w*(?:get|find|search|query)\w*)\s*\([^)]*\):'
        
        matches = re.finditer(query_pattern, content)
        
        for match in matches:
            func_name = match.group(1)
            func_start = match.start()
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¼“å­˜è£…é¥°å™¨
            before_func = content[:func_start]
            if '@cache' not in before_func[-200:]:
                # æ·»åŠ ç¼“å­˜è£…é¥°å™¨
                cache_decorator = f"""    @cache(timeout=300)  # 5åˆ†é’Ÿç¼“å­˜
"""
                content = content[:func_start] + cache_decorator + content[func_start:]
        
        return content
    
    def optimize_batch_operations(self, content: str) -> str:
        """ä¼˜åŒ–æ‰¹é‡æ“ä½œ"""
        # ä¼˜åŒ–æ‰¹é‡æ’å…¥
        content = re.sub(
            r'for\s+\w+\s+in\s+\w+:\s*\n\s*\w+\.objects\.create\(',
            'bulk_data = []\nfor item in items:\n    bulk_data.append(Model(',
            content
        )
        
        # æ·»åŠ æ‰¹é‡åˆ›å»ºå»ºè®®
        if 'objects.create(' in content and 'bulk_create' not in content:
            content += """
# æ€§èƒ½ä¼˜åŒ–å»ºè®®: ä½¿ç”¨bulk_createè¿›è¡Œæ‰¹é‡æ’å…¥
# Model.objects.bulk_create(bulk_data, batch_size=1000)
"""
        
        return content
    
    def optimize_api(self):
        """APIæ€§èƒ½ä¼˜åŒ–"""
        logger.info("ğŸ”§ å¼€å§‹APIä¼˜åŒ–...")
        
        # æŸ¥æ‰¾APIç›¸å…³æ–‡ä»¶
        api_files = []
        for pattern in ["**/main.py", "**/*api*.py", "**/*router*.py"]:
            api_files.extend(list(self.project_root.glob(pattern)))
        
        api_files = [f for f in api_files if self.is_api_file(f)]
        
        for file_path in api_files:
            try:
                optimized = self.optimize_api_file(file_path)
                if optimized:
                    self.optimization_report["api"]["optimized"] += 1
                self.optimization_report["api"]["total"] += 1
                
            except Exception as e:
                logger.warning(f"APIæ–‡ä»¶ä¼˜åŒ–å¤±è´¥ {file_path}: {e}")
        
        logger.info(f"âœ… APIä¼˜åŒ–å®Œæˆ: {self.optimization_report['api']['optimized']}/{self.optimization_report['api']['total']}")
    
    def is_api_file(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºAPIæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            api_indicators = [
                'from fastapi import',
                '@app.',
                'FastAPI(',
                'APIRouter(',
                '@router.',
                'app = FastAPI'
            ]
            
            return any(indicator in content for indicator in api_indicators)
            
        except Exception:
            return False
    
    def optimize_api_file(self, file_path: Path) -> bool:
        """ä¼˜åŒ–å•ä¸ªAPIæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            original_content = content
            optimizations = []
            
            # ä¼˜åŒ–1: æ·»åŠ å“åº”ç¼“å­˜
            content = self.add_response_cache(content)
            if content != original_content:
                optimizations.append("æ·»åŠ å“åº”ç¼“å­˜")
            
            # ä¼˜åŒ–2: æ·»åŠ è¯·æ±‚é™æµ
            content = self.add_rate_limiting(content)
            if content != original_content:
                optimizations.append("æ·»åŠ è¯·æ±‚é™æµ")
            
            # ä¼˜åŒ–3: ä¼˜åŒ–å¼‚æ­¥å¤„ç†
            content = self.optimize_async_handling(content)
            if content != original_content:
                optimizations.append("ä¼˜åŒ–å¼‚æ­¥å¤„ç†")
            
            # ä¼˜åŒ–4: æ·»åŠ å“åº”å‹ç¼©
            content = self.add_response_compression(content)
            if content != original_content:
                optimizations.append("æ·»åŠ å“åº”å‹ç¼©")
            
            if content != original_content:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                self.optimization_report["api"]["improvements"].extend(optimizations)
                logger.info(f"âœ… ä¼˜åŒ–APIæ–‡ä»¶: {file_path.name}")
                return True
            
            return False
            
        except Exception as e:
            logger.warning(f"ä¼˜åŒ–APIæ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return False
    
    def add_response_cache(self, content: str) -> str:
        """æ·»åŠ å“åº”ç¼“å­˜"""
        # æŸ¥æ‰¾GETè·¯ç”±
        get_pattern = r'@(?:app|router)\.get\s*\([^)]*\)'
        
        matches = re.finditer(get_pattern, content)
        
        for match in matches:
            route_start = match.start()
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¼“å­˜è£…é¥°å™¨
            before_route = content[:route_start]
            if '@cache' not in before_route[-200:]:
                # æ·»åŠ ç¼“å­˜è£…é¥°å™¨
                cache_decorator = '@cache(expire=300)  # 5åˆ†é’Ÿç¼“å­˜\n'
                content = content[:route_start] + cache_decorator + content[route_start:]
        
        return content
    
    def add_rate_limiting(self, content: str) -> str:
        """æ·»åŠ è¯·æ±‚é™æµ"""
        # æŸ¥æ‰¾è·¯ç”±å®šä¹‰
        route_pattern = r'@(?:app|router)\.(get|post|put|delete)\s*\([^)]*\)'
        
        matches = re.finditer(route_pattern, content)
        
        for match in matches:
            route_start = match.start()
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰é™æµè£…é¥°å™¨
            before_route = content[:route_start]
            if '@limiter.limit' not in before_route[-200:]:
                # æ·»åŠ é™æµè£…é¥°å™¨
                limit_decorator = '@limiter.limit("100/minute")  # æ¯åˆ†é’Ÿ100æ¬¡è¯·æ±‚\n'
                content = content[:route_start] + limit_decorator + content[route_start:]
        
        return content
    
    def optimize_async_handling(self, content: str) -> str:
        """ä¼˜åŒ–å¼‚æ­¥å¤„ç†"""
        # å°†åŒæ­¥å‡½æ•°è½¬æ¢ä¸ºå¼‚æ­¥å‡½æ•°
        sync_pattern = r'def\s+(\w+)\s*\([^)]*\):'
        
        def replace_with_async(match):
            func_name = match.group(1)
            # åªè½¬æ¢APIè·¯ç”±å‡½æ•°
            if any(keyword in func_name.lower() for keyword in ['get', 'post', 'put', 'delete', 'api']):
                return f'async def {func_name}('
            return match.group(0)
        
        content = re.sub(sync_pattern, replace_with_async, content)
        
        return content
    
    def add_response_compression(self, content: str) -> str:
        """æ·»åŠ å“åº”å‹ç¼©"""
        if 'from fastapi import FastAPI' in content and 'GZipMiddleware' not in content:
            # æ·»åŠ å‹ç¼©ä¸­é—´ä»¶å¯¼å…¥
            import_line = 'from fastapi.middleware.gzip import GZipMiddleware\n'
            content = import_line + content
            
            # æ·»åŠ ä¸­é—´ä»¶é…ç½®
            if 'app = FastAPI(' in content:
                middleware_config = """
# æ€§èƒ½ä¼˜åŒ–: æ·»åŠ å“åº”å‹ç¼©
app.add_middleware(GZipMiddleware, minimum_size=1000)
"""
                app_line = content.find('app = FastAPI(')
                app_end = content.find('\n', app_line) + 1
                content = content[:app_end] + middleware_config + content[app_end:]
        
        return content
    
    def optimize_frontend(self):
        """å‰ç«¯æ€§èƒ½ä¼˜åŒ–"""
        logger.info("ğŸ¨ å¼€å§‹å‰ç«¯ä¼˜åŒ–...")
        
        # æŸ¥æ‰¾å‰ç«¯æ–‡ä»¶
        frontend_files = []
        for pattern in ["src/**/*.tsx", "src/**/*.ts", "src/**/*.js", "src/**/*.jsx"]:
            frontend_files.extend(list(self.project_root.glob(pattern)))
        
        for file_path in frontend_files:
            try:
                optimized = self.optimize_frontend_file(file_path)
                if optimized:
                    self.optimization_report["frontend"]["optimized"] += 1
                self.optimization_report["frontend"]["total"] += 1
                
            except Exception as e:
                logger.warning(f"å‰ç«¯æ–‡ä»¶ä¼˜åŒ–å¤±è´¥ {file_path}: {e}")
        
        logger.info(f"âœ… å‰ç«¯ä¼˜åŒ–å®Œæˆ: {self.optimization_report['frontend']['optimized']}/{self.optimization_report['frontend']['total']}")
    
    def optimize_frontend_file(self, file_path: Path) -> bool:
        """ä¼˜åŒ–å•ä¸ªå‰ç«¯æ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            original_content = content
            optimizations = []
            
            # ä¼˜åŒ–1: æ·»åŠ React.memo
            content = self.add_react_memo(content)
            if content != original_content:
                optimizations.append("æ·»åŠ React.memo")
            
            # ä¼˜åŒ–2: ä¼˜åŒ–useEffectä¾èµ–
            content = self.optimize_useeffect(content)
            if content != original_content:
                optimizations.append("ä¼˜åŒ–useEffectä¾èµ–")
            
            # ä¼˜åŒ–3: æ·»åŠ æ‡’åŠ è½½
            content = self.add_lazy_loading(content)
            if content != original_content:
                optimizations.append("æ·»åŠ æ‡’åŠ è½½")
            
            # ä¼˜åŒ–4: ä¼˜åŒ–å›¾ç‰‡åŠ è½½
            content = self.optimize_images(content)
            if content != original_content:
                optimizations.append("ä¼˜åŒ–å›¾ç‰‡åŠ è½½")
            
            if content != original_content:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                self.optimization_report["frontend"]["improvements"].extend(optimizations)
                logger.info(f"âœ… ä¼˜åŒ–å‰ç«¯æ–‡ä»¶: {file_path.name}")
                return True
            
            return False
            
        except Exception as e:
            logger.warning(f"ä¼˜åŒ–å‰ç«¯æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return False
    
    def add_react_memo(self, content: str) -> str:
        """æ·»åŠ React.memoä¼˜åŒ–"""
        # æŸ¥æ‰¾å‡½æ•°ç»„ä»¶
        component_pattern = r'const\s+(\w+)\s*=\s*\([^)]*\)\s*=>\s*{'
        
        matches = re.finditer(component_pattern, content)
        
        for match in matches:
            component_name = match.group(1)
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»ä½¿ç”¨memo
            if f'React.memo({component_name})' not in content:
                # åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ memoå¯¼å‡º
                export_pattern = f'export default {component_name}'
                if export_pattern in content:
                    content = content.replace(
                        export_pattern,
                        f'export default React.memo({component_name})'
                    )
        
        return content
    
    def optimize_useeffect(self, content: str) -> str:
        """ä¼˜åŒ–useEffectä¾èµ–"""
        # æŸ¥æ‰¾useEffect
        useeffect_pattern = r'useEffect\s*\(\s*\(\s*\)\s*=>\s*{[^}]*},\s*\[\s*\]\s*\)'
        
        # æ·»åŠ ä¾èµ–é¡¹å»ºè®®æ³¨é‡Š
        content = re.sub(
            useeffect_pattern,
            lambda m: m.group(0) + '  // æ£€æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ ä¾èµ–é¡¹',
            content
        )
        
        return content
    
    def add_lazy_loading(self, content: str) -> str:
        """æ·»åŠ æ‡’åŠ è½½"""
        # æŸ¥æ‰¾ç»„ä»¶å¯¼å…¥
        import_pattern = r"import\s+(\w+)\s+from\s+['\"]([^'\"]+)['\"]"
        
        matches = re.finditer(import_pattern, content)
        
        for match in matches:
            component_name = match.group(1)
            import_path = match.group(2)
            
            # å¦‚æœæ˜¯ç»„ä»¶å¯¼å…¥ä¸”è·¯å¾„åŒ…å«screensæˆ–components
            if any(keyword in import_path for keyword in ['screens', 'components']) and component_name[0].isupper():
                # æ›¿æ¢ä¸ºæ‡’åŠ è½½
                lazy_import = f"const {component_name} = React.lazy(() => import('{import_path}'))"
                content = content.replace(match.group(0), lazy_import)
        
        return content
    
    def optimize_images(self, content: str) -> str:
        """ä¼˜åŒ–å›¾ç‰‡åŠ è½½"""
        # ä¼˜åŒ–Imageç»„ä»¶
        image_pattern = r'<Image\s+([^>]*)\s*/?>'
        
        def optimize_image_props(match):
            props = match.group(1)
            
            # æ·»åŠ æ€§èƒ½ä¼˜åŒ–å±æ€§
            if 'loading=' not in props:
                props += ' loading="lazy"'
            if 'decoding=' not in props:
                props += ' decoding="async"'
            
            return f'<Image {props} />'
        
        content = re.sub(image_pattern, optimize_image_props, content)
        
        return content
    
    def generate_report(self):
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        # è®¡ç®—æ€»ä½“è¯„åˆ†
        total_optimized = (
            self.optimization_report["database"]["optimized"] +
            self.optimization_report["api"]["optimized"] +
            self.optimization_report["frontend"]["optimized"]
        )
        
        total_files = (
            self.optimization_report["database"]["total"] +
            self.optimization_report["api"]["total"] +
            self.optimization_report["frontend"]["total"]
        )
        
        if total_files > 0:
            self.optimization_report["overall_score"] = round((total_optimized / total_files) * 100, 2)
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.project_root / "PERFORMANCE_OPTIMIZATION_REPORT.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.optimization_report, f, indent=2, ensure_ascii=False)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self.generate_markdown_report()
        
        logger.info(f"ğŸ“Š æ€§èƒ½ä¼˜åŒ–æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        logger.info(f"ğŸ¯ æ€»ä½“ä¼˜åŒ–ç‡: {self.optimization_report['overall_score']}%")
    
    def generate_markdown_report(self):
        """ç”ŸæˆMarkdownæ ¼å¼çš„ä¼˜åŒ–æŠ¥å‘Š"""
        report = self.optimization_report
        
        content = f"""# æ€§èƒ½ä¼˜åŒ–æŠ¥å‘Š

## ä¼˜åŒ–æ¦‚è§ˆ

- **æ€»ä½“ä¼˜åŒ–ç‡**: {report['overall_score']}%
- **ä¼˜åŒ–æ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}

## è¯¦ç»†ç»“æœ

### ğŸ“Š æ•°æ®åº“ä¼˜åŒ–
- **ä¼˜åŒ–æ–‡ä»¶æ•°**: {report['database']['optimized']}/{report['database']['total']}
- **ä¼˜åŒ–ç‡**: {round(report['database']['optimized']/max(report['database']['total'], 1)*100, 2)}%
- **ä¼˜åŒ–é¡¹ç›®**:
"""
        
        for improvement in report['database']['improvements']:
            content += f"  - {improvement}\n"
        
        content += f"""
### ğŸ”§ APIä¼˜åŒ–
- **ä¼˜åŒ–æ–‡ä»¶æ•°**: {report['api']['optimized']}/{report['api']['total']}
- **ä¼˜åŒ–ç‡**: {round(report['api']['optimized']/max(report['api']['total'], 1)*100, 2)}%
- **ä¼˜åŒ–é¡¹ç›®**:
"""
        
        for improvement in report['api']['improvements']:
            content += f"  - {improvement}\n"
        
        content += f"""
### ğŸ¨ å‰ç«¯ä¼˜åŒ–
- **ä¼˜åŒ–æ–‡ä»¶æ•°**: {report['frontend']['optimized']}/{report['frontend']['total']}
- **ä¼˜åŒ–ç‡**: {round(report['frontend']['optimized']/max(report['frontend']['total'], 1)*100, 2)}%
- **ä¼˜åŒ–é¡¹ç›®**:
"""
        
        for improvement in report['frontend']['improvements']:
            content += f"  - {improvement}\n"
        
        content += """
## æ€§èƒ½æå‡å»ºè®®

### æ•°æ®åº“å±‚é¢
1. å®šæœŸåˆ†ææ…¢æŸ¥è¯¢æ—¥å¿—
2. ç›‘æ§æ•°æ®åº“è¿æ¥æ± ä½¿ç”¨æƒ…å†µ
3. è€ƒè™‘è¯»å†™åˆ†ç¦»å’Œåˆ†åº“åˆ†è¡¨
4. å®šæœŸæ›´æ–°ç»Ÿè®¡ä¿¡æ¯

### APIå±‚é¢
1. å®æ–½APIç‰ˆæœ¬ç®¡ç†
2. æ·»åŠ è¯·æ±‚/å“åº”æ—¥å¿—
3. ç›‘æ§APIå“åº”æ—¶é—´
4. è€ƒè™‘ä½¿ç”¨CDNåŠ é€Ÿ

### å‰ç«¯å±‚é¢
1. å®æ–½ä»£ç åˆ†å‰²
2. ä¼˜åŒ–æ‰“åŒ…é…ç½®
3. ä½¿ç”¨Service Workerç¼“å­˜
4. ç›‘æ§Core Web Vitals

## ç›‘æ§å»ºè®®

å»ºè®®è®¾ç½®ä»¥ä¸‹æ€§èƒ½ç›‘æ§æŒ‡æ ‡ï¼š
- æ•°æ®åº“æŸ¥è¯¢æ—¶é—´ < 100ms
- APIå“åº”æ—¶é—´ < 500ms
- é¡µé¢åŠ è½½æ—¶é—´ < 3s
- é¦–å±æ¸²æŸ“æ—¶é—´ < 1.5s

---

*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        report_file = self.project_root / "PERFORMANCE_OPTIMIZATION_REPORT.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(content)

def main():
    """ä¸»å‡½æ•°"""
    project_root = os.getcwd()
    
    logger.info("ğŸš€ å¯åŠ¨æ€§èƒ½ä¼˜åŒ–å™¨")
    
    optimizer = PerformanceOptimizer(project_root)
    
    try:
        success = optimizer.optimize_all()
        
        if success:
            logger.info("ğŸ‰ æ€§èƒ½ä¼˜åŒ–å®Œæˆï¼")
            return 0
        else:
            logger.warning("âš ï¸ æ€§èƒ½ä¼˜åŒ–å¤±è´¥")
            return 1
            
    except Exception as e:
        logger.error(f"âŒ æ€§èƒ½ä¼˜åŒ–å¤±è´¥: {e}")
        return 1

if __name__ == "__main__":
    exit(main()) 