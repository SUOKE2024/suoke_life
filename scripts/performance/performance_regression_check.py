#!/usr/bin/env python3
"""
æ€§èƒ½å›å½’æ£€æµ‹è„šæœ¬
ç”¨äºCI/CDä¸­è‡ªåŠ¨æ£€æµ‹æ€§èƒ½å›å½’é—®é¢˜
"""

import json
import os
import sys
import subprocess
from pathlib import Path
from typing import Dict, List, Optional
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime


class PerformanceRegressionChecker:
    """æ€§èƒ½å›å½’æ£€æµ‹å™¨"""
    
    def __init__(self):
        self.baseline_dir = Path("performance_baselines")
        self.current_results_dir = Path("current_performance")
        self.report_file = Path("performance_report.md")
        self.threshold_degradation = 0.20  # 20%æ€§èƒ½ä¸‹é™é˜ˆå€¼
        
    def load_baseline_data(self, service: str) -> Optional[Dict]:
        """åŠ è½½åŸºçº¿æ€§èƒ½æ•°æ®"""
        baseline_file = self.baseline_dir / f"{service}_baseline.json"
        if baseline_file.exists():
            with open(baseline_file, 'r') as f:
                return json.load(f)
        return None
    
    def run_current_performance_tests(self) -> Dict[str, Dict]:
        """è¿è¡Œå½“å‰æ€§èƒ½æµ‹è¯•"""
        results = {}
        
        # Auth-Serviceæ€§èƒ½æµ‹è¯•
        print("è¿è¡ŒAuth-Serviceæ€§èƒ½æµ‹è¯•...")
        auth_result = self._run_service_performance_test(
            "auth-service",
            "services/auth-service",
            "tests/test_auth_advanced_fixed.py::TestAuthServicePerformance"
        )
        if auth_result:
            results["auth-service"] = auth_result
        
        # User-Serviceæ€§èƒ½æµ‹è¯•
        print("è¿è¡ŒUser-Serviceæ€§èƒ½æµ‹è¯•...")
        user_result = self._run_service_performance_test(
            "user-service", 
            "services/user-service",
            "test/test_performance_simple.py::TestUserServicePerformanceSimple"
        )
        if user_result:
            results["user-service"] = user_result
        
        return results
    
    def _run_service_performance_test(self, service_name: str, service_path: str, test_path: str) -> Optional[Dict]:
        """è¿è¡Œå•ä¸ªæœåŠ¡çš„æ€§èƒ½æµ‹è¯•"""
        try:
            # åˆ‡æ¢åˆ°æœåŠ¡ç›®å½•
            original_dir = os.getcwd()
            os.chdir(service_path)
            
            # è¿è¡Œæ€§èƒ½æµ‹è¯•
            cmd = [
                "python", "-m", "pytest", test_path,
                "--benchmark-only",
                "--benchmark-json=benchmark_results.json",
                "-v"
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
            
            # è¯»å–ç»“æœ
            if os.path.exists("benchmark_results.json"):
                with open("benchmark_results.json", 'r') as f:
                    benchmark_data = json.load(f)
                
                # è§£ææ€§èƒ½æŒ‡æ ‡
                performance_metrics = self._parse_benchmark_results(benchmark_data)
                return performance_metrics
            
        except Exception as e:
            print(f"è¿è¡Œ{service_name}æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        finally:
            os.chdir(original_dir)
        
        return None
    
    def _parse_benchmark_results(self, benchmark_data: Dict) -> Dict:
        """è§£æbenchmarkç»“æœ"""
        metrics = {}
        
        if "benchmarks" in benchmark_data:
            for benchmark in benchmark_data["benchmarks"]:
                test_name = benchmark["name"]
                stats = benchmark["stats"]
                
                metrics[test_name] = {
                    "mean": stats["mean"],
                    "min": stats["min"],
                    "max": stats["max"],
                    "stddev": stats["stddev"],
                    "median": stats["median"],
                    "ops_per_sec": 1.0 / stats["mean"] if stats["mean"] > 0 else 0
                }
        
        return metrics
    
    def compare_performance(self, current_results: Dict[str, Dict]) -> Dict[str, Dict]:
        """æ¯”è¾ƒå½“å‰æ€§èƒ½ä¸åŸºçº¿æ€§èƒ½"""
        comparison_results = {}
        
        for service, current_metrics in current_results.items():
            baseline_metrics = self.load_baseline_data(service)
            
            if not baseline_metrics:
                print(f"è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°{service}çš„åŸºçº¿æ•°æ®")
                comparison_results[service] = {
                    "status": "no_baseline",
                    "message": "æ²¡æœ‰åŸºçº¿æ•°æ®ï¼Œå°†å½“å‰ç»“æœä½œä¸ºæ–°åŸºçº¿"
                }
                self._save_baseline_data(service, current_metrics)
                continue
            
            # æ¯”è¾ƒæ€§èƒ½æŒ‡æ ‡
            service_comparison = self._compare_service_metrics(
                baseline_metrics, current_metrics, service
            )
            comparison_results[service] = service_comparison
        
        return comparison_results
    
    def _compare_service_metrics(self, baseline: Dict, current: Dict, service: str) -> Dict:
        """æ¯”è¾ƒå•ä¸ªæœåŠ¡çš„æ€§èƒ½æŒ‡æ ‡"""
        regressions = []
        improvements = []
        stable = []
        
        for test_name in current.keys():
            if test_name not in baseline:
                continue
            
            baseline_mean = baseline[test_name]["mean"]
            current_mean = current[test_name]["mean"]
            
            # è®¡ç®—æ€§èƒ½å˜åŒ–ç™¾åˆ†æ¯”
            change_percent = (current_mean - baseline_mean) / baseline_mean
            
            if change_percent > self.threshold_degradation:
                regressions.append({
                    "test": test_name,
                    "baseline_mean": baseline_mean,
                    "current_mean": current_mean,
                    "degradation_percent": change_percent * 100
                })
            elif change_percent < -0.05:  # 5%ä»¥ä¸Šçš„æ”¹è¿›
                improvements.append({
                    "test": test_name,
                    "baseline_mean": baseline_mean,
                    "current_mean": current_mean,
                    "improvement_percent": abs(change_percent) * 100
                })
            else:
                stable.append({
                    "test": test_name,
                    "baseline_mean": baseline_mean,
                    "current_mean": current_mean,
                    "change_percent": change_percent * 100
                })
        
        # ç¡®å®šæ•´ä½“çŠ¶æ€
        if regressions:
            status = "regression"
        elif improvements:
            status = "improvement"
        else:
            status = "stable"
        
        return {
            "status": status,
            "regressions": regressions,
            "improvements": improvements,
            "stable": stable,
            "total_tests": len(current)
        }
    
    def _save_baseline_data(self, service: str, metrics: Dict):
        """ä¿å­˜åŸºçº¿æ•°æ®"""
        self.baseline_dir.mkdir(exist_ok=True)
        baseline_file = self.baseline_dir / f"{service}_baseline.json"
        
        baseline_data = {
            "timestamp": datetime.now().isoformat(),
            "metrics": metrics
        }
        
        with open(baseline_file, 'w') as f:
            json.dump(baseline_data, f, indent=2)
    
    def generate_performance_report(self, comparison_results: Dict[str, Dict]) -> str:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        report_lines = [
            "# ğŸš€ æ€§èƒ½å›å½’æ£€æµ‹æŠ¥å‘Š",
            "",
            f"**æ£€æµ‹æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "## ğŸ“Š æ€»ä½“æ¦‚å†µ",
            ""
        ]
        
        # æ€»ä½“çŠ¶æ€ç»Ÿè®¡
        total_services = len(comparison_results)
        regression_services = sum(1 for r in comparison_results.values() if r["status"] == "regression")
        improvement_services = sum(1 for r in comparison_results.values() if r["status"] == "improvement")
        stable_services = sum(1 for r in comparison_results.values() if r["status"] == "stable")
        
        report_lines.extend([
            f"- ğŸ“ˆ **æ€»æœåŠ¡æ•°**: {total_services}",
            f"- ğŸ”´ **æ€§èƒ½å›å½’**: {regression_services}",
            f"- ğŸŸ¢ **æ€§èƒ½æ”¹è¿›**: {improvement_services}",
            f"- ğŸŸ¡ **æ€§èƒ½ç¨³å®š**: {stable_services}",
            ""
        ])
        
        # è¯¦ç»†æœåŠ¡æŠ¥å‘Š
        for service, results in comparison_results.items():
            report_lines.extend([
                f"## ğŸ”§ {service.title()} æ€§èƒ½åˆ†æ",
                ""
            ])
            
            if results["status"] == "no_baseline":
                report_lines.extend([
                    "â„¹ï¸ **çŠ¶æ€**: æ–°åŸºçº¿å»ºç«‹",
                    "ğŸ“ **è¯´æ˜**: æ²¡æœ‰å†å²åŸºçº¿æ•°æ®ï¼Œå½“å‰ç»“æœå·²ä¿å­˜ä¸ºæ–°åŸºçº¿",
                    ""
                ])
                continue
            
            # æ€§èƒ½å›å½’
            if results["regressions"]:
                report_lines.extend([
                    "### ğŸ”´ æ€§èƒ½å›å½’",
                    ""
                ])
                for reg in results["regressions"]:
                    report_lines.extend([
                        f"- **{reg['test']}**",
                        f"  - åŸºçº¿: {reg['baseline_mean']:.4f}s",
                        f"  - å½“å‰: {reg['current_mean']:.4f}s", 
                        f"  - ğŸ”» ä¸‹é™: {reg['degradation_percent']:.1f}%",
                        ""
                    ])
            
            # æ€§èƒ½æ”¹è¿›
            if results["improvements"]:
                report_lines.extend([
                    "### ğŸŸ¢ æ€§èƒ½æ”¹è¿›",
                    ""
                ])
                for imp in results["improvements"]:
                    report_lines.extend([
                        f"- **{imp['test']}**",
                        f"  - åŸºçº¿: {imp['baseline_mean']:.4f}s",
                        f"  - å½“å‰: {imp['current_mean']:.4f}s",
                        f"  - ğŸ”º æ”¹è¿›: {imp['improvement_percent']:.1f}%",
                        ""
                    ])
            
            # ç¨³å®šæ€§èƒ½
            if results["stable"]:
                report_lines.extend([
                    "### ğŸŸ¡ ç¨³å®šæ€§èƒ½",
                    ""
                ])
                for stable in results["stable"][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                    report_lines.extend([
                        f"- **{stable['test']}**: {stable['current_mean']:.4f}s (å˜åŒ–: {stable['change_percent']:+.1f}%)",
                        ""
                    ])
                
                if len(results["stable"]) > 3:
                    report_lines.append(f"- ... è¿˜æœ‰ {len(results['stable']) - 3} ä¸ªç¨³å®šæµ‹è¯•")
                    report_lines.append("")
        
        # å»ºè®®å’Œè¡ŒåŠ¨é¡¹
        report_lines.extend([
            "## ğŸ’¡ å»ºè®®å’Œè¡ŒåŠ¨é¡¹",
            ""
        ])
        
        if regression_services > 0:
            report_lines.extend([
                "âš ï¸ **å‘ç°æ€§èƒ½å›å½’ï¼Œå»ºè®®é‡‡å–ä»¥ä¸‹è¡ŒåŠ¨**:",
                "1. æ£€æŸ¥æœ€è¿‘çš„ä»£ç å˜æ›´",
                "2. åˆ†ææ€§èƒ½ç“¶é¢ˆç‚¹",
                "3. è€ƒè™‘ä¼˜åŒ–ç®—æ³•æˆ–æ•°æ®ç»“æ„",
                "4. å¢åŠ æ€§èƒ½ç›‘æ§å’Œå‘Šè­¦",
                ""
            ])
        else:
            report_lines.extend([
                "âœ… **æ€§èƒ½è¡¨ç°è‰¯å¥½**:",
                "1. ç»§ç»­ä¿æŒå½“å‰çš„å¼€å‘å®è·µ",
                "2. å®šæœŸæ›´æ–°æ€§èƒ½åŸºçº¿",
                "3. è€ƒè™‘è¿›ä¸€æ­¥çš„æ€§èƒ½ä¼˜åŒ–æœºä¼š",
                ""
            ])
        
        return "\n".join(report_lines)
    
    def run_regression_check(self) -> bool:
        """è¿è¡Œå®Œæ•´çš„æ€§èƒ½å›å½’æ£€æµ‹"""
        print("ğŸš€ å¼€å§‹æ€§èƒ½å›å½’æ£€æµ‹...")
        
        # 1. è¿è¡Œå½“å‰æ€§èƒ½æµ‹è¯•
        current_results = self.run_current_performance_tests()
        
        if not current_results:
            print("âŒ æ²¡æœ‰è·å–åˆ°æ€§èƒ½æµ‹è¯•ç»“æœ")
            return False
        
        # 2. ä¸åŸºçº¿æ¯”è¾ƒ
        comparison_results = self.compare_performance(current_results)
        
        # 3. ç”ŸæˆæŠ¥å‘Š
        report = self.generate_performance_report(comparison_results)
        
        # 4. ä¿å­˜æŠ¥å‘Š
        with open(self.report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"ğŸ“Š æ€§èƒ½æŠ¥å‘Šå·²ç”Ÿæˆ: {self.report_file}")
        
        # 5. æ£€æŸ¥æ˜¯å¦æœ‰æ€§èƒ½å›å½’
        has_regression = any(
            r["status"] == "regression" 
            for r in comparison_results.values()
        )
        
        if has_regression:
            print("âš ï¸ æ£€æµ‹åˆ°æ€§èƒ½å›å½’!")
            return False
        else:
            print("âœ… æ€§èƒ½æ£€æµ‹é€šè¿‡!")
            return True


def main():
    """ä¸»å‡½æ•°"""
    checker = PerformanceRegressionChecker()
    
    # è¿è¡Œæ€§èƒ½å›å½’æ£€æµ‹
    success = checker.run_regression_check()
    
    # æ ¹æ®ç»“æœè®¾ç½®é€€å‡ºç 
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main() 