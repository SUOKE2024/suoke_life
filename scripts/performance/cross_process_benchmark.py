"""
cross_process_benchmark - ç´¢å…‹ç”Ÿæ´»é¡¹ç›®æ¨¡å—
"""

from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
from dataclasses import dataclass
from multiprocessing import shared_memory
from numba import jit
from typing import List, Dict, Any, Callable
import asyncio
import json
import multiprocessing
import psutil
import time

#!/usr/bin/env python3
"""
è·¨è¿›ç¨‹å†…å­˜éš”ç¦»æ€§èƒ½åŸºå‡†æµ‹è¯•
è¯„ä¼°ä¸åŒGILä¼˜åŒ–ç­–ç•¥çš„æ€§èƒ½è¡¨ç°
"""


@dataclass
class BenchmarkResult:
    strategy: str
    execution_time: float
    memory_usage: float
    cpu_utilization: float
    throughput: float
    scalability_score: float

class CrossProcessBenchmark:
    """è·¨è¿›ç¨‹æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def __init__(self):
        self.cpu_count = multiprocessing.cpu_count()
        self.results = {}
        
    def cpu_intensive_task(self, n: int = 5000000) -> int:
        """CPUå¯†é›†å‹ä»»åŠ¡"""
        result = 0
        for i in range(n):
            result += i * i + i * 3 + i // 2
        return result
    
    def memory_intensive_task(self, size: int = 10000) -> np.ndarray:
        """å†…å­˜å¯†é›†å‹ä»»åŠ¡"""
        # åˆ›å»ºå¤§å‹çŸ©é˜µå¹¶è¿›è¡Œè®¡ç®—
        matrix1 = np.random.rand(size, size).astype(np.float32)
        matrix2 = np.random.rand(size, size).astype(np.float32)
        
        # å¤šæ¬¡çŸ©é˜µè¿ç®—
        result = matrix1
        for _ in range(5):
            result = np.dot(result, matrix2)
            result = np.transpose(result)
        
        return result
    
    @staticmethod
    @jit(nopython=True)
    def jit_optimized_task(n: int = 5000000) -> int:
        """JITä¼˜åŒ–çš„CPUå¯†é›†å‹ä»»åŠ¡"""
        result = 0
        for i in range(n):
            result += i * i + i * 3 + i // 2
        return result
    
    def io_simulation_task(self, delay: float = 0.1) -> str:
        """I/Oæ¨¡æ‹Ÿä»»åŠ¡"""
        time.sleep(delay)
        return f"IO task completed after {delay}s"
    
    async def async_io_task(self, delay: float = 0.1) -> str:
        """å¼‚æ­¥I/Oä»»åŠ¡"""
        await asyncio.sleep(delay)
        return f"Async IO task completed after {delay}s"
    
    def benchmark_single_thread(self, task_func: Callable, *args, **kwargs) -> BenchmarkResult:
        """å•çº¿ç¨‹åŸºå‡†æµ‹è¯•"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024
        start_cpu = psutil.cpu_percent(interval=None)
        
        # æ‰§è¡Œä»»åŠ¡
        results = []
        for _ in range(8):  # æ¨¡æ‹Ÿ8ä¸ªä»»åŠ¡
            result = task_func(*args, **kwargs)
            results.append(result)
        
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        end_cpu = psutil.cpu_percent(interval=None)
        
        execution_time = end_time - start_time
        memory_usage = end_memory - start_memory
        cpu_utilization = (start_cpu + end_cpu) / 2
        throughput = len(results) / execution_time
        
        return BenchmarkResult(
            strategy="single_thread",
            execution_time=execution_time,
            memory_usage=memory_usage,
            cpu_utilization=cpu_utilization,
            throughput=throughput,
            scalability_score=1.0  # åŸºå‡†åˆ†æ•°
        )
    
    def benchmark_thread_pool(self, task_func: Callable, *args, **kwargs) -> BenchmarkResult:
        """çº¿ç¨‹æ± åŸºå‡†æµ‹è¯•"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        with ThreadPoolExecutor(max_workers=self.cpu_count) as executor:
            futures = [executor.submit(task_func, *args, **kwargs) for _ in range(8)]
            results = [f.result() for f in futures]
        
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        execution_time = end_time - start_time
        memory_usage = end_memory - start_memory
        cpu_utilization = psutil.cpu_percent()
        throughput = len(results) / execution_time
        
        single_thread_result = self.benchmark_single_thread(task_func, *args, **kwargs)
        
        return BenchmarkResult(
            strategy="thread_pool",
            execution_time=execution_time,
            memory_usage=memory_usage,
            cpu_utilization=cpu_utilization,
            throughput=throughput,
            scalability_score=throughput / single_thread_result.throughput
        )
    
    def benchmark_process_pool(self, task_func: Callable, *args, **kwargs) -> BenchmarkResult:
        """è¿›ç¨‹æ± åŸºå‡†æµ‹è¯•"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        with ProcessPoolExecutor(max_workers=self.cpu_count) as executor:
            futures = [executor.submit(task_func, *args, **kwargs) for _ in range(8)]
            results = [f.result() for f in futures]
        
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        execution_time = end_time - start_time
        memory_usage = end_memory - start_memory
        cpu_utilization = psutil.cpu_percent()
        throughput = len(results) / execution_time
        
        single_thread_result = self.benchmark_single_thread(task_func, *args, **kwargs)
        
        return BenchmarkResult(
            strategy="process_pool",
            execution_time=execution_time,
            memory_usage=memory_usage,
            cpu_utilization=cpu_utilization,
            throughput=throughput,
            scalability_score=throughput / single_thread_result.throughput
        )
    
    async def benchmark_async_io(self, task_func: Callable, *args, **kwargs) -> BenchmarkResult:
        """å¼‚æ­¥I/OåŸºå‡†æµ‹è¯•"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        tasks = [task_func(*args, **kwargs) for _ in range(8)]
        results = await asyncio.gather(*tasks)
        
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        execution_time = end_time - start_time
        memory_usage = end_memory - start_memory
        cpu_utilization = psutil.cpu_percent()
        throughput = len(results) / execution_time
        
        return BenchmarkResult(
            strategy="async_io",
            execution_time=execution_time,
            memory_usage=memory_usage,
            cpu_utilization=cpu_utilization,
            throughput=throughput,
            scalability_score=throughput / 8.0  # ç†è®ºæœ€ä¼˜ä¸ºå¹¶å‘æ‰§è¡Œ
        )
    
    def benchmark_shared_memory(self, size: int = 5000) -> BenchmarkResult:
        """å…±äº«å†…å­˜åŸºå‡†æµ‹è¯•"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        # åˆ›å»ºå…±äº«å†…å­˜æ•°ç»„
        shm_size = size * size * 4  # float32
        shm = shared_memory.SharedMemory(create=True, size=shm_size)
        
        try:
            # åˆ›å»ºnumpyæ•°ç»„è§†å›¾
            shared_array = np.ndarray((size, size), dtype=np.float32, buffer=shm.buf)
            shared_array[:] = np.random.rand(size, size)
            
            # å¤šè¿›ç¨‹å¤„ç†å…±äº«æ•°ç»„
            def process_shared_array(shm_name: str, shape: tuple):
                existing_shm = shared_memory.SharedMemory(name=shm_name)
                array = np.ndarray(shape, dtype=np.float32, buffer=existing_shm.buf)
                # æ‰§è¡Œè®¡ç®—
                result = np.sum(array * array)
                existing_shm.close()
                return result
            
            with ProcessPoolExecutor(max_workers=self.cpu_count) as executor:
                futures = [
                    executor.submit(process_shared_array, shm.name, (size, size))
                    for _ in range(8)
                ]
                results = [f.result() for f in futures]
        
        finally:
            shm.close()
            shm.unlink()
        
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        execution_time = end_time - start_time
        memory_usage = end_memory - start_memory
        cpu_utilization = psutil.cpu_percent()
        throughput = len(results) / execution_time
        
        return BenchmarkResult(
            strategy="shared_memory",
            execution_time=execution_time,
            memory_usage=memory_usage,
            cpu_utilization=cpu_utilization,
            throughput=throughput,
            scalability_score=throughput / 2.0  # ç›¸å¯¹åŸºå‡†
        )
    
    def benchmark_hybrid_approach(self) -> BenchmarkResult:
        """æ··åˆæ–¹æ³•åŸºå‡†æµ‹è¯•"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        async def hybrid_execution():
            # CPUå¯†é›†å‹ä»»åŠ¡ä½¿ç”¨è¿›ç¨‹æ± 
            cpu_pool = ProcessPoolExecutor(max_workers=self.cpu_count // 2)
            # I/Oå¯†é›†å‹ä»»åŠ¡ä½¿ç”¨å¼‚æ­¥
            
            loop = asyncio.get_event_loop()
            
            # å¹¶è¡Œæ‰§è¡Œä¸åŒç±»å‹çš„ä»»åŠ¡
            cpu_tasks = [
                loop.run_in_executor(cpu_pool, self.cpu_intensive_task, 1000000)
                for _ in range(4)
            ]
            
            io_tasks = [
                self.async_io_task(0.05)
                for _ in range(4)
            ]
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            cpu_results = await asyncio.gather(*cpu_tasks)
            io_results = await asyncio.gather(*io_tasks)
            
            cpu_pool.shutdown(wait=True)
            return cpu_results + io_results
        
        results = asyncio.run(hybrid_execution())
        
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        execution_time = end_time - start_time
        memory_usage = end_memory - start_memory
        cpu_utilization = psutil.cpu_percent()
        throughput = len(results) / execution_time
        
        return BenchmarkResult(
            strategy="hybrid_approach",
            execution_time=execution_time,
            memory_usage=memory_usage,
            cpu_utilization=cpu_utilization,
            throughput=throughput,
            scalability_score=throughput / 4.0  # ç›¸å¯¹åŸºå‡†
        )
    
    def run_comprehensive_benchmark(self) -> Dict[str, Dict[str, BenchmarkResult]]:
        """è¿è¡Œç»¼åˆåŸºå‡†æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹è·¨è¿›ç¨‹å†…å­˜éš”ç¦»æ€§èƒ½åŸºå‡†æµ‹è¯•...")
        print(f"ç³»ç»Ÿé…ç½®: {self.cpu_count} CPUæ ¸å¿ƒ")
        print("-" * 80)
        
        test_scenarios = {
            "CPUå¯†é›†å‹ä»»åŠ¡": {
                "function": self.cpu_intensive_task,
                "args": (1000000,),
                "kwargs": {}
            },
            "å†…å­˜å¯†é›†å‹ä»»åŠ¡": {
                "function": self.memory_intensive_task,
                "args": (3000,),
                "kwargs": {}
            },
            "I/Oæ¨¡æ‹Ÿä»»åŠ¡": {
                "function": self.io_simulation_task,
                "args": (0.05,),
                "kwargs": {}
            }
        }
        
        all_results = {}
        
        for scenario_name, scenario_config in test_scenarios.items():
            print(f"\nğŸ“Š æµ‹è¯•åœºæ™¯: {scenario_name}")
            scenario_results = {}
            
            # å•çº¿ç¨‹åŸºå‡†
            print("  æµ‹è¯•: å•çº¿ç¨‹åŸºå‡†...")
            scenario_results["single_thread"] = self.benchmark_single_thread(
                scenario_config["function"],
                *scenario_config["args"],
                **scenario_config["kwargs"]
            )
            
            # çº¿ç¨‹æ± æµ‹è¯•
            print("  æµ‹è¯•: çº¿ç¨‹æ± ...")
            scenario_results["thread_pool"] = self.benchmark_thread_pool(
                scenario_config["function"],
                *scenario_config["args"],
                **scenario_config["kwargs"]
            )
            
            # è¿›ç¨‹æ± æµ‹è¯•
            print("  æµ‹è¯•: è¿›ç¨‹æ± ...")
            scenario_results["process_pool"] = self.benchmark_process_pool(
                scenario_config["function"],
                *scenario_config["args"],
                **scenario_config["kwargs"]
            )
            
            all_results[scenario_name] = scenario_results
        
        # å¼‚æ­¥I/Oæµ‹è¯•
        print(f"\nğŸ“Š æµ‹è¯•åœºæ™¯: å¼‚æ­¥I/O")
        async_result = asyncio.run(self.benchmark_async_io(self.async_io_task, 0.05))
        all_results["å¼‚æ­¥I/O"] = {"async_io": async_result}
        
        # å…±äº«å†…å­˜æµ‹è¯•
        print(f"\nğŸ“Š æµ‹è¯•åœºæ™¯: å…±äº«å†…å­˜")
        shared_memory_result = self.benchmark_shared_memory(2000)
        all_results["å…±äº«å†…å­˜"] = {"shared_memory": shared_memory_result}
        
        # æ··åˆæ–¹æ³•æµ‹è¯•
        print(f"\nğŸ“Š æµ‹è¯•åœºæ™¯: æ··åˆæ–¹æ³•")
        hybrid_result = self.benchmark_hybrid_approach()
        all_results["æ··åˆæ–¹æ³•"] = {"hybrid": hybrid_result}
        
        self.results = all_results
        return all_results
    
    def generate_performance_report(self, output_file: str = "cross_process_benchmark_report.json"):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        performance_summary = self._calculate_performance_metrics()
        
        report = {
            "benchmark_timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
            "system_info": {
                "cpu_count": self.cpu_count,
                "total_memory_gb": psutil.virtual_memory().total / 1024 / 1024 / 1024,
                "python_version": f"{multiprocessing.sys.version_info.major}.{multiprocessing.sys.version_info.minor}"
            },
            "detailed_results": self._serialize_results(),
            "performance_summary": performance_summary,
            "recommendations": self._generate_recommendations(performance_summary)
        }
        
        # ä¿å­˜æŠ¥å‘Š
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nğŸ“„ æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        return report
    
    def _serialize_results(self) -> Dict:
        """åºåˆ—åŒ–ç»“æœ"""
        serialized = {}
        for scenario, strategies in self.results.items():
            serialized[scenario] = {}
            for strategy, result in strategies.items():
                serialized[scenario][strategy] = {
                    "execution_time": result.execution_time,
                    "memory_usage": result.memory_usage,
                    "cpu_utilization": result.cpu_utilization,
                    "throughput": result.throughput,
                    "scalability_score": result.scalability_score
                }
        return serialized
    
    def _calculate_performance_metrics(self) -> Dict:
        """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
        metrics = {}
        
        for scenario, strategies in self.results.items():
            if len(strategies) < 2:
                continue
                
            # æ‰¾åˆ°æœ€ä½³ç­–ç•¥
            best_strategy = max(strategies.items(), key=lambda x: x[1].scalability_score)
            worst_strategy = min(strategies.items(), key=lambda x: x[1].scalability_score)
            
            metrics[scenario] = {
                "best_strategy": {
                    "name": best_strategy[0],
                    "scalability_score": best_strategy[1].scalability_score,
                    "throughput": best_strategy[1].throughput
                },
                "worst_strategy": {
                    "name": worst_strategy[0],
                    "scalability_score": worst_strategy[1].scalability_score,
                    "throughput": worst_strategy[1].throughput
                },
                "performance_gap": best_strategy[1].scalability_score / worst_strategy[1].scalability_score
            }
        
        return metrics
    
    def _generate_recommendations(self, performance_summary: Dict) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        for scenario, metrics in performance_summary.items():
            best_strategy = metrics["best_strategy"]["name"]
            performance_gap = metrics["performance_gap"]
            
            if performance_gap > 3.0:
                recommendations.append(
                    f"{scenario}: å¼ºçƒˆå»ºè®®ä½¿ç”¨{best_strategy}ï¼Œæ€§èƒ½æå‡{performance_gap:.1f}å€"
                )
            elif performance_gap > 1.5:
                recommendations.append(
                    f"{scenario}: å»ºè®®è€ƒè™‘ä½¿ç”¨{best_strategy}ï¼Œæ€§èƒ½æå‡{performance_gap:.1f}å€"
                )
            else:
                recommendations.append(
                    f"{scenario}: å½“å‰ç­–ç•¥å·²è¾ƒä¼˜ï¼Œæ€§èƒ½å·®å¼‚ä¸å¤§"
                )
        
        # æ€»ä½“å»ºè®®
        recommendations.append("æ€»ä½“å»ºè®®: é‡‡ç”¨æ··åˆæ¶æ„ï¼Œæ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©æœ€ä¼˜ç­–ç•¥")
        
        return recommendations
    
    def print_summary(self):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print("\n" + "="*80)
        print("ğŸ¯ è·¨è¿›ç¨‹å†…å­˜éš”ç¦»æ€§èƒ½åŸºå‡†æµ‹è¯•æ‘˜è¦")
        print("="*80)
        
        for scenario, strategies in self.results.items():
            print(f"\nğŸ“Š {scenario}:")
            
            # æŒ‰æ€§èƒ½æ’åº
            sorted_strategies = sorted(
                strategies.items(),
                key=lambda x: x[1].scalability_score,
                reverse=True
            )
            
            for i, (strategy, result) in enumerate(sorted_strategies, 1):
                emoji = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else "ğŸ“Š"
                print(f"  {emoji} {strategy}:")
                print(f"    æ‰§è¡Œæ—¶é—´: {result.execution_time:.3f}s")
                print(f"    ååé‡: {result.throughput:.2f} tasks/s")
                print(f"    å¯æ‰©å±•æ€§: {result.scalability_score:.2f}x")
                print(f"    å†…å­˜ä½¿ç”¨: {result.memory_usage:.1f}MB")

def main():
    """ä¸»å‡½æ•°"""
    benchmark = CrossProcessBenchmark()
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    results = benchmark.run_comprehensive_benchmark()
    
    # æ‰“å°æ‘˜è¦
    benchmark.print_summary()
    
    # ç”ŸæˆæŠ¥å‘Š
    report = benchmark.generate_performance_report()
    
    # æ‰“å°å»ºè®®
    print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
    for i, recommendation in enumerate(report['recommendations'], 1):
        print(f"  {i}. {recommendation}")

if __name__ == "__main__":
    main() 