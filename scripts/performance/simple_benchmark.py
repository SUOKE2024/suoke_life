"""
simple_benchmark - ç´¢å…‹ç”Ÿæ´»é¡¹ç›®æ¨¡å—
"""

from concurrent.futures import ProcessPoolExecutor
from dataclasses import dataclass
from datetime import datetime
from numba import jit
from typing import Dict, List, Any
import json
import logging
import multiprocessing
import psutil
import time

#!/usr/bin/env python3
"""
ç´¢å…‹ç”Ÿæ´» - ç®€åŒ–æ€§èƒ½åŸºå‡†æµ‹è¯•
éªŒè¯åŸºç¡€ä¼˜åŒ–æ•ˆæžœ
"""


# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æžœ"""
    test_name: str
    execution_time: float
    memory_usage: float
    throughput: float
    speedup: float
    additional_metrics: Dict[str, Any]

@jit(nopython=True)
def jit_vector_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """JITä¼˜åŒ–çš„å‘é‡ç›¸ä¼¼åº¦è®¡ç®—"""
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    return dot_product / (norm1 * norm2)

@jit(nopython=True)
def jit_matrix_multiply(matrix1: np.ndarray, matrix2: np.ndarray) -> np.ndarray:
    """JITä¼˜åŒ–çš„çŸ©é˜µä¹˜æ³•"""
    return np.dot(matrix1, matrix2)

def cpu_intensive_task(data_size: int) -> float:
    """CPUå¯†é›†åž‹ä»»åŠ¡"""
    data = np.random.rand(data_size, data_size)
    result = np.sum(np.dot(data, data.T))
    return result

class SimpleBenchmark:
    """ç®€åŒ–æ€§èƒ½åŸºå‡†æµ‹è¯•"""

    def __init__(self):
        self.results: List[BenchmarkResult] = []

    def run_all_benchmarks(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
        logger.info("ðŸš€ å¼€å§‹è¿è¡Œç®€åŒ–æ€§èƒ½åŸºå‡†æµ‹è¯•...")

        # 1. JITç¼–è¯‘ä¼˜åŒ–æµ‹è¯•
        self._test_jit_optimization()

        # 2. è¿›ç¨‹æ± ä¼˜åŒ–æµ‹è¯•
        self._test_process_pool_optimization()

        # 3. å†…å­˜ä¼˜åŒ–æµ‹è¯•
        self._test_memory_optimization()

        # ç”ŸæˆæŠ¥å‘Š
        return self._generate_report()

    def _test_jit_optimization(self):
        """æµ‹è¯•JITç¼–è¯‘ä¼˜åŒ–"""
        logger.info("ðŸ“Š æµ‹è¯•JITç¼–è¯‘ä¼˜åŒ–...")

        # å‡†å¤‡æµ‹è¯•æ•°æ®
        vec1 = np.random.rand(10000).astype(np.float32)
        vec2 = np.random.rand(10000).astype(np.float32)
        matrix1 = np.random.rand(1000, 1000).astype(np.float32)
        matrix2 = np.random.rand(1000, 1000).astype(np.float32)

        iterations = 100

        # æµ‹è¯•æ™®é€šè®¡ç®—
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024

        for _ in range(iterations):
            # æ™®é€šå‘é‡ç›¸ä¼¼åº¦
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            similarity = dot_product / (norm1 * norm2)

        normal_time = time.time() - start_time
        normal_memory = psutil.Process().memory_info().rss / 1024 / 1024 - start_memory

        # æµ‹è¯•JITä¼˜åŒ–è®¡ç®—
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024

        for _ in range(iterations):
            # JITä¼˜åŒ–å‘é‡ç›¸ä¼¼åº¦
            similarity = jit_vector_similarity(vec1, vec2)

        jit_time = time.time() - start_time
        jit_memory = psutil.Process().memory_info().rss / 1024 / 1024 - start_memory

        speedup = normal_time / jit_time if jit_time > 0 else 1.0

        self.results.append(BenchmarkResult(
            test_name="JITç¼–è¯‘ä¼˜åŒ–",
            execution_time=jit_time,
            memory_usage=jit_memory,
            throughput=iterations / jit_time,
            speedup=speedup,
            additional_metrics={
                "normal_time": normal_time,
                "jit_time": jit_time,
                "iterations": iterations,
                "vector_size": len(vec1)
            }
        ))

        logger.info(f"âœ… JITä¼˜åŒ–æµ‹è¯•å®Œæˆ - åŠ é€Ÿæ¯”: {speedup:.2f}x")

    def _test_process_pool_optimization(self):
        """æµ‹è¯•è¿›ç¨‹æ± ä¼˜åŒ–"""
        logger.info("ðŸ“Š æµ‹è¯•è¿›ç¨‹æ± ä¼˜åŒ–...")

        data_sizes = [300, 300, 300, 300]  # 4ä¸ªä»»åŠ¡

        # ä¸²è¡Œæ‰§è¡Œ
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024

        serial_results = []
        for size in data_sizes:
            result = cpu_intensive_task(size)
            serial_results.append(result)

        serial_time = time.time() - start_time
        serial_memory = psutil.Process().memory_info().rss / 1024 / 1024 - start_memory

        # å¹¶è¡Œæ‰§è¡Œï¼ˆè¿›ç¨‹æ± ï¼‰
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024

        with ProcessPoolExecutor(max_workers=multiprocessing.cpu_count()) as executor:
            parallel_results = list(executor.map(cpu_intensive_task, data_sizes))

        parallel_time = time.time() - start_time
        parallel_memory = psutil.Process().memory_info().rss / 1024 / 1024 - start_memory

        speedup = serial_time / parallel_time if parallel_time > 0 else 1.0

        self.results.append(BenchmarkResult(
            test_name="è¿›ç¨‹æ± ä¼˜åŒ–",
            execution_time=parallel_time,
            memory_usage=parallel_memory,
            throughput=len(data_sizes) / parallel_time,
            speedup=speedup,
            additional_metrics={
                "serial_time": serial_time,
                "parallel_time": parallel_time,
                "cpu_cores": multiprocessing.cpu_count(),
                "tasks_count": len(data_sizes)
            }
        ))

        logger.info(f"âœ… è¿›ç¨‹æ± ä¼˜åŒ–æµ‹è¯•å®Œæˆ - åŠ é€Ÿæ¯”: {speedup:.2f}x")

    def _test_memory_optimization(self):
        """æµ‹è¯•å†…å­˜ä¼˜åŒ–"""
        logger.info("ðŸ“Š æµ‹è¯•å†…å­˜ä¼˜åŒ–...")

        def memory_efficient_processing(data_size: int) -> Dict[str, float]:
            """å†…å­˜é«˜æ•ˆçš„æ•°æ®å¤„ç†"""
            start_memory = psutil.Process().memory_info().rss / 1024 / 1024

            # ä½¿ç”¨ç”Ÿæˆå™¨é¿å…å¤§é‡å†…å­˜åˆ†é…
            def data_generator():
                for i in range(data_size):
                    yield np.random.rand(100)

            # æµå¼å¤„ç†
            total_sum = 0.0
            count = 0
            for data_chunk in data_generator():
                total_sum += np.sum(data_chunk)
                count += 1

            end_memory = psutil.Process().memory_info().rss / 1024 / 1024

            return {
                "average": total_sum / count if count > 0 else 0.0,
                "memory_used": end_memory - start_memory,
                "processed_chunks": count
            }

        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024

        result = memory_efficient_processing(10000)

        execution_time = time.time() - start_time
        memory_usage = psutil.Process().memory_info().rss / 1024 / 1024 - start_memory

        self.results.append(BenchmarkResult(
            test_name="å†…å­˜ä¼˜åŒ–",
            execution_time=execution_time,
            memory_usage=memory_usage,
            throughput=result["processed_chunks"] / execution_time,
            speedup=1.0,  # åŸºå‡†å€¼
            additional_metrics={
                "processed_chunks": result["processed_chunks"],
                "average_value": result["average"],
                "memory_efficiency": result["processed_chunks"] / memory_usage if memory_usage > 0 else 0
            }
        ))

        logger.info(f"âœ… å†…å­˜ä¼˜åŒ–æµ‹è¯•å®Œæˆ - å¤„ç†äº† {result['processed_chunks']} ä¸ªæ•°æ®å—")

    def _generate_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        logger.info("ðŸ“‹ ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š...")

        total_speedup = sum(result.speedup for result in self.results) / len(self.results)
        total_memory = sum(result.memory_usage for result in self.results)
        total_throughput = sum(result.throughput for result in self.results)

        report = {
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_tests": len(self.results),
                "average_speedup": total_speedup,
                "total_memory_usage_mb": total_memory,
                "total_throughput": total_throughput,
                "cpu_cores": multiprocessing.cpu_count(),
                "system_memory_gb": psutil.virtual_memory().total / 1024 / 1024 / 1024
            },
            "detailed_results": [
                {
                    "test_name": result.test_name,
                    "execution_time_s": result.execution_time,
                    "memory_usage_mb": result.memory_usage,
                    "throughput": result.throughput,
                    "speedup": result.speedup,
                    "additional_metrics": result.additional_metrics
                }
                for result in self.results
            ],
            "recommendations": self._generate_recommendations()
        }

        # ä¿å­˜æŠ¥å‘Š
        report_file = f"performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        logger.info(f"ðŸ“Š æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

        # æ‰“å°æ‘˜è¦
        print("\n" + "="*60)
        print("ðŸŽ¯ ç´¢å…‹ç”Ÿæ´» - æ€§èƒ½ä¼˜åŒ–åŸºå‡†æµ‹è¯•æŠ¥å‘Š")
        print("="*60)
        print(f"ðŸ“ˆ å¹³å‡åŠ é€Ÿæ¯”: {total_speedup:.2f}x")
        print(f"ðŸ’¾ æ€»å†…å­˜ä½¿ç”¨: {total_memory:.2f} MB")
        print(f"âš¡ æ€»åžåé‡: {total_throughput:.2f} ops/s")
        print(f"ðŸ–¥ï¸  CPUæ ¸å¿ƒæ•°: {multiprocessing.cpu_count()}")
        print(f"ðŸ§  ç³»ç»Ÿå†…å­˜: {psutil.virtual_memory().total / 1024 / 1024 / 1024:.1f} GB")
        print("\nðŸ“‹ è¯¦ç»†ç»“æžœ:")

        for result in self.results:
            print(f"  â€¢ {result.test_name}:")
            print(f"    - æ‰§è¡Œæ—¶é—´: {result.execution_time:.3f}s")
            print(f"    - å†…å­˜ä½¿ç”¨: {result.memory_usage:.2f}MB")
            print(f"    - åŠ é€Ÿæ¯”: {result.speedup:.2f}x")
            print(f"    - åžåé‡: {result.throughput:.2f} ops/s")

        print("\nðŸ’¡ ä¼˜åŒ–å»ºè®®:")
        for i, recommendation in enumerate(report["recommendations"], 1):
            print(f"  {i}. {recommendation}")

        print("="*60)

        return report

    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        # åŸºäºŽæµ‹è¯•ç»“æžœç”Ÿæˆå»ºè®®
        jit_result = next((r for r in self.results if r.test_name == "JITç¼–è¯‘ä¼˜åŒ–"), None)
        if jit_result and jit_result.speedup > 2.0:
            recommendations.append("JITç¼–è¯‘æ˜¾è‘—æå‡æ€§èƒ½ï¼Œå»ºè®®åœ¨æ•°å€¼è®¡ç®—å¯†é›†çš„æ¨¡å—ä¸­å¹¿æ³›ä½¿ç”¨")

        process_result = next((r for r in self.results if r.test_name == "è¿›ç¨‹æ± ä¼˜åŒ–"), None)
        if process_result and process_result.speedup > 1.5:
            recommendations.append("è¿›ç¨‹æ± ä¼˜åŒ–æ•ˆæžœè‰¯å¥½ï¼Œå»ºè®®åœ¨CPUå¯†é›†åž‹ä»»åŠ¡ä¸­ä½¿ç”¨å¤šè¿›ç¨‹")

        memory_result = next((r for r in self.results if r.test_name == "å†…å­˜ä¼˜åŒ–"), None)
        if memory_result and memory_result.memory_usage < 100:
            recommendations.append("å†…å­˜ä½¿ç”¨æ•ˆçŽ‡é«˜ï¼Œå»ºè®®ç»§ç»­ä½¿ç”¨æµå¼å¤„ç†å’Œç”Ÿæˆå™¨æ¨¡å¼")

        # é€šç”¨å»ºè®®
        recommendations.extend([
            "ç»§ç»­ç›‘æŽ§ç”Ÿäº§çŽ¯å¢ƒæ€§èƒ½æŒ‡æ ‡",
            "å®šæœŸè¿è¡ŒåŸºå‡†æµ‹è¯•éªŒè¯ä¼˜åŒ–æ•ˆæžœ",
            "è€ƒè™‘ä½¿ç”¨ç¼“å­˜æœºåˆ¶è¿›ä¸€æ­¥æå‡æ€§èƒ½"
        ])

        return recommendations

def main():
    """ä¸»å‡½æ•°"""
    benchmark = SimpleBenchmark()
    report = benchmark.run_all_benchmarks()
    return report

if __name__ == "__main__":
    main() 